{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa252f14-c3d7-462c-8a63-b733136636d6",
   "metadata": {},
   "source": [
    "# Step 1: Setting up the environment\n",
    "\n",
    "#### To create and activate a virtual environment, run on your terminal:\n",
    "\n",
    "**Windows:**\n",
    "\n",
    "\n",
    "```\n",
    "python -m venv venv\n",
    "```\n",
    "```\n",
    "venv\\Scripts\\Activate\n",
    "```\n",
    "\n",
    "**macOS/Linux:**\n",
    "\n",
    "```\n",
    "python3 -m venv venv\n",
    "```\n",
    "\n",
    "```\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "#### After the environment is activated, install the requirements:\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5368ccd1-8969-49b2-93c0-f3af7b30e185",
   "metadata": {},
   "source": [
    "# Step 2: Pre-processing the document\n",
    "\n",
    "#### What you will need:\n",
    "1. PDF Document (skin cancer detection paper in my case)\n",
    "2. Embedding model (I am using Sentence-Bert)\n",
    "\n",
    "As described in the Readme file, the aim of this tutorial is to build a RAG-assisted LLM that can retrieve information from research papers, helping students and researchers get a quicker understanding of the paper. This example will refer to a pdf titled *Real and simulated multispectral imaging of a palimpsest*. This paper is not yet publicly available since I am currently in the process of publishing, but feel free to use any paper you would like to retrieve information from. The same steps will apply to any paper / book. All you need to do is download it in pdf format and add your file path to the variable 'path'.\n",
    "\n",
    "To pre-process the PDF document, we will use an embedding model. \n",
    "\n",
    "#### ⚠️ Now, what does “embedding” mean in AI?\n",
    "\n",
    "In this tutorial, we are trying to get our AI model to understand a paper (complex text data). The problem is, our model can only understand numbers. That is where embeddings come in.\n",
    "\n",
    "> An embedding is a way of representing complex data (like words or images) as a list of numbers — called a vector — in such a way that the relationships between items are preserved.\n",
    "\n",
    "\n",
    "#### Let’s dive into that:\n",
    "\n",
    "Think of each item (a word, an image, a sentence) as a point in space - a location on a map. The closer two points are, the more related their meanings are.\n",
    "\n",
    "For example:\n",
    "\n",
    "- The word “cat” will be close to “dog”.\n",
    "\n",
    "- The word “car” will be far away from “banana”.\n",
    "\n",
    "That’s because in real life, cats and dogs are similar (both animals, pets), while a car and a banana are not.\n",
    "\n",
    "So embeddings help us map meaning into a mathematical space.\n",
    "\n",
    "#### 🧐 What is an embedding model?\n",
    "\n",
    "An embedding model is an AI model that has learned how to take something complex — like a sentence — and turn it into a vector (a list of numbers) that captures its meaning.\n",
    "\n",
    "Different embedding models specialize in different kinds of data. The table below shows some examples of open-source embedding models for different use cases:\n",
    "\n",
    "\n",
    "| Data Type      | Embedding model examples               | What do they capture?                            |\n",
    "|----------------|----------------------------------------|--------------------------------------------------|\n",
    "| Words          | Word2Vec, GloVe, FastText              | Word meanings, analogies, syntactic similarity   |\n",
    "| Sentences/Text | Sentence-BERT (SBERT), Instructor, E5  | Semantic similarity between sentences/documents  |\n",
    "| Images         | DINO, OpenCLIP                         | Visual concepts, cross-modal (image-text) meaning|\n",
    "| Audio          | Wav2Vec 2.0, Whisper                   | Speech content, audio features                   |\n",
    "| Code           | CodeBERT, GraphCodeBERT                | Code syntax and semantics                        |\n",
    "  \n",
    "\n",
    "In this tutorial we are looking to read PDF documents, therefore, we need a model that embeds data based on semantic similarity. I have chosen Sentence-BERT, but it is interchangable for any sentence / text embedding model. Once you have build your own RAG-assisted LLM, you can experiment with different models and decide what works best for you\n",
    "\n",
    "Note that embedding models do not exactly embed words or sentences, they embed tokens.\n",
    "\n",
    "#### ❓ What is a token?\n",
    "\n",
    "A token is a smallest unit of input that a language model (like GPT or BERT) understands.\n",
    "\n",
    "In most modern NLP systems, tokens are not exactly words — they can be:\n",
    "\n",
    "- A whole word (hello)\n",
    "\n",
    "- A subword (un, believ, able)\n",
    "\n",
    "- A punctuation mark (!, .)\n",
    "\n",
    "- Even just a few characters (Th, is)\n",
    "\n",
    "Think of a token as a \"chunk\" of text — a building block the model processes one at a time.\n",
    "\n",
    "> **Example**\n",
    "> \n",
    "> Sentence: \"This is amazing!\" might be tokenized as:\n",
    "> \n",
    "> ['This', ' is', ' amazing', '!']\n",
    "\n",
    "\n",
    ".\n",
    "> Note that, on average in English text, 1 token is equal to 4 characters.\n",
    "\n",
    "\n",
    "#### Now that we know how the data pre_processing will work, let's get started. \r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2159f5-13b3-4291-b35c-664f452d28ce",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2.1. Importing the relevant modules, getting the PDF we want to read, and extracting text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91f0ab73-6e6a-4fa7-9367-347cf32452cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\My Drive\\feines 2025\\Github\\tutorial_RAG-assisted-LLM\\venv_llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file 'G:\\My Drive\\feines 2025\\MS Imaging paper\\to submit.pdf' exists.\n",
      " \n",
      "'For submission to Heritage Science Real and simulated multispectral imaging...'\n"
     ]
    }
   ],
   "source": [
    "# Import relevant modules\n",
    "import fitz\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Get PDF path (change this variable to your pdf path)\n",
    "#____________________________________________________________________\n",
    "path = r\"G:\\My Drive\\feines 2025\\MS Imaging paper\\to submit.pdf\"\n",
    "#____________________________________________________________________\n",
    "\n",
    "# Check that the path exists\n",
    "if os.path.exists(path):\n",
    "    print(f\"PDF file '{path}' exists.\")\n",
    "else:\n",
    "    print(f\"PDF file '{path}' does not exist\")\n",
    "\n",
    "# Open the PDF file\n",
    "paper = fitz.open(path)\n",
    "    \n",
    "# Define a helper function to extract text from the pdf\n",
    "def extract_text(paper: fitz.Document):\n",
    "  \"\"\"Applies formatting to the PDF textand stores the content in a list of dictionaries\n",
    "  Inputs: \n",
    "      paper (fitz.Document): PDF document\n",
    "  Outputs: \n",
    "      output (list[dict]): List of dictionaries containing the formatted extracted text from each PDF page \n",
    "      and the corresponding page number\n",
    "  \"\"\"\n",
    "\n",
    "  # Define an empty list that will be filled with the extracted text\n",
    "  output = []\n",
    "\n",
    "  for page_number, page in enumerate(paper):\n",
    "    paper_text = page.get_text()\n",
    "    paper_text = re.sub(r'\\s+', ' ', paper_text).strip() # removes any \\n or white spaces\n",
    "    output.append({\"page_number\": page_number,       \n",
    "                   \"text\": paper_text\n",
    "                   })\n",
    "  return output\n",
    "\n",
    "# Check that the helper function works as expected by printing the first sentence\n",
    "output = extract_text(paper=paper)\n",
    "display = output[0][\"text\"][:75]\n",
    "print(\" \")\n",
    "print(f\"'{display}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13288faf-a606-4933-bd7e-10dedd482feb",
   "metadata": {},
   "source": [
    "## 2.2. Segmenting the extracted text\n",
    "\n",
    "First of all, we will use the Natural Language Processing (NLP) library **SpaCy** to divide our extracted text in sentences.\n",
    "\n",
    "This is due to the fact that embedding models cannot process an infinite number of tokens, therefore we need to limit the number of tokens by chunking the text into groups of sentences.\n",
    "\n",
    "For this tutorial I have split the text in chunks of 8 sentences, although this number is arbitrary. Feel free to experiment and decide what works best with your model. What is the criteria to keep in mind:\n",
    "1. Smaller groups of text will be easier to inspect, making it easier to filter content\n",
    "2. The text chunks need to fit into our embedding model's context window\n",
    "3. Chunks too large will make the context that will be passed to the LLM too vague\n",
    "4. Chunks too short might leave out information that is also relevant / be misleading\n",
    "5. We want to find a chunk size so that the context passed to the LLM will be specific and focused\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe39fd69-bacc-46d4-966d-8891123d7263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This cell tests that the NLP engine (SpaCy) is detecting sentences as expected, and that the split_into_segments function splits the text in groups of 8 sentences. \n",
      "\n",
      "[SpaCy., is an NLP., library., that splits., text., into sentences., Let's., test., it.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['SpaCy.',\n",
       "  'is an NLP.',\n",
       "  'library.',\n",
       "  'that splits.',\n",
       "  'text.',\n",
       "  'into sentences.',\n",
       "  \"Let's.\",\n",
       "  'test.'],\n",
       " ['it.']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the English language model from spaCy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Initialize the language processor\n",
    "nlp_engine = English()\n",
    "\n",
    "# Add sentence segmentation capability\n",
    "# This NLP engine (SpaCy) breaks text into individual sentences\n",
    "nlp_engine.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Define how many sentences should be in each text segment\n",
    "sentences_per_segment = 8\n",
    "\n",
    "# Verify that sentence segmentation works correctly\n",
    "sample_text = nlp_engine(\"SpaCy. is an NLP. library. that splits. text. into sentences. Let's. test. it.\")\n",
    "assert len(list(sample_text.sents)) == 9\n",
    "print(f\"This cell tests that the NLP engine (SpaCy) is detecting sentences as expected, \\\n",
    "and that the split_into_segments function splits the text in groups of \\\n",
    "{sentences_per_segment} sentences. \\n\")\n",
    "print(list(sample_text.sents))\n",
    "\n",
    "# Function to divide sentences into manageable segments\n",
    "def split_into_segments(sentence_collection: list[str], \n",
    "                   segment_length: int = sentences_per_segment):\n",
    "    \"\"\"\n",
    "    Divides a collection of sentences into segments of specified length\n",
    "    \n",
    "    Args:\n",
    "        sentence_collection: List of sentences to divide\n",
    "        segment_length: Maximum number of sentences per segment\n",
    "        \n",
    "    Returns:\n",
    "        List of sentence segments\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    for i in range(0, len(sentence_collection), segment_length):\n",
    "        segments.append(sentence_collection[i:i + segment_length])\n",
    "    return segments\n",
    "\n",
    "# Test segmentation function\n",
    "test_sentences = [sent.text for sent in sample_text.sents]\n",
    "split_into_segments(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5885364d-0169-4f97-8898-0b27d50b5b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 14/14 [00:00<00:00, 61.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process each page in the PDF document\n",
    "for item in tqdm(output):\n",
    "    # Identify sentences in the document text\n",
    "    pdf_processed = nlp_engine(item[\"text\"])\n",
    "    \n",
    "    # Store sentences as strings\n",
    "    item[\"sentences\"] = [str(sent) for sent in pdf_processed.sents]\n",
    "    \n",
    "    # Create sentence segments and count them\n",
    "    item[\"sentence_segments\"] = split_into_segments(\n",
    "        sentence_collection=item[\"sentences\"],\n",
    "        segment_length=sentences_per_segment\n",
    "    )\n",
    "    \n",
    "    # Record metadata\n",
    "    item[\"segment_count\"] = len(item[\"sentence_segments\"])\n",
    "    item[\"sentence_count\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b52ed-48b0-4211-8d83-3a73dc5f0efc",
   "metadata": {},
   "source": [
    "Once the document has been divided in groups of sentences (segments or chunks), we will display a sample of one page in the PDF, to ensure this task was performed correctly. Additionally, we will print statistics on the average number of sentences and sentence segments per page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "705f1166-6672-4924-ab8b-876a1cb8f348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Sample (shortened for display):\n",
      "{ 'page_number': 6,\n",
      "  'segment_count': 2,\n",
      "  'sentence_count': 10,\n",
      "  'sentence_segments': [ [ 'Figure 3: ...',\n",
      "                           'As in Fig ...',\n",
      "                           'b)-(e) are...',\n",
      "                           'There is a...',\n",
      "                           'At interme...',\n",
      "                           'There is l...',\n",
      "                           'This diffe...',\n",
      "                           'There is l...'],\n",
      "                         ['The contra...', 'This is pr...']],\n",
      "  'sentences': [ 'Figure 3: ...',\n",
      "                 'As in Fig ...',\n",
      "                 'b)-(e) are...',\n",
      "                 'There is a...',\n",
      "                 'At interme...',\n",
      "                 'There is l...',\n",
      "                 'This diffe...',\n",
      "                 'There is l...',\n",
      "                 'The contra...',\n",
      "                 'This is pr...'],\n",
      "  'text': 'Figure 3: ...'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Print a random sample from the output\n",
    "print(\"\\nRandom Sample (shortened for display):\")\n",
    "sample_item = random.sample(output, k=1)[0]  # Get one random item\n",
    "\n",
    "# Create a copy of the sample to modify for display\n",
    "display_sample = sample_item.copy()\n",
    "\n",
    "# Truncate the text to first 10 characters\n",
    "if \"text\" in display_sample:\n",
    "    display_sample[\"text\"] = display_sample[\"text\"][:10] + \"...\" if len(display_sample[\"text\"]) > 10 else display_sample[\"text\"]\n",
    "\n",
    "# Truncate each sentence to first 10 characters\n",
    "if \"sentences\" in display_sample:\n",
    "    display_sample[\"sentences\"] = [s[:10] + \"...\" if len(s) > 10 else s for s in display_sample[\"sentences\"]]\n",
    "\n",
    "# Truncate each sentence segment to first 10 characters\n",
    "if \"sentence_segments\" in display_sample:\n",
    "    # For each segment in the list of segments\n",
    "    display_sample[\"sentence_segments\"] = [\n",
    "        # For each sentence in the segment\n",
    "        [s[:10] + \"...\" if len(s) > 10 else s for s in segment]\n",
    "        for segment in display_sample[\"sentence_segments\"]]\n",
    "\n",
    "# Print the modified sample\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(display_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e302d73-72a5-4abb-ba0e-5b30607c228a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document statistics:\n",
      "Total pages: 14\n",
      "Total sentences: 248\n",
      "Average sentences per page: 17.7\n",
      "Average segments per page: 2.6\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract the data for plotting\n",
    "sentence_counts = [item[\"sentence_count\"] for item in output]\n",
    "segment_counts = [item[\"segment_count\"] for item in output]\n",
    "\n",
    "# Calculate averages\n",
    "avg_sentence_count = np.mean(sentence_counts)\n",
    "avg_segment_count = np.mean(segment_counts)\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# First chart: Sentences per page\n",
    "ax1.bar(range(len(sentence_counts)), sentence_counts, color='skyblue')\n",
    "ax1.axhline(y=avg_sentence_count, color='red', linestyle='--', \n",
    "            label=f'Average: {avg_sentence_count:.1f}')\n",
    "ax1.set_title('Number of Sentences per Page')\n",
    "ax1.set_xlabel('Page Index')\n",
    "ax1.set_ylabel('Sentence Count')\n",
    "ax1.legend()\n",
    "\n",
    "# Second chart: Sentence segments per page\n",
    "ax2.bar(range(len(segment_counts)), segment_counts, color='lightgreen')\n",
    "ax2.axhline(y=avg_segment_count, color='red', linestyle='--', \n",
    "            label=f'Average: {avg_segment_count:.1f}')\n",
    "ax2.set_title('Number of Sentence Segments per Page')\n",
    "ax2.set_xlabel('Page Index')\n",
    "ax2.set_ylabel('Segment Count')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.savefig('document_analysis.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Document statistics:\")\n",
    "print(f\"Total pages: {len(output)}\")\n",
    "print(f\"Total sentences: {sum(sentence_counts)}\")\n",
    "print(f\"Average sentences per page: {avg_sentence_count:.1f}\")\n",
    "print(f\"Average segments per page: {avg_segment_count:.1f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf2e5d1-faab-4b1e-9633-26a7695e0741",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "The final step in section _'2.2. Segmenting the extracted text'_ is to join all sentences within 1 segmented, converting the segment back into a paragraph. Each joined paragraph will be treated as a document entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93d226e6-ed3c-4cba-911f-1a00f3f04b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 14/14 [00:00<00:00, 2303.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total segments created: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define text formatting function\n",
    "def format_text(text):\n",
    "    \"\"\"Apply consistent formatting to scientific text for improved readability.\"\"\"\n",
    "    # Fix spacing after common punctuation marks\n",
    "    for punct in ['.', ',', ';', ':', '?', '!']:\n",
    "        text = re.sub(f'\\\\{punct}([^\\\\s])', f'{punct} \\\\1', text)\n",
    "    \n",
    "    # Remove duplicate spaces\n",
    "    text = re.sub(r' {2,}', ' ', text)\n",
    "\n",
    "    # Apply strip() function and remove '\\n'\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "    \n",
    "    # Standardize citation formatting (e.g., \"[1]text\" → \"[1] text\")\n",
    "    text = re.sub(r'(\\[\\d+\\])([A-Za-z])', r'\\1 \\2', text)\n",
    "    \n",
    "    return text\n",
    "    \n",
    "# Convert each segment into its own document entry\n",
    "final_segments = []\n",
    "for item in tqdm(output):\n",
    "  for segment in item[\"sentence_segments\"]:\n",
    "    # Create a new dictionary for this segment\n",
    "    segment_entry = {}\n",
    "    \n",
    "    # Preserve the source page information\n",
    "    segment_entry[\"page_number\"] = item[\"page_number\"]\n",
    "    \n",
    "    # Combine sentences into a single coherent paragraph\n",
    "    combined_text = \"\".join(segment)\n",
    "\n",
    "    # Clean formatting using helper function\n",
    "    cleaned_text = format_text(combined_text)\n",
    "    \n",
    "    # Store the processed text\n",
    "    segment_entry[\"content\"] = cleaned_text\n",
    "    \n",
    "    # Calculate approximate token count (rough estimation)\n",
    "    segment_entry[\"estimated_tokens\"] = len(cleaned_text) / 4\n",
    "    \n",
    "    # Add to our collection\n",
    "    final_segments.append(segment_entry)\n",
    "\n",
    "# Display total number of segments created\n",
    "print(f\"Total segments created: {len(final_segments)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "474804c9-b6ae-445d-824e-91452fd16782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variable 'final_segments' has a length of: 36\n",
      " \n",
      "'Full MSI imaging was able to r...'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(f\"The variable 'final_segments' has a length of: {len(final_segments)}\")\n",
    "\n",
    "display_segment = random. randint(0, len(final_segments))\n",
    "display = final_segments[display_segment][\"content\"][:30]\n",
    "print(\" \")\n",
    "print(f\"'{display}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ecaf0-cbcf-41aa-8747-10ca670909a4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2.3. Embedding each text chunk\n",
    "The next step is to **embed** each segment of sentences into its own **numerical representation**.\n",
    "\n",
    "For this tutorial, I use the [`all-MiniLM-L6-v2`](https://www.sbert.net/docs/pretrained_models.html) model from the `sentence-transformers` library. This lightweight model offers a strong balance between speed and performance. Trained on a diverse mix of Natural Language Inference (NLI) and paraphrase datasets, it is particularly effective for general-purpose **semantic similarity** tasks and is well-suited for applications with resource constraints. In my own experiments with the paper at hand, it showed the best balance between computational cost and accurate results.\n",
    "\n",
    "However, the **optimal embedding model may vary** depending on the domain of your content. For example, if you're working with complex scientific papers, you may want to experiment with [`specter2_base`](https://huggingface.co/allenai/specter2_base), developed by **Allen AI**. **SPECTER2** is trained specifically on citation networks and scientific corpora, making it highly effective for embedding scholarly documents in a way that captures **citation-based semantic relationships**. It is particularly useful for literature recommendation and clustering.\n",
    "\n",
    "Another robust alternative is [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2), also from `sentence-transformers`. It is considered one of the **highest-performing models** for general sentence embeddings across various benchmarks like **Semantic Textual Similarity (STS)** and clustering tasks. It provides better performance than `MiniLM` in many contexts but comes with a **higher computational cost**.\n",
    "\n",
    "In short, while `all-MiniLM-L6-v2` is a great starting point for many use cases, I encourage you to **test different models** depending on the specificity of your domain and performance needs. Embedding model choice can significantly affect downstream task accuracy, especially in specialized fields such as **law**, **medicine**, or **scientific research**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f7b36dd-9fe6-4c36-921c-ead9bb911473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from time import perf_counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def embed_chunks(final_segments):\n",
    "    \"\"\"\n",
    "    Generate embeddings for all sentence segments using MiniLM model.\n",
    "    \n",
    "    Args:\n",
    "        final_segments: List of dictionaries containing sentence segments\n",
    "        \n",
    "    Returns:\n",
    "        Updated list with embeddings added to each dictionary\n",
    "    \"\"\"\n",
    "    # Load model - MiniLM is faster and lighter than MPNet\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Set device (use GPU if available)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Process batches to improve efficiency\n",
    "    # We can use a larger batch size with the smaller model\n",
    "    batch_size = 32\n",
    "    \n",
    "    for i in tqdm(range(0, len(final_segments), batch_size), desc=\"Generating embeddings\"):\n",
    "        # Get the current batch\n",
    "        batch = final_segments[i:i+batch_size]\n",
    "        \n",
    "        # Extract texts from the batch\n",
    "        texts = []\n",
    "        for item in batch:\n",
    "            if \"sentence_segments\" in item and isinstance(item[\"sentence_segments\"], list) and len(item[\"sentence_segments\"]) > 0:\n",
    "                # Join the first segment of sentences\n",
    "                texts.append(\" \".join(item[\"sentence_segments\"][0]))\n",
    "            elif \"sentences\" in item and isinstance(item[\"sentences\"], list):\n",
    "                # Join all sentences\n",
    "                texts.append(\" \".join(item[\"sentences\"]))\n",
    "            elif \"content\" in item:\n",
    "                # Use content directly if it exists\n",
    "                if isinstance(item[\"content\"], list):\n",
    "                    texts.append(\" \".join(item[\"content\"]))\n",
    "                else:\n",
    "                    texts.append(item[\"content\"])\n",
    "            else:\n",
    "                # Fallback to empty string if no recognizable content\n",
    "                texts.append(\"\")\n",
    "        \n",
    "        # Generate embeddings directly (simpler than with Transformers)\n",
    "        # The encode method handles tokenization, model inference, and extraction\n",
    "        embeddings = model.encode(texts, convert_to_tensor=True)\n",
    "        \n",
    "        # Convert to numpy and store\n",
    "        embeddings_np = embeddings.cpu().numpy()\n",
    "        \n",
    "        # Assign embeddings to the items in the batch\n",
    "        for j, item in enumerate(batch):\n",
    "            final_segments[i+j][\"embedding\"] = embeddings_np[j]\n",
    "    \n",
    "    return final_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842b6864-f5ac-481b-bf73-98b29e32120e",
   "metadata": {},
   "source": [
    "# Step 3: Similarity Search\n",
    "\n",
    "Once we have embedded each text chunk, we can move on to performing **semantic similarity search**. This allows us to retrieve the most relevant information from the paper based on a user query — a key step in building a **Retrieval-Augmented Generation (RAG)** system.\n",
    "\n",
    "#### What we're doing:\n",
    "We want to search for a query (e.g. *\"How many times is UCL equipment mentioned in the paper?\"*) and retrieve the most relevant segments of text. These segments can then be passed to an LLM to generate an accurate and grounded response.\n",
    "\n",
    "#### Steps involved:\n",
    "1. **Define the query string**  \n",
    "2. **Convert the query string into an embedding**, using the same model used to embed the text chunks  \n",
    "3. **Compute similarity** between the query embedding and each text chunk embedding using a similarity function like the **dot product** or **cosine similarity** function\n",
    "4. **Sort the results** by similarity score in descending order to identify the most relevant chunks  \n",
    "\n",
    "> **Note:**  \n",
    "> When using dot product, make sure that:\n",
    "> - The query and document embeddings have the same vector shape\n",
    "> - Both are converted to the same data type (e.g., float32 tensors)\n",
    "\n",
    "This similarity step is crucial in ensuring the LLM receives contextually relevant information, improving the accuracy of the final response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "874a7fc4-b305-4235-a087-f8c294d518c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████████████████████████████| 2/2 [00:01<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 36 segments\n",
      "Embedding dimension: (384,)\n",
      "Analyzing similarity distribution for query: How many times is UCL equipment for multispectral imaging mentioned in the paper\n",
      "Score Statistics:\n",
      "  Mean: 0.3891\n",
      "  Std Dev: 0.1171\n",
      "  Min: 0.1267\n",
      "  Max: 0.6939\n",
      "  Suggested Threshold: 0.5648\n",
      "  Documents above threshold: 2/36\n",
      "Query: How many times is UCL equipment for multispectral imaging mentioned in the paper\n",
      "Search completed in 0.0477 seconds\n",
      "\n",
      "Top results:\n",
      "1. Score: 0.6939\n",
      "   Page: 3\n",
      "   Text: Multispectral images were acquired of 13 sheets using the UCL Multispectral Imaging System. Simulated MSI images were generated using the process desc...\n",
      "\n",
      "2. Score: 0.6162\n",
      "   Page: 2\n",
      "   Text: 2. 3 Multispectral imaging The UCL Multispectral Imaging system was supplied by R B Toth Associates (USA). It is based around a PhaseOne XF camera (Ph...\n",
      "\n",
      "3. Score: 0.5381\n",
      "   Page: 0\n",
      "   Text: For submission to Heritage Science Real and simulated multispectral imaging of a palimpsest Adam Gibson1, Amy Howe2, Steve Wright2, Martina Sabate Mon...\n",
      "\n",
      "4. Score: 0.5353\n",
      "   Page: 0\n",
      "   Text: Multispectral imaging (MSI) uses wavelength-selective illumination to record multiple photographs of an object, showing its response in the near ultra...\n",
      "\n",
      "5. Score: 0.5117\n",
      "   Page: 4\n",
      "   Text: All of the image processing was carried out in Matlab R2023b (The Mathworks, USA) using the Image Processing toolbox and Hyperspectral Viewer. 3. Resu...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search_similar_chunks(final_segments, query_text, top_k=5):\n",
    "    \"\"\"\n",
    "    Search for segments similar to a query using cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        final_segments: List of dictionaries containing text segments and their embeddings\n",
    "        query_text: The query text to search for\n",
    "        top_k: Number of top results to return\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (indices, scores, results) for the top similar segments\n",
    "    \"\"\"\n",
    "    # Load model (same as used for embedding segments)\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Query: {query_text}\")\n",
    "    \n",
    "    # 1. Embed the query using the same model\n",
    "    start_time = perf_counter()\n",
    "    query_embedding = model.encode(query_text, convert_to_tensor=True).to(device)\n",
    "    \n",
    "    # 2. Convert all segment embeddings to a tensor for batch processing\n",
    "    all_embeddings = np.array([segment[\"embedding\"] for segment in final_segments])\n",
    "    all_embeddings_tensor = torch.tensor(all_embeddings).to(device)\n",
    "    \n",
    "    # 3. Compute cosine similarity\n",
    "    # SentenceTransformer models already normalize outputs, but let's ensure\n",
    "    # Normalize embeddings if needed\n",
    "    all_embeddings_tensor = all_embeddings_tensor / all_embeddings_tensor.norm(dim=1, keepdim=True)\n",
    "    query_embedding = query_embedding / query_embedding.norm()\n",
    "    \n",
    "    # Compute similarity scores (dot product of normalized vectors = cosine similarity)\n",
    "    similarity_scores = torch.matmul(query_embedding.unsqueeze(0), all_embeddings_tensor.T)[0]\n",
    "    \n",
    "    # 4. Get top-k results\n",
    "    top_k_scores, top_k_indices = torch.topk(similarity_scores, min(top_k, len(final_segments)))\n",
    "    \n",
    "    end_time = perf_counter()\n",
    "    print(f\"Search completed in {end_time - start_time:.4f} seconds\")\n",
    "    \n",
    "    # Convert to Python lists\n",
    "    top_k_indices = top_k_indices.cpu().numpy()\n",
    "    top_k_scores = top_k_scores.cpu().numpy()\n",
    "    \n",
    "    # Get the actual segments for the top results\n",
    "    top_results = [final_segments[idx] for idx in top_k_indices]\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nTop results:\")\n",
    "    for i, (score, result) in enumerate(zip(top_k_scores, top_results)):\n",
    "        print(f\"{i+1}. Score: {score:.4f}\")\n",
    "        \n",
    "        # Display page number if available\n",
    "        if \"page_number\" in result:\n",
    "            print(f\"   Page: {result['page_number']}\")\n",
    "        \n",
    "        # Display text content based on available keys\n",
    "        if \"sentence_segments\" in result and isinstance(result[\"sentence_segments\"], list) and len(result[\"sentence_segments\"]) > 0:\n",
    "            segment_text = \" \".join(result[\"sentence_segments\"][0])\n",
    "            print(f\"   Text: {segment_text[:150]}...\")\n",
    "        elif \"sentences\" in result and isinstance(result[\"sentences\"], list):\n",
    "            sentences_text = \" \".join(result[\"sentences\"])\n",
    "            print(f\"   Text: {sentences_text[:150]}...\")\n",
    "        elif \"content\" in result:\n",
    "            if isinstance(result[\"content\"], list):\n",
    "                content_text = \" \".join(result[\"content\"])\n",
    "            else:\n",
    "                content_text = result[\"content\"]\n",
    "            print(f\"   Text: {content_text[:150]}...\")\n",
    "        else:\n",
    "            print(\"   Text: [No text content available]\")\n",
    "            \n",
    "        print()\n",
    "    \n",
    "    return top_k_indices, top_k_scores, top_results\n",
    "\n",
    "\n",
    "# Enhanced version with statistical analysis\n",
    "def analyze_similarity_distribution(final_segments, query_text):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of similarity scores for a query.\n",
    "    \n",
    "    Args:\n",
    "        final_segments: List of dictionaries containing text segments and their embeddings\n",
    "        query_text: The query text to search for\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (scores, threshold)\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Analyzing similarity distribution for query: {query_text}\")\n",
    "    \n",
    "    # Embed the query\n",
    "    query_embedding = model.encode(query_text, convert_to_tensor=True).to(device)\n",
    "    \n",
    "    # Get all embeddings\n",
    "    all_embeddings = np.array([segment[\"embedding\"] for segment in final_segments])\n",
    "    all_embeddings_tensor = torch.tensor(all_embeddings).to(device)\n",
    "    \n",
    "    # Ensure normalized vectors\n",
    "    all_embeddings_tensor = all_embeddings_tensor / all_embeddings_tensor.norm(dim=1, keepdim=True)\n",
    "    query_embedding = query_embedding / query_embedding.norm()\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    similarity_scores = torch.matmul(query_embedding.unsqueeze(0), all_embeddings_tensor.T)[0].cpu().numpy()\n",
    "    \n",
    "    # Analyze distribution\n",
    "    mean_score = np.mean(similarity_scores)\n",
    "    std_score = np.std(similarity_scores)\n",
    "    min_score = np.min(similarity_scores)\n",
    "    max_score = np.max(similarity_scores)\n",
    "    \n",
    "    # Calculate a reasonable threshold (mean + 1.5*std is often useful)\n",
    "    threshold = mean_score + 1.5 * std_score\n",
    "    \n",
    "    print(f\"Score Statistics:\")\n",
    "    print(f\"  Mean: {mean_score:.4f}\")\n",
    "    print(f\"  Std Dev: {std_score:.4f}\")\n",
    "    print(f\"  Min: {min_score:.4f}\")\n",
    "    print(f\"  Max: {max_score:.4f}\")\n",
    "    print(f\"  Suggested Threshold: {threshold:.4f}\")\n",
    "    print(f\"  Documents above threshold: {np.sum(similarity_scores > threshold)}/{len(similarity_scores)}\")\n",
    "    \n",
    "    return similarity_scores, threshold\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Embed the segments\n",
    "    final_embedded_segments = embed_chunks(final_segments)\n",
    "    print(f\"Generated embeddings for {len(final_segments)} segments\")\n",
    "    \n",
    "    # Optional: Check the embedding dimension\n",
    "    if final_segments:\n",
    "        print(f\"Embedding dimension: {final_embedded_segments[0]['embedding'].shape}\")\n",
    "    \n",
    "    # First analyze distribution\n",
    "    query = \"How many times is UCL equipment for multispectral imaging mentioned in the paper\"\n",
    "    scores, threshold = analyze_similarity_distribution(final_embedded_segments, query)\n",
    "    \n",
    "    # Then search with that threshold\n",
    "    indices, scores, results = search_similar_chunks(final_embedded_segments, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666af661-2d47-4a39-a96e-89b3a1261cf8",
   "metadata": {},
   "source": [
    "# Step 4: Import an LLM and provide additional context (RAG pipeline)\n",
    "\n",
    "In this step, we integrate a **Large Language Model (LLM)** to complete our Retrieval-Augmented Generation (RAG) pipeline. For this, we will looad an **open-source LLM** such as one of the [LLaMA models](https://huggingface.co/models?search=llama), although you are free to choose any compatible model. We will provide **additional context** by incorporating the results from our similarity search through a carefully engineered prompt.\n",
    "\n",
    "### What we will be doing:\n",
    "- We **select the top 5 most relevant text segments** from the similarity search above.\n",
    "- Only the segments with a **similarity score above 0.7** will be considered\n",
    "- These selected segments are then **inserted into a prompt** engineered for the LLM, alongside the user query\n",
    "\n",
    "This context-aware prompt ensures the LLM generates a response grounded in the source material, improving both **accuracy** and **reliability**.\n",
    "\n",
    "We will import the **transformers** library to load our open-source model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6472dd-94d3-4ef7-b469-ddbe92fbff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "import torch\n",
    "\n",
    "# Log in - HuggingFace\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "\n",
    "\n",
    "# 1. Create a quantisation config\n",
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "# Bonus: flash attention 2 - faster attention mechanism\n",
    "# Flash Attention 2 requires GPU with compute score of 8+\n",
    "# For now sdpa as I'm running on CPU\n",
    "if (is_flash_attn_2_available()):\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    attn_implementation = \"sdpa\"\n",
    "\n",
    "print(attn_implementation)\n",
    "\n",
    "# 2. Pick a model to use + log in to HuggingFace\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "\n",
    "# 3. Instantiate tokeniser (turns text into tokens)\n",
    "tokeniser = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "\n",
    "\n",
    "# 4. Instantiate the model\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                                 low_cpu_mem_usage=False, # use as much memory as we can. If true it offloads memory to cpu\n",
    "                                                 attn_implementation=attn_implementation)\n",
    "\n",
    "\n",
    "if not use_quantization_config: # quantization takes care of device setting automatically, so if it's not used, send model to GPU\n",
    "    llm_model.to(\"cuda\")\n",
    "\n",
    "\n",
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    # Get model parameters and buffer sizes\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate model sizes\n",
    "    model_mem_bytes = mem_params + mem_buffers\n",
    "    model_mem_mb = model_mem_bytes / (1024**2)\n",
    "    model_mem_gb = model_mem_bytes / (1024**3)\n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes,\n",
    "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "\n",
    "    input_text = \"What are the macronutrients, and what roles do they play in the human body?\"\n",
    "print_input_text = \"Input text: \" + input_text\n",
    "print_input_text\n",
    "\n",
    "\n",
    "# Create a prompt template\n",
    "dialogue_template = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": input_text}\n",
    "]\n",
    "\n",
    "# Applies the chat template\n",
    "prompt = tokeniser.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False,\n",
    "                                       add_generation_prompt=True)\n",
    "prompt\n",
    "# This is the format we need as input for Gemma model, that is why we need the 'apply_chat_template' function\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize input text (turn into numbers) and send to CPU\n",
    "input_ids = tokeniser(prompt,\n",
    "                      return_tensors=\"pt\").to(\"cuda\")\n",
    "print(f\"Model inputs: {input_ids}\")\n",
    "\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             max_new_tokens=256)\n",
    "print(\"processed\")\n",
    "outputs[0]\n",
    "#** before input_ids means pass in (input_ids + attention mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad399033-28a5-4ae7-b33f-aca1106c1ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_________________________________________________________________________________----\n",
    "#___________________________________________________________________________________________-\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "def generate_prompt_with_context(query, top_k_results, top_k_scores, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Builds a prompt for the LLM by including relevant text chunks above a similarity threshold.\n",
    "\n",
    "    Args:\n",
    "        query (str): User's question.\n",
    "        top_k_results (list): List of top-k segment dictionaries.\n",
    "        top_k_scores (list): Corresponding similarity scores.\n",
    "        threshold (float): Minimum score required to include a chunk.\n",
    "        \n",
    "    Returns:\n",
    "        str: Final prompt string for the LLM.\n",
    "    \"\"\"\n",
    "    # Filter results based on the threshold\n",
    "    selected_chunks = []\n",
    "    for chunk, score in zip(top_k_results, top_k_scores):\n",
    "        if score >= threshold:\n",
    "            # Extract text from the most appropriate field\n",
    "            if \"content\" in chunk:\n",
    "                text = \" \".join(chunk[\"content\"]) if isinstance(chunk[\"content\"], list) else chunk[\"content\"]\n",
    "            elif \"sentences\" in chunk:\n",
    "                text = \" \".join(chunk[\"sentences\"])\n",
    "            elif \"sentence_segments\" in chunk and len(chunk[\"sentence_segments\"]) > 0:\n",
    "                text = \" \".join(chunk[\"sentence_segments\"][0])\n",
    "            else:\n",
    "                continue  # Skip if no valid content\n",
    "            selected_chunks.append(text.strip())\n",
    "\n",
    "    # Combine into a single context block\n",
    "    context_text = \"\\n\\n\".join(selected_chunks[:5])  # Limit to top 5 chunks\n",
    "\n",
    "    # Create final prompt\n",
    "    prompt = f\"\"\"You are an AI assistant helping analyze a scientific paper.\n",
    "\n",
    "Answer the following question based on the relevant context extracted from the paper.\n",
    "\n",
    "### Context:\n",
    "{context_text}\n",
    "\n",
    "### Question:\n",
    "{query}\n",
    "\n",
    "### Answer:\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Example usage\n",
    "prompt = generate_prompt_with_context(\n",
    "    query=\"How many times is UCL equipment for multispectral imaging mentioned in the paper?\",\n",
    "    top_k_results=results,\n",
    "    top_k_scores=scores,\n",
    "    threshold=0.7\n",
    ")\n",
    "\n",
    "# Load open-source LLM (example: TinyLLaMA or another compact model)\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # Replace with your preferred model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# Generate response\n",
    "response = llm_pipeline(prompt, max_new_tokens=300, do_sample=False)[0][\"generated_text\"]\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
