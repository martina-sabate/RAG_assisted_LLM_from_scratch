{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa252f14-c3d7-462c-8a63-b733136636d6",
   "metadata": {},
   "source": [
    "# Step 1: Setting up the environment\n",
    "\n",
    "#### To create and activate a virtual environment, run on your terminal:\n",
    "\n",
    "**Windows:**\n",
    "\n",
    "\n",
    "```\n",
    "python -m venv venv\n",
    "```\n",
    "```\n",
    "venv\\Scripts\\Activate\n",
    "```\n",
    "\n",
    "**macOS/Linux:**\n",
    "\n",
    "```\n",
    "python3 -m venv venv\n",
    "```\n",
    "\n",
    "```\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "#### After the environment is activated, install the requirements:\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5368ccd1-8969-49b2-93c0-f3af7b30e185",
   "metadata": {},
   "source": [
    "# Step 2: Pre-processing the document\n",
    "\n",
    "#### What you will need:\n",
    "1. PDF Document (skin cancer detection paper in my case)\n",
    "2. Embedding model (I am using Sentence-Bert)\n",
    "\n",
    "As described in the Readme file, the aim of this tutorial is to build a RAG-assisted LLM that can retrieve information from research papers, helping students and researchers get a quicker understanding of the paper. I will be using the paper *Skin Cancer Detection using ML Techniques* <sup>1</sup> for this example.  Feel free to use any paper you would like to retrieve information from, the same steps will apply to any paper / book. All you need to do is download it in pdf format and add your file path to the variable 'path'.  \n",
    "\n",
    "To pre-process the PDF document, we will use an embedding model. \n",
    "\n",
    "#### âš ï¸ Now, what does â€œembeddingâ€ mean in AI?\n",
    "\n",
    "In this tutorial, we are trying to get our AI model to understand a paper (complex text data). The problem is, our model can only understand numbers. That is where embeddings come in.\n",
    "\n",
    "> An embedding is a way of representing complex data (like words or images) as a list of numbers â€” called a vector â€” in such a way that the relationships between items are preserved.\n",
    "\n",
    "\n",
    "#### Letâ€™s dive into that:\n",
    "\n",
    "Think of each item (a word, an image, a sentence) as a point in space - a location on a map. The closer two points are, the more related their meanings are.\n",
    "\n",
    "For example:\n",
    "\n",
    "- The word â€œcatâ€ will be close to â€œdogâ€.\n",
    "\n",
    "- The word â€œcarâ€ will be far away from â€œbananaâ€.\n",
    "\n",
    "Thatâ€™s because in real life, cats and dogs are similar (both animals, pets), while a car and a banana are not.\n",
    "\n",
    "So embeddings help us map meaning into a mathematical space.\n",
    "\n",
    "#### ðŸ§ What is an embedding model?\n",
    "\n",
    "An embedding model is an AI model that has learned how to take something complex â€” like a sentence â€” and turn it into a vector (a list of numbers) that captures its meaning.\n",
    "\n",
    "Different embedding models specialize in different kinds of data. The table below shows some examples of open-source embedding models for different use cases:\n",
    "\n",
    "\n",
    "| Data Type         | Embedding model examples    | What do they capture? |\r\n",
    "|-------------------|-----------------------------|-------------------------------\n",
    "| Words | Word2Vec, GloVe, FastText | Word meanings, analogies, syntactic similarity |\n",
    "| Sentences / Text | Sentence-BERT (SBERT), Instructor, E5 | Semantic similarity between sentences/documents |\n",
    "| Images              |  DINO, OpenCLIP   | Visual concepts, cross-modal (image-text) meaning   |\n",
    "| Audio               |  Wav2Vec 2.0, Whisper  | Speech content, audio features   |\n",
    "| Code | CodeBERT, GraphCodeBERT | Code syntax and semantics |  \n",
    "\n",
    "In this tutorial we are looking to read PDF documents, therefore, we need a model that embeds data based on semantic similarity. I have chosen Sentence-BERT, but it is interchangable for any sentence / text embedding model. Once you have build your own RAG-assisted LLM, you can experiment with different models and decide what works best for you\n",
    "\n",
    "Note that embedding models do not exactly embed words or sentences, they embed tokens.\n",
    "\n",
    "#### â“ What is a token?\n",
    "\n",
    "A token is a smallest unit of input that a language model (like GPT or BERT) understands.\n",
    "\n",
    "In most modern NLP systems, tokens are not exactly words â€” they can be:\n",
    "\n",
    "- A whole word (hello)\n",
    "\n",
    "- A subword (un, believ, able)\n",
    "\n",
    "- A punctuation mark (!, .)\n",
    "\n",
    "- Even just a few characters (Th, is)\n",
    "\n",
    "Think of a token as a \"chunk\" of text â€” a building block the model processes one at a time.\n",
    "\n",
    "> **Example**\n",
    "> \n",
    "> Sentence: \"This is amazing!\" might be tokenized as:\n",
    "> \n",
    "> ['This', ' is', ' amazing', '!']\n",
    "\n",
    "\n",
    ".\n",
    "> Note that, on average in English text, 1 token is equal to 4 characters.\n",
    "\n",
    "\n",
    "#### Now that we know how the data pre_processing will work, let's get started!\n",
    "\r\n",
    "\n",
    "\n",
    "\n",
    "<sup>1</sup> M. Vidya and M.V. Karki \"Skin Cancer Detection using Machine Learning Techniques\", 2020 IEEE International Conference on Electronics, Computing and Communication Technologies, Bangalore, India, 2020, pp. 1-5, doi 10.1109/CONECCT50063.2020.9198489.98489. \r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2159f5-13b3-4291-b35c-664f452d28ce",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 2.1. Importing the relevant modules, getting the PDF we want to read, and extracting text from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "91f0ab73-6e6a-4fa7-9367-347cf32452cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file 'G:\\My Drive\\feines 2025\\MS Imaging paper\\to submit.pdf' exists.\n",
      "{'page_number': 1, 'text': 'Various studies have proposed using smartphone cameras for MSI, mainly motivated by the biomedical optics community, with the aim of monitoring haemodynamics by detecting the different spectral characteristics of oxygenated and deoxygenated haemoglobin in blood. Some of these require modifications or additions to the smartphone [7â€“10] but a new approach by He and Wang [11,12] was able to derive simulated multispectral images from an unmodified smartphone camera. Here, we adapt the method of He and Wang and apply it to generating simulated multispectral images from digitised photographs of a palimpsest. The photographs were acquired using standard digitisation protocols so the method described here could be applied to any digitised images. The technique requires a colourchecker chart which is imaged using a multispectral imaging system and with standard photography. These images are processed to provide a matrix which can convert a red-green-blue (RGB) colour photograph to a simulated multi-wavelength MSI dataset. Multispectral images have a number of advantages over RGB images, and the method described here can only emulate some of these advantages. Cameras used for RGB and MSI photography are both based on silicon sensors so have the same intrinsic spectral range from about 300 nm in the ultraviolet to 1100 nm in the near infrared. However, a commercial RGB camera has its effective spectral range reduced to about 400-800 nm to match that of the eye, by using standard glass lenses that absorb in the ultraviolet and an infrared blocking filter that excludes longer wavelengths. An MSI system maintains sensitivity further into the ultraviolet and infrared than a commercial RGB camera by using UV-transparent glass lenses and omitting the infrared blocking filter. However, more importantly, the wavelength-selective filters supplied with MSI systems are optimised for enhancing sensitivity to fluorescence. A post-processing technique such as the one proposed here could in principle offer a slightly increased spectral range by enhancing the limited sensitivity at the longest and shortest wavelengths, but it cannot offer significant sensitivity to fluorescence without changes to hardware. Here, therefore we distinguish between four methods: (i) full multispectral imaging of a palimpsest; (ii) reduced MSI imaging (excluding fluorescence); (iii) unprocessed RGB photographs; and (iv) simulated MSI images obtained by processing the RGB images with knowledge gained from the colourchecker chart. This method has the potential to offer simulated MSI for institutions that do not have access to MSI systems, and perhaps more importantly, to the many millions of objects that have already been digitised using standard photographic processes. The software was written in Matlab R2023b (The Mathworks USA) and is available ***. 2. Methods 2.1 Palimpsest The manuscript is held by UCL Special Collections (MS LAT/15). It is an early 14th Century manuscript volume and was bequeathed to UCL by John Graves (1806-1870), mathematician and Professor of Jurisprudence. It contains a number of different works on mathematics, astronomy and astrology, some by Johannes de Sacro Bosco (c. 1195 â€“ c. 1256) which were some of the first Western European texts to use Arabic numerals [13]. They are written in various hands but bound in a single volume of 33 leaves cut to 217 x 162 mm [14]. Some of the leaves are palimpsests with the undertext visible in the margin of the overtext (Figure 1). Prior to this study, the manuscript was described in the catalogue record [15] as â€œa'}\n"
     ]
    }
   ],
   "source": [
    "# Import relevant modules\n",
    "import fitz\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "# Get PDF path (change this variable to your pdf path)\n",
    "#____________________________________________________________________\n",
    "path = r\"G:\\My Drive\\feines 2025\\MS Imaging paper\\to submit.pdf\"\n",
    "#____________________________________________________________________\n",
    "\n",
    "# Check that the path exists\n",
    "if os.path.exists(path):\n",
    "    print(f\"PDF file '{path}' exists.\")\n",
    "else:\n",
    "    print(f\"PDF file '{path}' does not exist\")\n",
    "\n",
    "# Open the PDF file\n",
    "paper = fitz.open(path)\n",
    "    \n",
    "# Define a helper function to extract text from the pdf\n",
    "def extract_text(paper: fitz.Document):\n",
    "  \"\"\"Applies formatting to the PDF textand stores the content in a list of dictionaries\n",
    "  Inputs: \n",
    "      paper (fitz.Document): PDF document\n",
    "  Outputs: \n",
    "      output (list[dict]): List of dictionaries containing the formatted extracted text from each PDF page \n",
    "      and the corresponding page number\n",
    "  \"\"\"\n",
    "\n",
    "  # Define an empty list that will be filled with the extracted text\n",
    "  output = []\n",
    "\n",
    "  # CHANGE THIS LOOP A BIT MORE !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "  for page_number, page in enumerate(paper):\n",
    "    paper_text = page.get_text()\n",
    "    paper_text = re.sub(r'\\s+', ' ', paper_text).strip() # removes any \\n or white spaces\n",
    "    output.append({\"page_number\": page_number,       \n",
    "                   \"text\": paper_text\n",
    "                   })\n",
    "  return output\n",
    "\n",
    "# Check that the helper function works as expected by printing the first page\n",
    "output = extract_text(paper=paper)\n",
    "print(output[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13288faf-a606-4933-bd7e-10dedd482feb",
   "metadata": {},
   "source": [
    "#### 2.2. Chunking the extracted text\n",
    "\n",
    "First of all, we will the NLP library **SpaCy** to divide our extracted text in sentences.\n",
    "\n",
    "This is due to the fact that embedding models cannot process an infinite number of tokens, therefore we need to limit the number of tokens by chunking the text into groups of sentences.\n",
    "\n",
    "For this tutorial I have split the text in chunks of 15 sentences, although this number is arbitrary. Feel free to experiment and decide what works best with your model. What is the criteria to keep in mind:\n",
    "1. Smaller groups of text will be easier to inspect, making it easier to filter content\n",
    "2. The text chunks need to fit into our embedding model's context window\n",
    "3. Chunks too large will make the context that will be passed to the LLM too vague\n",
    "4. Chunks too short might leave out information that is also relevant / be misleading\n",
    "5. We want to find a chunk size so that the context passed to the LLM will be specific and focused\n",
    "\n",
    "# REVIEW CONTENT (siml) FROM HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe39fd69-bacc-46d4-966d-8891123d7263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SpaCy is an NLP library., It splits text into sentences., Let's test it.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
       " [15, 16, 17, 18, 19, 20, 21, 22, 23, 24]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "spacy = English()\n",
    "\n",
    "# Add a sentencizer pipeline (sentencizer turns text into sentences)\n",
    "# You can check the documentation at https://spacy.io/api/sentencizer\n",
    "spacy.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Test that the sentencizer works\n",
    "test_spacy = spacy(\"SpaCy is an NLP library. It splits text into sentences. Let's test it.\")\n",
    "assert len(list(test_spacy.sents)) == 3\n",
    "print(list(test_spacy.sents))\n",
    "\n",
    "# Define the number of sentences per chunk\n",
    "len_chunks = 15\n",
    "\n",
    "# Create a function to split the text into chunk size\n",
    "def split_list(input_list: list[str],\n",
    "               slice_size: int = len_chunks):\n",
    "  \"\"\"Splits text into chunk size\"\"\"\n",
    "  return [input_list[i:(i + slice_size)] for i in range(0, len(input_list), (slice_size))] #CHANGE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "  # Reminder: range(start, stop, step)\n",
    "\n",
    "test_list = list(range(25))\n",
    "split_list(test_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5885364d-0169-4f97-8898-0b27d50b5b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:00<00:00, 194.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop through pages and split the text into sentences\n",
    "for item in tqdm(output):\n",
    "  # Get a list of sentences in the current item's text:\n",
    "  item[\"sentences\"] = list(spacy(item[\"text\"]).sents)\n",
    "  # Make sure all sentences are strings:\n",
    "  item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "  # Loop through pages and split sentences into chunks, then get number of sentences per chunk:\n",
    "  item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                       slice_size=len_chunks)\n",
    "  item[\"num_chunks\"] = len(item[\"sentence_chunks\"])\n",
    "  # Count the sentences:\n",
    "  item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9abadeee-55d7-415b-b1fc-1d13e16644bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 5,\n",
       "  'token_count': 52.0,\n",
       "  'text': 'Figure 2: Real and simulated multispectral images of MS LAT/15 f20v. (a) is the original digitised \\nimage. (b)-(e) are four selected reflectance MSI images and (f)-(l) the corresponding simulated MSI \\nimages.',\n",
       "  'sentences': ['Figure 2: Real and simulated multispectral images of MS LAT/15 f20v. (',\n",
       "   'a) is the original digitised \\nimage. (',\n",
       "   'b)-(e) are four selected reflectance MSI images and (f)-(l) the corresponding simulated MSI \\nimages.'],\n",
       "  'page_sentence_count_spacy': 3,\n",
       "  'sentence_chunks': [['Figure 2: Real and simulated multispectral images of MS LAT/15 f20v. (',\n",
       "    'a) is the original digitised \\nimage. (',\n",
       "    'b)-(e) are four selected reflectance MSI images and (f)-(l) the corresponding simulated MSI \\nimages.']],\n",
       "  'num_chunks': 1}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(output, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996ba01a-cab7-404f-bb9c-97110f329c09",
   "metadata": {},
   "source": [
    "#### 2.3. Embedding each text chunk\n",
    "# CHANGE\n",
    "We want to **embed** each chunk of sentences into its own **numerical representation**.\n",
    "\n",
    "That will give us a good level of granularity, meaning, we can dive specifically into the text sample that was used in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93d226e6-ed3c-4cba-911f-1a00f3f04b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Split each chunk into its own item\n",
    "final_chunks = []\n",
    "for item in tqdm(output):\n",
    "  for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "    chunk_dict = {}\n",
    "    chunk_dict[\"page_number\"] = item[\"page_number\"] #get the page number\n",
    "\n",
    "    # Join sentences in a chunk into a paragraph:\n",
    "    joined_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").replace(\"\\n\", \" \").strip() # is there a way to replace all multiple spaces for 1\n",
    "\n",
    "    # ADD FORMATTING SPECIFIC TO MY DOCUMENT_______________________________________________________________\n",
    "\n",
    "    # For the 'joined_chunk', new sentences will be joined as 'end.Start'\n",
    "    # To add a space, we use library regex (re). '\\.([A-Z])' means for any chars\n",
    "    # with this format (. followed by any capital letter (A-Z)), add\n",
    "    # 1 space after '.'. So: \".A\" -> \". A\" (for any capital letter).\n",
    "    joined_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_chunk)\n",
    "\n",
    "    chunk_dict[\"sentence_chunk\"] = joined_chunk #add the joined paragraph as \"sentence_chunks\"\n",
    "\n",
    "    #Get stats:\n",
    "    chunk_dict[\"chunk_token_count\"] = len(joined_chunk) / 4\n",
    "\n",
    "    final_chunks.append(chunk_dict)\n",
    "\n",
    "len(final_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "474804c9-b6ae-445d-824e-91452fd16782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 3,\n",
       "  'sentence_chunk': 'The original digitised images were downloaded and inspected. Multispectral images were acquired  of 13 sheets using the UCL Multispectral Imaging System. Simulated MSI images were generated  using the process described in section 2.4, and using a W matrix that mapped between the Canon  EOS 6D camera used for digitisation and the PhaseOne system used for MSI.    2.5  Post processing',\n",
       "  'chunk_token_count': 96.0}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(final_chunks, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb914d00-e8b1-4498-8263-2ea89e8ece58",
   "metadata": {},
   "source": [
    "# FILTER OUT IRRELEVANT CHUNKS_____________\n",
    "e.g. he does under 30 tokens see what would be useful for me otherwise remove\n",
    "might be unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e16797e-52bd-41f9-aae6-5497a3787506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk token count: 26.5 | Text: In  this case, the simulated images unexpectedly appeared to show the greatest contrast to the  undertext.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5c10f9-de43-43ba-8dcf-d51f563fb4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Selects one of many open-source embedding models (high quality but slower)\n",
    "# model doc: https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\",\n",
    "                                      device = \"cpu\")\n",
    "\n",
    "# Creates list of sentences as an embedding demo\n",
    "sentences = [\"Sentence Transformer library provides an easy way to embed data.\",\n",
    "             \"Sentences can be embedded one by one or in a list.\",\n",
    "             \"I like dancing!\"]\n",
    "# With embedding we try to capture meaning with a value. In this case, sentence\n",
    "# number 3 should be further from 1 and 2.\n",
    "\n",
    "embeddings = embedding_model.encode(sentences)\n",
    "embeddings_dict = dict(zip(sentences, embeddings))\n",
    "\n",
    "# Shows embeddings\n",
    "for sentence, embedding in embeddings_dict.items():\n",
    "  print(f\"Sentence: {sentence}\")\n",
    "  print(f\"Embedding: {embedding}\")\n",
    "  print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aafc6af8-0c4d-4617-8575-017b607ac651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Unable to convert output to PyTorch tensors format, PyTorch is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      7\u001b[39m sentences = [\u001b[33m\"\u001b[39m\u001b[33mSentence Transformer library provides an easy way to embed data.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m              \u001b[33m\"\u001b[39m\u001b[33mSentences can be embedded one by one or in a list.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m              \u001b[33m\"\u001b[39m\u001b[33mI like dancing!\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;66;03m# CHANGE______________________________________________________________\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Tokenize the sentences\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m inputs = \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Forward pass to get hidden states\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\My Drive\\feines 2025\\Github\\tutorial_RAG-assisted-LLM\\venv_llm\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2887\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2885\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2886\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2887\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2888\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2889\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\My Drive\\feines 2025\\Github\\tutorial_RAG-assisted-LLM\\venv_llm\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2975\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   2970\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2971\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not match batch length of `text_pair`:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2972\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2973\u001b[39m         )\n\u001b[32m   2974\u001b[39m     batch_text_or_text_pairs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2978\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2980\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2981\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2982\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2986\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2987\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2988\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2989\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2990\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2991\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2992\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2993\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2994\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2995\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2996\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2997\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encode_plus(\n\u001b[32m   2998\u001b[39m         text=text,\n\u001b[32m   2999\u001b[39m         text_pair=text_pair,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3017\u001b[39m         **kwargs,\n\u001b[32m   3018\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\My Drive\\feines 2025\\Github\\tutorial_RAG-assisted-LLM\\venv_llm\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3177\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   3167\u001b[39m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[32m   3168\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3169\u001b[39m     padding=padding,\n\u001b[32m   3170\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3174\u001b[39m     **kwargs,\n\u001b[32m   3175\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3179\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3195\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3197\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\My Drive\\feines 2025\\Github\\tutorial_RAG-assisted-LLM\\venv_llm\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:587\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[39m\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m input_ids \u001b[38;5;129;01min\u001b[39;00m sanitized_tokens[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    586\u001b[39m     \u001b[38;5;28mself\u001b[39m._eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[32m--> \u001b[39m\u001b[32m587\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msanitized_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitized_encodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\My Drive\\feines 2025\\Github\\tutorial_RAG-assisted-LLM\\venv_llm\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:241\u001b[39m, in \u001b[36mBatchEncoding.__init__\u001b[39m\u001b[34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[39m\n\u001b[32m    237\u001b[39m     n_sequences = encoding[\u001b[32m0\u001b[39m].n_sequences\n\u001b[32m    239\u001b[39m \u001b[38;5;28mself\u001b[39m._n_sequences = n_sequences\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mG:\\My Drive\\feines 2025\\Github\\tutorial_RAG-assisted-LLM\\venv_llm\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:731\u001b[39m, in \u001b[36mBatchEncoding.convert_to_tensors\u001b[39m\u001b[34m(self, tensor_type, prepend_batch_axis)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m tensor_type == TensorType.PYTORCH:\n\u001b[32m    730\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnable to convert output to PyTorch tensors format, PyTorch is not installed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    732\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m    734\u001b[39m     is_tensor = torch.is_tensor\n",
      "\u001b[31mImportError\u001b[39m: Unable to convert output to PyTorch tensors format, PyTorch is not installed."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')\n",
    "#model = AutoModel.from_pretrained('allenai/specter2_base')\n",
    "\n",
    "sentences = [\"Sentence Transformer library provides an easy way to embed data.\",\n",
    "             \"Sentences can be embedded one by one or in a list.\",\n",
    "             \"I like dancing!\"] # CHANGE______________________________________________________________\n",
    "\n",
    "# Tokenize the sentences\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass to get hidden states\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]  # Take the CLS token (first token) as the embedding\n",
    "\n",
    "# Now `embeddings` is a tensor with shape (3, hidden_size)\n",
    "print(embeddings.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1307cd4-d532-4cfa-9d82-131b6a842501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
