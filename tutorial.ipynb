{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa252f14-c3d7-462c-8a63-b733136636d6",
   "metadata": {},
   "source": [
    "# Step 1: Setting up the environment\n",
    "\n",
    "#### To create and activate a virtual environment, run on your terminal:\n",
    "\n",
    "**Windows:**\n",
    "\n",
    "\n",
    "```\n",
    "python -m venv venv\n",
    "```\n",
    "```\n",
    "venv\\Scripts\\Activate\n",
    "```\n",
    "\n",
    "**macOS/Linux:**\n",
    "\n",
    "```\n",
    "python3 -m venv venv\n",
    "```\n",
    "\n",
    "```\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "#### After the environment is activated, install the requirements:\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5368ccd1-8969-49b2-93c0-f3af7b30e185",
   "metadata": {},
   "source": [
    "# Step 2: Pre-processing the document\n",
    "\n",
    "#### What you will need:\n",
    "1. PDF Document (skin cancer detection paper in my case)\n",
    "2. Embedding model (I am using Sentence-Bert)\n",
    "\n",
    "As described in the Readme file, the aim of this tutorial is to build a RAG-assisted LLM that can retrieve information from research papers, helping students and researcher get a quicker understanding of the paper. I will be using the paper *Skin Cancer Detection using ML Techniques* <sup>1</sup> for this example.  Feel free to use any paper you would like to retrieve information from, the same steps will apply to any paper / book. All you need to do is download it in pdf format and add your file path to the variable 'path'.  \n",
    "\n",
    "To pre-process the PDF document, we will use an embedding model. \n",
    "\n",
    "#### ‚ö†Ô∏è Now, what does ‚Äúembedding‚Äù mean in AI?\n",
    "\n",
    "In this tutorial, we are trying to get our AI model to understand a paper (complex text data). The problem is, our model can only understand numbers. That is where embeddings come in.\n",
    "\n",
    "> An embedding is a way of representing complex data (like words or images) as a list of numbers ‚Äî called a vector ‚Äî in such a way that the relationships between items are preserved.\n",
    "\n",
    "\n",
    "#### Let‚Äôs dive into that:\n",
    "\n",
    "Think of each item (a word, an image, a sentence) as a point in space - a location on a map. The closer two points are, the more related their meanings are.\n",
    "\n",
    "For example:\n",
    "\n",
    "- The word ‚Äúcat‚Äù will be close to ‚Äúdog‚Äù.\n",
    "\n",
    "- The word ‚Äúcar‚Äù will be far away from ‚Äúbanana‚Äù.\n",
    "\n",
    "That‚Äôs because in real life, cats and dogs are similar (both animals, pets), while a car and a banana are not.\n",
    "\n",
    "So embeddings help us map meaning into a mathematical space.\n",
    "\n",
    "#### üßê What is an embedding model?\n",
    "\n",
    "An embedding model is an AI model that has learned how to take something complex ‚Äî like a sentence ‚Äî and turn it into a vector (a list of numbers) that captures its meaning.\n",
    "\n",
    "Different embedding models specialize in different kinds of data. The table below shows some examples of open-source embedding models for different use cases:\n",
    "\n",
    "\n",
    "| Data Type         | Embedding model examples    | What do they capture? |\r\n",
    "|-------------------|-----------------------------|-------------------------------\n",
    "| Words | Word2Vec, GloVe, FastText | Word meanings, analogies, syntactic similarity |\n",
    "| Sentences / Text | Sentence-BERT (SBERT), Instructor, E5 | Semantic similarity between sentences/documents |\n",
    "| Images              |  DINO, OpenCLIP   | Visual concepts, cross-modal (image-text) meaning   |\n",
    "| Audio               |  Wav2Vec 2.0, Whisper  | Speech content, audio features   |\n",
    "| Code | CodeBERT, GraphCodeBERT | Code syntax and semantics |  \n",
    "\n",
    "In this tutorial we are looking to read PDF documents, therefore, we need a model that embeds data based on semantic similarity. I have chosen Sentence-BERT, but it is interchangable for any sentence / text embedding model. Once you have build your own RAG-assisted LLM, you can experiment with different models and decide what works best for you.\n",
    "\n",
    "#### Now that we know how the data pre-processing will work, let's get started!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "< |\r\n",
    "\n",
    "\n",
    "\n",
    "<sup>1</sup> M. Vidya and M.V. Karki \"Skin Cancer Detection using Machine Learning Techniques\", 2020 IEEE International Conference on Electronics, Computing and Communication Technologies, Bangalore, India, 2020, pp. 1-5, doi 10.1109/CONECCT50063.2020.9198489.98489. \r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91f0ab73-6e6a-4fa7-9367-347cf32452cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file 'G:\\My Drive\\feines 2025\\MS Imaging paper\\to submit.pdf' exists.\n"
     ]
    }
   ],
   "source": [
    "# Import relevant modules\n",
    "import os\n",
    "import requests\n",
    "import fitz\n",
    "from tqdm.auto import tqdm\n",
    "# Note: tqdm is used to obtain a progress bar from any loop you run with it\n",
    "\n",
    "# Get PDF path \n",
    "# Change this variable to your pdf path______________________________\n",
    "path = r\"G:\\My Drive\\feines 2025\\MS Imaging paper\\to submit.pdf\"\n",
    "#____________________________________________________________________\n",
    "\n",
    "# Check that the path exists\n",
    "if os.path.exists(pdf_path):\n",
    "    print(f\"PDF file '{path}' exists.\")\n",
    "else:\n",
    "    print(f\"PDF file '{path}' does not exist\")\n",
    "\n",
    "# Open the PDF file\n",
    "paper = fitz.open(path)\n",
    "\n",
    "# Define a helper function to clean text\n",
    "def format_pdf(paper_text: str): # REMOVE -> STR ???????????????????????\n",
    "  \"\"\"Use this function for any formatting you want to apply to the text prior to embedding.\n",
    "  Input: PDF text\n",
    "  Output: PDF text with leading or trailing whitespaces removed\"\"\"\n",
    "  formatted_pdf = paper_text.strip()\n",
    "  # add any other formatting features here\n",
    "  return formatted_pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "caa3e98c-708c-47a9-800d-5be7642239ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 0,\n",
       "  'page_token_count': 723.0,\n",
       "  'text': 'For submission to Heritage Science \\n \\nReal and simulated multispectral imaging of a palimpsest \\n \\nAdam Gibson1, Amy Howe2, Steve Wright2, Martina Sabate Monfort1, Terence Leung1, Angela \\nWarren-Thomas3, Tabitha Tuckett3, Katy Makin3 \\n1. UCL Medical Physics and Biomedical Engineering, Gower St, London WC1E 6BT \\n2. UCL Library Services, Gower St, London WC1E 6BT \\n3. UCL Special Collections, Gower St, London WC1E 6BT \\n \\nAbstract. We have recovered undertext from a palimpsest using multispectral imaging. \\nMoreover, we have developed a method for generating simulated multispectral images \\nfrom previously acquired digitised images of the manuscript, using knowledge of how a \\ncolourchecker chart appears in the multispectral images and in the standard digitised \\nimages. The ability to identify the undertext was generally better in the real \\nmultispectral images, though there were examples of improved identification in the \\nsimulated images. However, the method was unsuccessful when applied to freely \\navailable images of the Archimedes Palimpsest, suggesting that the mapping between \\nmultispectral and standard images must be calculated using the same camera systems \\nthat were used to acquire the original images. This approach may allow deeper analysis \\nof previously acquired digitised images using this low-cost simulated multispectral \\nimage approach. \\n \\n1. \\nIntroduction \\nPalimpsests present an iconic challenge in heritage imaging. A palimpsest is a manuscript page, \\nusually parchment, which has been reused. The older text is removed by either scraping or washing, \\nthen the page is often rotated, written over and rebound. The undertext is sometimes visible to the \\nnaked eye, but it is faint and is obscured by the overwriting. The imaging challenge is to reveal the \\nundertext. Multispectral imaging was developed as a method for recovering the lost text and proved \\nto be extremely successful when applied to the Archimedes Palimpsest in the late 1990s and early \\n2000s, revealing works by Archimedes and other authors that were thought to have been lost [1‚Äì3]. \\nMultispectral imaging (MSI) uses wavelength-selective illumination to record multiple photographs \\nof an object, showing its response in the near ultraviolet, visible and near infrared spectral range. \\nFilters may increase sensitivity to fluorescence by excluding the illuminating light. As well as being \\nused to reveal underwriting, MSI has also been used to detect other features that are invisible to the \\nhuman eye such as erased inscriptions, faded text and previous versions of text or paintings [4,5]. \\nMSI typically offers excellent spatial resolution but modest spectral resolution and its spectral data is \\nuncalibrated. Using Delaney‚Äôs [6] terminology, MSI is referred to as spectral imaging and offers \\nenhanced visualisation as opposed to imaging spectroscopy which provides calibrated spectra.'},\n",
       " {'page_number': 1,\n",
       "  'page_token_count': 909.25,\n",
       "  'text': 'Various studies have proposed using smartphone cameras for MSI, mainly motivated by the \\nbiomedical optics community, with the aim of monitoring haemodynamics by detecting the different \\nspectral characteristics of oxygenated and deoxygenated haemoglobin in blood. Some of these \\nrequire modifications or additions to the smartphone [7‚Äì10] but a new approach by He and Wang \\n[11,12] was able to derive simulated multispectral images from an unmodified smartphone camera. \\nHere, we adapt the method of He and Wang and apply it to generating simulated multispectral \\nimages from digitised photographs of a palimpsest. The photographs were acquired using standard \\ndigitisation protocols so the method described here could be applied to any digitised images. The \\ntechnique requires a colourchecker chart which is imaged using a multispectral imaging system and \\nwith standard photography. These images are processed to provide a matrix which can convert a \\nred-green-blue (RGB) colour photograph to a simulated multi-wavelength MSI dataset. \\nMultispectral images have a number of advantages over RGB images, and the method described \\nhere can only emulate some of these advantages. Cameras used for RGB and MSI photography are \\nboth based on silicon sensors so have the same intrinsic spectral range from about 300 nm in the \\nultraviolet to 1100 nm in the near infrared. However, a commercial RGB camera has its effective \\nspectral range reduced to about 400-800 nm to match that of the eye, by using standard glass lenses \\nthat absorb in the ultraviolet and an infrared blocking filter that excludes longer wavelengths. An \\nMSI system maintains sensitivity further into the ultraviolet and infrared than a commercial RGB \\ncamera by using UV-transparent glass lenses and omitting the infrared blocking filter. However, \\nmore importantly, the wavelength-selective filters supplied with MSI systems are optimised for \\nenhancing sensitivity to fluorescence. A post-processing technique such as the one proposed here \\ncould in principle offer a slightly increased spectral range by enhancing the limited sensitivity at the \\nlongest and shortest wavelengths, but it cannot offer significant sensitivity to fluorescence without \\nchanges to hardware. Here, therefore we distinguish between four methods: (i) full multispectral \\nimaging of a palimpsest; (ii) reduced MSI imaging (excluding fluorescence); (iii) unprocessed RGB \\nphotographs; and (iv) simulated MSI images obtained by processing the RGB images with knowledge \\ngained from the colourchecker chart. \\nThis method has the potential to offer simulated MSI for institutions that do not have access to MSI \\nsystems, and perhaps more importantly, to the many millions of objects that have already been \\ndigitised using standard photographic processes. The software was written in Matlab R2023b (The \\nMathworks USA) and is available ***. \\n \\n2. \\nMethods \\n2.1 \\nPalimpsest \\nThe manuscript is held by UCL Special Collections (MS LAT/15). It is an early 14th Century manuscript \\nvolume and was bequeathed to UCL by John Graves (1806-1870), mathematician and Professor of \\nJurisprudence. It contains a number of different works on mathematics, astronomy and astrology, \\nsome by Johannes de Sacro Bosco (c. 1195 ‚Äì c. 1256) which were some of the first Western \\nEuropean texts to use Arabic numerals [13]. They are written in various hands but bound in a single \\nvolume of 33 leaves cut to 217 x 162 mm [14]. \\nSome of the leaves are palimpsests with the undertext visible in the margin of the overtext \\n(Figure 1). Prior to this study, the manuscript was described in the catalogue record [15] as ‚Äúa'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define helper function to extract text from the pdf\n",
    "def extract_text(path: str):\n",
    "  paper = fitz.open(path)\n",
    "\n",
    "  # Define an empty list that will be filled with the extracted text\n",
    "  output = []\n",
    "\n",
    "  # CHANGE THIS LOOP A BIT MORE !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "  for page_number, page in enumerate(paper):\n",
    "    paper_text = page.get_text()\n",
    "    paper_text = format_pdf(paper_text=paper_text)\n",
    "    output.append({\"page_number\": page_number,       \n",
    "                   \"page_token_count\": len(paper_text) / 4,       #tokens in the page\n",
    "                   \"text\": paper_text                             #text in the page\n",
    "                   # Reminder: 'Hello, World!' has 4 tokens: 'Hello', ',',\n",
    "                   # 'World', and '!'. 1 token ~= 4 char on average.\n",
    "                   })\n",
    "  return output\n",
    "\n",
    "# Apply open and read helper function to pdf and display pages 1-3\n",
    "output = extract_text(path=path)\n",
    "output[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe39fd69-bacc-46d4-966d-8891123d7263",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
