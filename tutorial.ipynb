{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa252f14-c3d7-462c-8a63-b733136636d6",
   "metadata": {},
   "source": [
    "# Step 1: Setting up the environment\n",
    "\n",
    "#### To create and activate a virtual environment, run on your terminal:\n",
    "\n",
    "**Windows:**\n",
    "\n",
    "\n",
    "```\n",
    "python -m venv venv\n",
    "```\n",
    "```\n",
    "venv\\Scripts\\Activate\n",
    "```\n",
    "\n",
    "**macOS/Linux:**\n",
    "\n",
    "```\n",
    "python3 -m venv venv\n",
    "```\n",
    "\n",
    "```\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "#### After the environment is activated, install the requirements:\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5368ccd1-8969-49b2-93c0-f3af7b30e185",
   "metadata": {},
   "source": [
    "# Step 2: Pre-processing the document\n",
    "\n",
    "#### What you will need:\n",
    "1. PDF Document (skin cancer detection paper in my case)\n",
    "2. Embedding model (I am using Sentence-Bert)\n",
    "\n",
    "As described in the Readme file, the aim of this tutorial is to build a RAG-assisted LLM that can retrieve information from research papers, helping students and researchers get a quicker understanding of the paper. I will be using the paper *Skin Cancer Detection using ML Techniques* <sup>1</sup> for this example.  Feel free to use any paper you would like to retrieve information from, the same steps will apply to any paper / book. All you need to do is download it in pdf format and add your file path to the variable 'path'.  \n",
    "\n",
    "To pre-process the PDF document, we will use an embedding model. \n",
    "\n",
    "#### ⚠️ Now, what does “embedding” mean in AI?\n",
    "\n",
    "In this tutorial, we are trying to get our AI model to understand a paper (complex text data). The problem is, our model can only understand numbers. That is where embeddings come in.\n",
    "\n",
    "> An embedding is a way of representing complex data (like words or images) as a list of numbers — called a vector — in such a way that the relationships between items are preserved.\n",
    "\n",
    "\n",
    "#### Let’s dive into that:\n",
    "\n",
    "Think of each item (a word, an image, a sentence) as a point in space - a location on a map. The closer two points are, the more related their meanings are.\n",
    "\n",
    "For example:\n",
    "\n",
    "- The word “cat” will be close to “dog”.\n",
    "\n",
    "- The word “car” will be far away from “banana”.\n",
    "\n",
    "That’s because in real life, cats and dogs are similar (both animals, pets), while a car and a banana are not.\n",
    "\n",
    "So embeddings help us map meaning into a mathematical space.\n",
    "\n",
    "#### 🧐 What is an embedding model?\n",
    "\n",
    "An embedding model is an AI model that has learned how to take something complex — like a sentence — and turn it into a vector (a list of numbers) that captures its meaning.\n",
    "\n",
    "Different embedding models specialize in different kinds of data. The table below shows some examples of open-source embedding models for different use cases:\n",
    "\n",
    "\n",
    "| Data Type         | Embedding model examples    | What do they capture? |\r\n",
    "|-------------------|-----------------------------|-------------------------------\n",
    "| Words | Word2Vec, GloVe, FastText | Word meanings, analogies, syntactic similarity |\n",
    "| Sentences / Text | Sentence-BERT (SBERT), Instructor, E5 | Semantic similarity between sentences/documents |\n",
    "| Images              |  DINO, OpenCLIP   | Visual concepts, cross-modal (image-text) meaning   |\n",
    "| Audio               |  Wav2Vec 2.0, Whisper  | Speech content, audio features   |\n",
    "| Code | CodeBERT, GraphCodeBERT | Code syntax and semantics |  \n",
    "\n",
    "In this tutorial we are looking to read PDF documents, therefore, we need a model that embeds data based on semantic similarity. I have chosen Sentence-BERT, but it is interchangable for any sentence / text embedding model. Once you have build your own RAG-assisted LLM, you can experiment with different models and decide what works best for you\n",
    "\n",
    "Note that embedding models do not exactly embed words or sentences, they embed tokens.\n",
    "\n",
    "#### ❓ What is a token?\n",
    "\n",
    "A token is a smallest unit of input that a language model (like GPT or BERT) understands.\n",
    "\n",
    "In most modern NLP systems, tokens are not exactly words — they can be:\n",
    "\n",
    "- A whole word (hello)\n",
    "\n",
    "- A subword (un, believ, able)\n",
    "\n",
    "- A punctuation mark (!, .)\n",
    "\n",
    "- Even just a few characters (Th, is)\n",
    "\n",
    "Think of a token as a \"chunk\" of text — a building block the model processes one at a time.\n",
    "\n",
    "> **Example**\n",
    "> \n",
    "> Sentence: \"This is amazing!\" might be tokenized as:\n",
    "> \n",
    "> ['This', ' is', ' amazing', '!']\n",
    "\n",
    "\n",
    ".\n",
    "> Note that, on average in English text, 1 token is equal to 4 characters.\n",
    "\n",
    "\n",
    "#### Now that we know how the data pre_processing will work, let's get started!\n",
    "\r\n",
    "\n",
    "\n",
    "\n",
    "<sup>1</sup> M. Vidya and M.V. Karki \"Skin Cancer Detection using Machine Learning Techniques\", 2020 IEEE International Conference on Electronics, Computing and Communication Technologies, Bangalore, India, 2020, pp. 1-5, doi 10.1109/CONECCT50063.2020.9198489.98489. \r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2159f5-13b3-4291-b35c-664f452d28ce",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 2.1. Importing the relevant modules, getting the PDF we want to read, and extracting text from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91f0ab73-6e6a-4fa7-9367-347cf32452cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file 'G:\\My Drive\\feines 2025\\MS Imaging paper\\to submit.pdf' exists.\n",
      "{'page_number': 1, 'text': 'Various studies have proposed using smartphone cameras for MSI, mainly motivated by the biomedical optics community, with the aim of monitoring haemodynamics by detecting the different spectral characteristics of oxygenated and deoxygenated haemoglobin in blood. Some of these require modifications or additions to the smartphone [7–10] but a new approach by He and Wang [11,12] was able to derive simulated multispectral images from an unmodified smartphone camera. Here, we adapt the method of He and Wang and apply it to generating simulated multispectral images from digitised photographs of a palimpsest. The photographs were acquired using standard digitisation protocols so the method described here could be applied to any digitised images. The technique requires a colourchecker chart which is imaged using a multispectral imaging system and with standard photography. These images are processed to provide a matrix which can convert a red-green-blue (RGB) colour photograph to a simulated multi-wavelength MSI dataset. Multispectral images have a number of advantages over RGB images, and the method described here can only emulate some of these advantages. Cameras used for RGB and MSI photography are both based on silicon sensors so have the same intrinsic spectral range from about 300 nm in the ultraviolet to 1100 nm in the near infrared. However, a commercial RGB camera has its effective spectral range reduced to about 400-800 nm to match that of the eye, by using standard glass lenses that absorb in the ultraviolet and an infrared blocking filter that excludes longer wavelengths. An MSI system maintains sensitivity further into the ultraviolet and infrared than a commercial RGB camera by using UV-transparent glass lenses and omitting the infrared blocking filter. However, more importantly, the wavelength-selective filters supplied with MSI systems are optimised for enhancing sensitivity to fluorescence. A post-processing technique such as the one proposed here could in principle offer a slightly increased spectral range by enhancing the limited sensitivity at the longest and shortest wavelengths, but it cannot offer significant sensitivity to fluorescence without changes to hardware. Here, therefore we distinguish between four methods: (i) full multispectral imaging of a palimpsest; (ii) reduced MSI imaging (excluding fluorescence); (iii) unprocessed RGB photographs; and (iv) simulated MSI images obtained by processing the RGB images with knowledge gained from the colourchecker chart. This method has the potential to offer simulated MSI for institutions that do not have access to MSI systems, and perhaps more importantly, to the many millions of objects that have already been digitised using standard photographic processes. The software was written in Matlab R2023b (The Mathworks USA) and is available ***. 2. Methods 2.1 Palimpsest The manuscript is held by UCL Special Collections (MS LAT/15). It is an early 14th Century manuscript volume and was bequeathed to UCL by John Graves (1806-1870), mathematician and Professor of Jurisprudence. It contains a number of different works on mathematics, astronomy and astrology, some by Johannes de Sacro Bosco (c. 1195 – c. 1256) which were some of the first Western European texts to use Arabic numerals [13]. They are written in various hands but bound in a single volume of 33 leaves cut to 217 x 162 mm [14]. Some of the leaves are palimpsests with the undertext visible in the margin of the overtext (Figure 1). Prior to this study, the manuscript was described in the catalogue record [15] as “a'}\n"
     ]
    }
   ],
   "source": [
    "# Import relevant modules\n",
    "import fitz\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Get PDF path (change this variable to your pdf path)\n",
    "#____________________________________________________________________\n",
    "path = r\"G:\\My Drive\\feines 2025\\MS Imaging paper\\to submit.pdf\"\n",
    "#____________________________________________________________________\n",
    "\n",
    "# Check that the path exists\n",
    "if os.path.exists(path):\n",
    "    print(f\"PDF file '{path}' exists.\")\n",
    "else:\n",
    "    print(f\"PDF file '{path}' does not exist\")\n",
    "\n",
    "# Open the PDF file\n",
    "paper = fitz.open(path)\n",
    "    \n",
    "# Define a helper function to extract text from the pdf\n",
    "def extract_text(paper: fitz.Document):\n",
    "  \"\"\"Applies formatting to the PDF textand stores the content in a list of dictionaries\n",
    "  Inputs: \n",
    "      paper (fitz.Document): PDF document\n",
    "  Outputs: \n",
    "      output (list[dict]): List of dictionaries containing the formatted extracted text from each PDF page \n",
    "      and the corresponding page number\n",
    "  \"\"\"\n",
    "\n",
    "  # Define an empty list that will be filled with the extracted text\n",
    "  output = []\n",
    "\n",
    "  # CHANGE THIS LOOP A BIT MORE !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "  for page_number, page in enumerate(paper):\n",
    "    paper_text = page.get_text()\n",
    "    paper_text = re.sub(r'\\s+', ' ', paper_text).strip() # removes any \\n or white spaces\n",
    "    output.append({\"page_number\": page_number,       \n",
    "                   \"text\": paper_text\n",
    "                   })\n",
    "  return output\n",
    "\n",
    "# Check that the helper function works as expected by printing the first page\n",
    "output = extract_text(paper=paper)\n",
    "print(output[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13288faf-a606-4933-bd7e-10dedd482feb",
   "metadata": {},
   "source": [
    "#### 2.2. Chunking the extracted text\n",
    "\n",
    "First of all, we will the NLP library **SpaCy** to divide our extracted text in sentences.\n",
    "\n",
    "This is due to the fact that embedding models cannot process an infinite number of tokens, therefore we need to limit the number of tokens by chunking the text into groups of sentences.\n",
    "\n",
    "For this tutorial I have split the text in chunks of 15 sentences, although this number is arbitrary. Feel free to experiment and decide what works best with your model. What is the criteria to keep in mind:\n",
    "1. Smaller groups of text will be easier to inspect, making it easier to filter content\n",
    "2. The text chunks need to fit into our embedding model's context window\n",
    "3. Chunks too large will make the context that will be passed to the LLM too vague\n",
    "4. Chunks too short might leave out information that is also relevant / be misleading\n",
    "5. We want to find a chunk size so that the context passed to the LLM will be specific and focused\n",
    "\n",
    "# REVIEW CONTENT (siml) FROM HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe39fd69-bacc-46d4-966d-8891123d7263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SpaCy is an NLP library., It splits text into sentences., Let's test it.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
       " [15, 16, 17, 18, 19, 20, 21, 22, 23, 24]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "spacy = English()\n",
    "\n",
    "# Add a sentencizer pipeline (sentencizer turns text into sentences)\n",
    "# You can check the documentation at https://spacy.io/api/sentencizer\n",
    "spacy.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Test that the sentencizer works\n",
    "test_spacy = spacy(\"SpaCy is an NLP library. It splits text into sentences. Let's test it.\")\n",
    "assert len(list(test_spacy.sents)) == 3\n",
    "print(list(test_spacy.sents))\n",
    "\n",
    "# Define the number of sentences per chunk\n",
    "len_chunks = 15\n",
    "\n",
    "# Create a function to split the text into chunk size\n",
    "def split_list(input_list: list[str],\n",
    "               slice_size: int = len_chunks):\n",
    "  \"\"\"Splits text into chunk size\"\"\"\n",
    "  return [input_list[i:(i + slice_size)] for i in range(0, len(input_list), (slice_size))] #CHANGE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "  # Reminder: range(start, stop, step)\n",
    "\n",
    "test_list = list(range(25))\n",
    "split_list(test_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5885364d-0169-4f97-8898-0b27d50b5b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00<00:00, 181.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop through pages and split the text into sentences\n",
    "for item in tqdm(output):\n",
    "  # Get a list of sentences in the current item's text:\n",
    "  item[\"sentences\"] = list(spacy(item[\"text\"]).sents)\n",
    "  # Make sure all sentences are strings:\n",
    "  item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "  # Loop through pages and split sentences into chunks, then get number of sentences per chunk:\n",
    "  item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                       slice_size=len_chunks)\n",
    "  item[\"num_chunks\"] = len(item[\"sentence_chunks\"])\n",
    "  # Count the sentences:\n",
    "  item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9abadeee-55d7-415b-b1fc-1d13e16644bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 10,\n",
       "  'text': '5. Application to Archimedes Palimpsest The photographs of the palimpsest and ColorChecker described in section 3 were taken using the same camera, lens, lighting system and processing pipeline. It would be useful to know how far this can be generalised – can a W matrix calculated for a particular photography set-up be applied to photographs taken using a different set-up? This will not give optimal results, but given that PCA is non-quantitative, an approximate rendering of the multispectral images might still offer some useful information. Note that if more sophisticated, calibrated or quantitative image analysis was to be attempted, we would not consider using an unmatched W matrix. Indeed, we advise against using simulated multispectral images for any extended analysis. We examine the hypothesis that a generic W matrix can be used in some circumstances by reanalysing photographs taken of the Archimedes Palimpsest between 1999 and 2008 ([1–3]), downloaded from https://www.archimedespalimpsest.net/. Previous work has demonstrated that image processing including PCA can enhance visibility of the undertext [3]. Images of many sheets are available. We chose to analyse 168r and 165v which were photographed together and used for more additional processing than most other pairs of sheets. The RGB image “168r-165v_Arch30v-29r_Sinar_TNGSTN_01_pack8.tif” was downloaded and processed as above, using the W matrix obtained in 3.1, which had been calculated for a different camera and MSI system. The stack of simulated MSI images were very similar, and needed a close examination to perceive difference between them which were only really visible in the part of the image that showed the ColorChecker squares (Figure 6). Neither the simulated multispectral images nor PCA were able to reveal any detail of the underwriting that wasn’t visible under standard lighting conditions.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(output, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996ba01a-cab7-404f-bb9c-97110f329c09",
   "metadata": {},
   "source": [
    "#### 2.3. Embedding each text chunk\n",
    "# CHANGE\n",
    "We want to **embed** each chunk of sentences into its own **numerical representation**.\n",
    "\n",
    "That will give us a good level of granularity, meaning, we can dive specifically into the text sample that was used in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93d226e6-ed3c-4cba-911f-1a00f3f04b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00<00:00, 16686.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Split each chunk into its own item\n",
    "final_chunks = []\n",
    "for item in tqdm(output):\n",
    "  for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "    chunk_dict = {}\n",
    "    chunk_dict[\"page_number\"] = item[\"page_number\"] #get the page number\n",
    "\n",
    "    # Join sentences in a chunk into a paragraph:\n",
    "    joined_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").replace(\"\\n\", \" \").strip() # is there a way to replace all multiple spaces for 1\n",
    "\n",
    "    # ADD FORMATTING SPECIFIC TO MY DOCUMENT_______________________________________________________________\n",
    "\n",
    "    # For the 'joined_chunk', new sentences will be joined as 'end.Start'\n",
    "    # To add a space, we use library regex (re). '\\.([A-Z])' means for any chars\n",
    "    # with this format (. followed by any capital letter (A-Z)), add\n",
    "    # 1 space after '.'. So: \".A\" -> \". A\" (for any capital letter).\n",
    "    joined_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_chunk)\n",
    "\n",
    "    chunk_dict[\"sentence_chunk\"] = joined_chunk #add the joined paragraph as \"sentence_chunks\"\n",
    "\n",
    "    #Get stats:\n",
    "    chunk_dict[\"chunk_token_count\"] = len(joined_chunk) / 4\n",
    "\n",
    "    final_chunks.append(chunk_dict)\n",
    "\n",
    "len(final_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "474804c9-b6ae-445d-824e-91452fd16782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 8,\n",
       "  'sentence_chunk': 'In this case, the simulated images unexpectedly appeared to show the greatest contrast to the undertext.',\n",
       "  'chunk_token_count': 26.0}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(final_chunks, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb914d00-e8b1-4498-8263-2ea89e8ece58",
   "metadata": {},
   "source": [
    "# FILTER OUT IRRELEVANT CHUNKS_____________\n",
    "e.g. he does under 30 tokens see what would be useful for me otherwise remove\n",
    "might be unnecessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1307cd4-d532-4cfa-9d82-131b6a842501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "# RESEARCH__________________________________________\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')\n",
    "model = AutoModel.from_pretrained('allenai/specter2_base')\n",
    "\n",
    "sentences = [\"Sentence Transformer library provides an easy way to embed data.\",\n",
    "             \"Sentences can be embedded one by one or in a list.\",\n",
    "             \"I like dancing!\"] # CHANGE______________________________________________________________\n",
    "\n",
    "for item in tqdm(pages_and_chunks_over_min_tokens):\n",
    "# Tokenize the sentences\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass to get hidden states\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]  # Take the CLS token (first token) as the embedding\n",
    "\n",
    "# Now `embeddings` is a tensor with shape (3, hidden_size)\n",
    "print(embeddings.shape) \n",
    "\n",
    "embeddings_dict = dict(zip(sentences, embeddings))\n",
    "print(embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b04606de-9cf5-4293-829c-5a2d21f8f718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:10<00:00,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_number': 0, 'sentence_chunk': 'Multispectral imaging was developed as a method for recovering the lost text and proved to be extremely successful when applied to the Archimedes Palimpsest in the late 1990s and early 2000s, revealing works by Archimedes and other authors that were thought to have been lost [1–3]. Multispectral imaging (MSI) uses wavelength-selective illumination to record multiple photographs of an object, showing its response in the near ultraviolet, visible and near infrared spectral range. Filters may increase sensitivity to fluorescence by excluding the illuminating light. As well as being used to reveal underwriting, MSI has also been used to detect other features that are invisible to the human eye such as erased inscriptions, faded text and previous versions of text or paintings [4,5]. MSI typically offers excellent spatial resolution but modest spectral resolution and its spectral data is uncalibrated. Using Delaney’s [6] terminology, MSI is referred to as spectral imaging and offers enhanced visualisation as opposed to imaging spectroscopy which provides calibrated spectra.', 'chunk_token_count': 271.0, 'embeddings': tensor([[ 1.1690e+00,  2.6034e-02,  7.5216e-02,  7.6431e-01, -3.9161e-01,\n",
      "         -3.2081e-01,  3.4174e-01, -2.2290e-01,  7.1778e-02, -5.1535e-02,\n",
      "          3.7099e-01,  3.2154e-01,  4.6729e-02,  9.6370e-02,  2.3242e-03,\n",
      "         -4.2069e-02, -7.9094e-01, -3.2243e-01,  2.3323e-01,  1.4815e-01,\n",
      "          4.4994e-01, -2.9374e-01, -4.5974e-01,  3.5988e-01, -2.3291e-02,\n",
      "          5.9665e-01,  4.4735e-02,  7.6431e-01, -2.0781e-01,  5.2728e-02,\n",
      "          3.7468e-02, -6.3726e-01,  1.1296e-01,  8.6942e-02, -5.1721e-03,\n",
      "          2.3868e-01,  5.3644e-01, -6.5438e-01, -7.1349e-01,  6.2759e-01,\n",
      "         -5.2088e-01, -2.0102e-01,  7.8467e-01,  1.8094e-01, -5.3588e-01,\n",
      "          3.6426e-01, -1.7957e-01,  5.6232e-01,  5.6240e-01, -6.1716e-01,\n",
      "          1.0949e+00, -8.5483e-01,  3.7796e-01,  1.0311e+00,  9.6057e-01,\n",
      "         -3.9113e-01, -5.8633e-01, -1.2306e-01, -3.5673e-01, -1.1288e-01,\n",
      "         -7.9254e-01,  1.3690e-01,  6.7637e-01, -3.6196e-01,  2.7762e-01,\n",
      "         -5.2683e-01, -1.2220e-01,  4.4308e-01,  7.9359e-01,  1.4536e+00,\n",
      "          8.2154e-01, -1.1893e+00, -6.4433e-01,  5.7175e-01,  4.5348e-02,\n",
      "          1.8645e-01, -6.0400e-01,  3.1121e-01, -1.0027e+00, -3.2861e-01,\n",
      "         -1.7562e-01,  2.0138e-01, -4.3779e-02, -8.4416e-02,  7.2412e-03,\n",
      "          2.2653e-01,  2.4910e-01, -3.0156e-01,  3.8105e-02,  4.4662e-02,\n",
      "          6.9379e-02,  3.1669e-02,  6.7045e-01, -8.5086e-02,  4.8905e-01,\n",
      "          4.2588e-01, -7.3261e-01, -8.4431e-01, -6.6006e-01,  2.3203e-01,\n",
      "         -1.5460e-02,  5.0432e-01,  6.4714e-01,  9.2578e-01,  5.2600e-01,\n",
      "          2.7205e-01,  1.9922e-01, -2.8714e-01, -1.9597e-01, -1.7753e-01,\n",
      "          2.8916e-01, -9.1755e-01, -1.1292e-01, -5.5011e-02, -2.0111e-01,\n",
      "         -1.3816e-03, -9.9132e-01,  3.7112e-02, -7.6209e-01,  7.0454e-02,\n",
      "         -2.6722e-01, -2.1100e-01,  2.7190e-01, -1.4134e-02,  2.6449e-01,\n",
      "          4.3826e-01, -2.9861e-01,  2.6596e-01,  9.3936e-01, -3.8341e-01,\n",
      "         -3.6594e-01, -7.2193e-01, -5.9191e-01, -5.7403e-01,  6.9265e-01,\n",
      "          1.0890e-01, -8.6947e-01, -2.2465e-01, -1.0137e+00,  3.3309e-02,\n",
      "         -8.7776e-02,  1.1023e-01,  1.8055e-01,  5.0420e-01, -1.3070e-01,\n",
      "          8.9311e-01,  2.3209e-01, -3.2037e-01,  1.7507e-01, -4.2179e-01,\n",
      "          5.5186e-01, -1.9668e-01, -2.1603e-01,  3.0748e-01, -2.2597e-01,\n",
      "         -1.0692e-01,  1.4978e-01, -6.3873e-02, -3.7180e-01, -3.5329e-01,\n",
      "          5.8295e-01, -9.0534e-02,  7.5748e-01, -5.8127e-01, -6.2884e-01,\n",
      "          1.1387e+00,  2.8056e-01,  4.8546e-01,  5.4205e-01, -4.9226e-01,\n",
      "         -1.1680e+00,  5.3386e-01,  3.3891e-01,  1.6350e-01,  7.0620e-01,\n",
      "         -4.1253e-01, -1.7355e-01, -1.8794e-01, -4.6745e-01, -3.4118e-01,\n",
      "         -7.4263e-01,  9.6457e-01,  2.3996e-01, -1.1997e+00,  4.5747e-02,\n",
      "          1.4597e-01,  9.7735e-01,  5.0161e-01, -4.7280e-01, -4.3812e-01,\n",
      "          1.2935e+00, -3.7947e-01,  3.1225e-01, -1.1037e+00, -7.6784e-01,\n",
      "         -2.3240e-01, -4.6299e-01, -2.3079e-01, -3.2900e-01,  6.2140e-01,\n",
      "         -2.4845e-01,  5.0841e-01, -9.1261e-01, -4.7626e-02, -5.5806e-01,\n",
      "         -2.6743e-01, -5.1493e-01,  7.0950e-01, -3.0429e-01,  6.0577e-01,\n",
      "         -2.8794e-01,  9.5255e-02,  1.0091e-01,  2.6039e-01,  5.4990e-01,\n",
      "          3.6722e-01, -2.8952e-01,  4.1726e-01,  4.0439e-01,  7.1939e-01,\n",
      "          6.2827e-01,  2.7878e-01, -7.8425e-02, -5.3059e-01, -2.3570e-01,\n",
      "         -2.6832e-01,  4.4069e-02,  1.3077e+00,  1.3645e-01,  7.0275e-01,\n",
      "          2.9420e-01, -4.0832e-01, -1.5572e-01,  1.1712e-01,  3.0592e-01,\n",
      "         -1.1073e+00,  9.4197e-02, -5.7695e-02, -1.2519e+00, -4.2496e-01,\n",
      "          3.7637e-01,  4.2634e-01, -7.0542e-03,  6.6323e-01, -2.5984e-03,\n",
      "          1.6580e-01, -7.8277e-01,  2.4914e-01,  2.7830e-01,  4.4404e-01,\n",
      "          8.4339e-02,  5.9589e-02,  1.2555e-01, -3.5375e-01, -3.1799e-01,\n",
      "          4.2093e-02,  1.0005e+00,  5.6238e-01, -3.7850e-01, -9.7881e-01,\n",
      "         -3.3932e-01, -5.3834e-01,  4.3671e-01,  1.0742e+00,  1.6784e-01,\n",
      "          3.0871e-01, -4.0842e-01, -2.8585e-01, -4.1032e-01, -1.1291e-01,\n",
      "         -1.0591e-01, -3.0923e-01, -4.9291e-01, -5.7396e-01,  1.1249e+00,\n",
      "         -3.6621e-01,  1.2836e+00, -6.4645e-01, -4.1824e-01, -8.2873e-01,\n",
      "         -3.1029e-02,  1.6364e-01, -8.5890e-01,  9.8308e-02,  7.7780e-02,\n",
      "         -5.4682e-01, -2.7253e-01, -4.6962e-01,  6.1926e-04, -2.4209e-02,\n",
      "          4.0453e-01,  6.5936e-02, -1.3688e+00,  5.4113e-01,  2.9190e-01,\n",
      "         -1.3577e+00, -4.6801e-01, -5.6666e-01, -1.1627e-01,  3.8114e-01,\n",
      "         -4.1332e-02, -3.2812e-02, -1.1418e-01,  8.6184e-02, -1.0864e+00,\n",
      "          1.2894e-01, -2.9839e-01,  4.4532e-01,  2.8449e-01,  1.9056e-01,\n",
      "          3.4031e-01, -2.5667e-01,  4.1020e-01,  3.9111e-01, -1.2829e-01,\n",
      "          7.3398e-02, -5.4597e-01, -6.7276e-01,  3.0228e-01, -2.1844e-01,\n",
      "         -7.4348e-01, -1.0079e+00,  3.3390e-01,  2.3411e-01, -5.0900e-01,\n",
      "          1.9065e-02,  3.5557e-01, -4.2414e-01,  4.4286e-02,  2.7023e-01,\n",
      "          2.8467e-01, -1.8487e-02, -2.3261e-01,  4.1997e-01,  1.5555e+00,\n",
      "          2.1987e-01,  1.4932e-01,  6.8828e-01, -1.2803e-01, -1.3222e+00,\n",
      "         -7.4808e-01, -1.6029e-01, -9.0669e-01, -1.1293e+00, -2.1656e-01,\n",
      "          2.2547e-01, -5.7384e-01,  2.1601e-01, -7.1918e-01, -1.9111e-02,\n",
      "         -2.3912e-01, -6.7527e-02,  5.5122e-02,  3.0957e-01, -5.5888e-01,\n",
      "         -1.1692e+00, -7.1893e-01, -9.8873e-02,  1.4792e-01,  3.1281e-01,\n",
      "         -3.7610e-01, -5.6151e-01,  2.2428e-01,  1.3650e-01, -1.0724e-01,\n",
      "          4.1380e-01,  7.5770e-01,  8.7209e-01, -1.9907e-01, -2.6072e-01,\n",
      "          7.3369e-02,  1.8343e-01,  3.9561e-02,  3.6559e-01, -4.7258e-01,\n",
      "          3.0184e-01, -1.3246e-01, -8.5252e-02,  1.1517e-01,  4.3918e-02,\n",
      "          5.9160e-01, -3.4485e-01,  1.6778e-01,  3.9961e-01,  1.2905e+00,\n",
      "         -5.0279e-01,  4.6625e-01,  4.1020e-01, -6.8613e-03,  8.1666e-01,\n",
      "         -2.4673e-01,  6.2270e-01, -4.7346e-01,  6.7195e-01,  2.0798e-01,\n",
      "         -3.3149e-01, -6.1173e-01, -6.1247e-01,  5.7656e-01,  1.2303e-01,\n",
      "          2.9291e-01,  8.0543e-03, -6.3038e-01,  6.2519e-01, -6.7547e-01,\n",
      "         -6.8481e-01,  1.4799e-01,  7.1680e-01, -3.8085e-01, -5.9131e-01,\n",
      "          1.3193e-01,  1.9357e-01,  7.0439e-01, -3.1354e-01, -7.2785e-01,\n",
      "         -2.3820e-01, -8.9396e-01,  5.8650e-01,  4.2031e-01,  3.9190e-01,\n",
      "         -6.3716e-01, -3.9174e-03,  1.5785e+01,  3.9392e-01,  2.6426e-01,\n",
      "          2.5595e-01, -1.4909e-03,  3.9825e-01, -5.3732e-01, -2.8017e-01,\n",
      "         -2.4788e-01,  2.8729e-01,  9.3315e-01,  4.4559e-01,  1.2424e-01,\n",
      "         -1.7294e-01,  2.3384e-01, -3.7899e-01,  3.2744e-01,  5.9754e-01,\n",
      "          9.8826e-01, -9.3159e-01, -1.7849e-01,  4.7269e-01,  2.1066e-01,\n",
      "          6.2395e-01,  3.7484e-01, -6.9125e-01,  3.2238e-01,  1.7284e-01,\n",
      "          1.2742e+00,  2.3307e-01,  1.0348e+00, -3.3603e-01, -2.4224e-02,\n",
      "          6.7127e-01, -5.2831e-01, -6.0381e-01, -4.3221e-01, -5.5281e-01,\n",
      "          8.3185e-01, -8.8622e-03, -4.5722e-01, -4.9459e-02,  4.5134e-01,\n",
      "          1.7980e-01, -5.1194e-01,  6.7300e-01, -3.3808e-01,  3.6266e-01,\n",
      "          2.3421e-01,  4.3120e-01, -2.9064e-01,  2.5052e-01,  6.0421e-01,\n",
      "          3.0375e-02, -7.0191e-02,  1.5344e-01,  4.3141e-01,  8.0208e-02,\n",
      "         -2.7684e-01, -6.4124e-01, -3.8980e-01, -1.6763e-01, -5.1612e-01,\n",
      "          4.0589e-01, -8.3722e-02,  8.0485e-01, -6.0508e-01,  1.0387e+00,\n",
      "         -3.7933e-01,  8.5580e-02, -3.6191e-03, -4.4249e-01, -1.7308e-01,\n",
      "          3.0315e-01, -1.1696e-01, -8.6131e-02, -4.8773e-01, -2.8552e-01,\n",
      "         -3.8866e-02,  5.9959e-01,  7.7657e-01, -1.3995e+00, -1.0918e+00,\n",
      "          1.7009e-01, -4.3378e-01, -2.9375e-01,  9.0037e-01, -1.0156e+00,\n",
      "         -3.4926e-01, -2.9730e-01, -8.2024e-01, -1.2783e+00, -5.2335e-01,\n",
      "         -5.0011e-02, -1.4604e-01, -1.5353e-01,  9.7167e-01, -6.1429e-01,\n",
      "          1.7677e-01, -1.9017e-01,  1.0735e-01, -4.7116e-02,  1.0410e-01,\n",
      "         -2.1386e-01, -2.6251e-01,  4.6675e-01, -7.8831e-02, -2.1215e-01,\n",
      "         -1.2002e-01, -7.4636e-01, -6.0195e-01,  6.7698e-01, -4.3208e-01,\n",
      "         -8.6541e-01, -1.3709e-01, -1.6068e-01, -5.6999e-01, -3.4127e-02,\n",
      "          7.0633e-01, -8.0499e-01,  7.2588e-01, -8.4943e-01, -3.7627e-01,\n",
      "         -8.7698e-02, -8.7122e-01, -2.0831e-03,  3.2839e-01, -1.0273e+00,\n",
      "         -9.5408e-01,  1.2483e-01, -1.9497e-01, -2.3254e-02, -6.3198e-01,\n",
      "          4.2863e-01,  1.2096e+00, -1.7896e-01,  1.5906e+00,  6.9836e-02,\n",
      "          4.2996e-01,  3.2548e-01, -1.3508e-01, -5.0879e-01, -2.0254e-01,\n",
      "         -3.5901e-01,  2.9298e-01,  2.3104e-01,  1.3109e-01,  1.6475e-01,\n",
      "          4.0075e-01,  4.4222e-01,  6.2864e-02,  1.0036e-01, -6.1939e-01,\n",
      "          1.0746e-01, -4.9301e-01,  4.3941e-02, -2.7220e-01, -3.2239e-01,\n",
      "         -2.8317e-01, -6.4950e-02, -1.3666e-01,  4.5407e-01,  2.9311e-01,\n",
      "         -2.2564e-01,  4.4826e-01, -9.8584e-01, -9.0266e-02, -1.0928e-01,\n",
      "         -7.8239e-01, -8.2685e-01, -1.3277e-01, -8.5676e-01,  5.8801e-02,\n",
      "         -2.8377e-01, -5.6514e-01,  8.3005e-01,  3.6283e-01, -3.3907e-01,\n",
      "          6.4271e-01,  2.3590e-01, -7.4493e-01,  4.0968e-02,  2.0327e-02,\n",
      "          2.2977e-01,  2.7541e-01, -1.2958e+00, -1.4441e-01, -2.2154e-01,\n",
      "         -2.8209e-01,  6.7478e-01,  3.6580e-01, -7.0882e-02, -4.7381e-01,\n",
      "         -1.2225e+00,  4.4918e-01, -6.7518e-01,  5.8765e-01, -6.3611e-01,\n",
      "          6.3513e-01,  1.0496e-01,  4.3595e-01, -4.9057e-01,  4.1892e-01,\n",
      "         -9.1753e-01,  9.2797e-02, -5.4207e-02, -2.0536e-01, -1.7780e-01,\n",
      "         -1.0048e-01, -1.2736e-02,  4.0831e-01,  6.0981e-02,  3.4977e-01,\n",
      "         -1.3664e+00, -7.4727e-01,  2.9393e-01, -8.6521e-01, -3.1664e-01,\n",
      "          2.8496e-01, -9.7592e-01, -8.3314e-01, -1.1046e-01, -2.2362e-01,\n",
      "          3.4711e-02, -6.2569e-02,  7.4701e-01,  4.3278e-01, -9.7077e-01,\n",
      "          1.6853e-01,  2.1202e-01, -1.1479e-01, -6.4829e-02,  7.4118e-01,\n",
      "          2.6624e-01, -7.1128e-01, -3.9112e-01,  1.6873e-01, -4.5184e-01,\n",
      "         -1.2346e+00, -1.9524e-01,  1.5574e-01,  6.8685e-01, -1.0186e+00,\n",
      "          4.1627e-01, -2.0576e-01, -5.6359e-01,  1.4139e-01,  6.3858e-01,\n",
      "          4.9971e-02, -2.0279e-01,  8.8637e-01,  7.5924e-01, -1.0493e-01,\n",
      "         -1.4877e-01, -2.7077e-01,  5.1197e-01,  1.8567e-01, -5.8238e-01,\n",
      "          1.0257e+00, -1.2320e+00,  3.6781e-02,  7.8827e-01,  2.9515e-01,\n",
      "          6.2483e-02, -6.9801e-01, -7.0377e-01, -3.6079e-01, -6.4072e-01,\n",
      "         -2.1164e-01, -8.6616e-01, -1.1448e-01,  5.0121e-01,  7.0368e-01,\n",
      "          7.0162e-02,  6.5180e-01,  6.2844e-01,  6.9625e-01,  3.1114e-01,\n",
      "          6.7313e-03, -1.2166e+00,  2.4334e-01,  2.7040e-01,  1.2792e+00,\n",
      "         -3.9232e-01,  6.7432e-01,  1.2722e-01, -1.9952e-01,  1.4495e+00,\n",
      "         -2.0839e-01,  5.4982e-01,  6.6384e-01, -3.6116e-01, -4.2489e-02,\n",
      "         -1.4402e-01,  6.0065e-01,  1.0336e-01,  4.3019e-01,  8.2005e-01,\n",
      "         -2.7516e-01,  3.3798e-01,  3.5251e-01,  3.5900e-01, -3.8323e-01,\n",
      "          2.8744e-01,  1.1352e+00,  1.3809e+00, -4.9925e-01, -4.4822e-03,\n",
      "         -3.8150e-01,  7.3238e-02, -6.4953e-01,  2.2922e-01, -7.0706e-01,\n",
      "          2.7140e-01,  6.2659e-01,  5.3446e-01,  5.3057e-01, -5.5095e-01,\n",
      "         -3.6338e-02,  2.0204e-01,  2.9773e-01, -2.2137e-01, -2.0605e-02,\n",
      "          4.0881e-01, -1.7543e-01,  4.5677e-01, -8.8738e-01, -7.8267e-01,\n",
      "          4.6246e-02, -2.9411e-01, -4.5008e-01, -3.9740e-01,  4.9746e-01,\n",
      "          9.7755e-01,  3.9404e-01, -2.1046e-01, -2.0319e-01, -3.7528e-01,\n",
      "          3.4079e-01, -4.4284e-01, -4.1055e-01, -6.2830e-01,  1.2263e-01,\n",
      "         -5.1032e-01, -1.1375e-01, -1.9502e-01]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# RESEARCH__________________________________________\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')\n",
    "model = AutoModel.from_pretrained('allenai/specter2_base')\n",
    "\n",
    "for item in tqdm(final_chunks):\n",
    "    # Tokenize the sentences\n",
    "    inputs = tokenizer(item[\"sentence_chunk\"], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # Forward pass to get hidden states\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        item[\"embeddings\"] = outputs.last_hidden_state[:, 0, :]  # Take the CLS token (first token) as the embedding\n",
    "\n",
    "# Now `embeddings` is a tensor with shape (3, hidden_size)\n",
    "#print(embeddings.shape) \n",
    "\n",
    "#embeddings_dict = dict(zip(sentences, embeddings))\n",
    "print(final_chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f7b36dd-9fe6-4c36-921c-ead9bb911473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:00<00:00, 11452.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For submission to Heritage Science Real and simulated multispectral imaging of a palimpsest Adam Gibson1, Amy Howe2, Steve Wright2, Martina Sabate Monfort1, Terence Leung1, Angela Warren-Thomas3, Tabitha Tuckett3, Katy Makin3 1. UCL Medical Physics and Biomedical Engineering, Gower St, London WC1E 6BT 2. UCL Library Services, Gower St, London WC1E 6BT 3. UCL Special Collections, Gower St, London WC1E 6BT Abstract. We have recovered undertext from a palimpsest using multispectral imaging. Moreover, we have developed a method for generating simulated multispectral images from previously acquired digitised images of the manuscript, using knowledge of how a colourchecker chart appears in the multispectral images and in the standard digitised images. The ability to identify the undertext was generally better in the real multispectral images, though there were examples of improved identification in the simulated images. However, the method was unsuccessful when applied to freely available images of the Archimedes Palimpsest, suggesting that the mapping between multispectral and standard images must be calculated using the same camera systems that were used to acquire the original images. This approach may allow deeper analysis of previously acquired digitised images using this low-cost simulated multispectral image approach.1. Introduction Palimpsests present an iconic challenge in heritage imaging. A palimpsest is a manuscript page, usually parchment, which has been reused. The older text is removed by either scraping or washing, then the page is often rotated, written over and rebound. The undertext is sometimes visible to the naked eye, but it is faint and is obscured by the overwriting. The imaging challenge is to reveal the undertext.\n",
      "Multispectral imaging was developed as a method for recovering the lost text and proved to be extremely successful when applied to the Archimedes Palimpsest in the late 1990s and early 2000s, revealing works by Archimedes and other authors that were thought to have been lost [1–3]. Multispectral imaging (MSI) uses wavelength-selective illumination to record multiple photographs of an object, showing its response in the near ultraviolet, visible and near infrared spectral range. Filters may increase sensitivity to fluorescence by excluding the illuminating light. As well as being used to reveal underwriting, MSI has also been used to detect other features that are invisible to the human eye such as erased inscriptions, faded text and previous versions of text or paintings [4,5]. MSI typically offers excellent spatial resolution but modest spectral resolution and its spectral data is uncalibrated. Using Delaney’s [6] terminology, MSI is referred to as spectral imaging and offers enhanced visualisation as opposed to imaging spectroscopy which provides calibrated spectra.\n",
      "Various studies have proposed using smartphone cameras for MSI, mainly motivated by the biomedical optics community, with the aim of monitoring haemodynamics by detecting the different spectral characteristics of oxygenated and deoxygenated haemoglobin in blood. Some of these require modifications or additions to the smartphone [7–10] but a new approach by He and Wang [11,12] was able to derive simulated multispectral images from an unmodified smartphone camera. Here, we adapt the method of He and Wang and apply it to generating simulated multispectral images from digitised photographs of a palimpsest. The photographs were acquired using standard digitisation protocols so the method described here could be applied to any digitised images. The technique requires a colourchecker chart which is imaged using a multispectral imaging system and with standard photography. These images are processed to provide a matrix which can convert a red-green-blue (RGB) colour photograph to a simulated multi-wavelength MSI dataset. Multispectral images have a number of advantages over RGB images, and the method described here can only emulate some of these advantages. Cameras used for RGB and MSI photography are both based on silicon sensors so have the same intrinsic spectral range from about 300 nm in the ultraviolet to 1100 nm in the near infrared. However, a commercial RGB camera has its effective spectral range reduced to about 400-800 nm to match that of the eye, by using standard glass lenses that absorb in the ultraviolet and an infrared blocking filter that excludes longer wavelengths. An MSI system maintains sensitivity further into the ultraviolet and infrared than a commercial RGB camera by using UV-transparent glass lenses and omitting the infrared blocking filter. However, more importantly, the wavelength-selective filters supplied with MSI systems are optimised for enhancing sensitivity to fluorescence. A post-processing technique such as the one proposed here could in principle offer a slightly increased spectral range by enhancing the limited sensitivity at the longest and shortest wavelengths, but it cannot offer significant sensitivity to fluorescence without changes to hardware. Here, therefore we distinguish between four methods: (i) full multispectral imaging of a palimpsest; (ii) reduced MSI imaging (excluding fluorescence); (iii) unprocessed RGB photographs; and (iv) simulated MSI images obtained by processing the RGB images with knowledge gained from the colourchecker chart. This method has the potential to offer simulated MSI for institutions that do not have access to MSI systems, and perhaps more importantly, to the many millions of objects that have already been digitised using standard photographic processes. The software was written in Matlab R2023b (The Mathworks USA) and is available ***.\n",
      "2. Methods 2.1 Palimpsest The manuscript is held by UCL Special Collections (MS LAT/15). It is an early 14th Century manuscript volume and was bequeathed to UCL by John Graves (1806-1870), mathematician and Professor of Jurisprudence. It contains a number of different works on mathematics, astronomy and astrology, some by Johannes de Sacro Bosco (c. 1195 – c. 1256) which were some of the first Western European texts to use Arabic numerals [13]. They are written in various hands but bound in a single volume of 33 leaves cut to 217 x 162 mm [14]. Some of the leaves are palimpsests with the undertext visible in the margin of the overtext (Figure 1). Prior to this study, the manuscript was described in the catalogue record [15] as “a\n",
      "palimpsest with the erased text (a 13th century Italian hand?)still visible most clearly on folios 11r- 14r and 19v-20v”.2.2 Digitisation The book was fully digitised in December 2023 and is available from UCL’s digital collections catalogue as part of the Manuscript Collections listed as “Sacrobosco, Sphera, Algorismus and other tracts” [15]. The photographs are 2696 x 3512 pixels and were acquired with a Canon EOS 6D camera with a 50 mm lens at f/8 and ISO100 using a copystand with white tungsten lighting. All images were acquired and colour corrected following UCL Special Collections’ digitisation protocols. Figure 1: MS LAT/15, showing the recto (left) and verso (right) of folio 13. The horizontal overtext is dominant, but the partially erased undertext can be seen running vertically in the margins. Red rubrication is clearly visible, as are some damage and repairs.2.3 Multispectral imaging The UCL Multispectral Imaging system was supplied by R B Toth Associates (USA). It is based around a PhaseOne XF camera (PhaseOne, Denmark) with an IQ3 digital back which generates 11608 × 8708 pixel, monochrome, 16 bit images, and a 120 mm apochromatic lens. Two LED lighting panels (Equipoise Imaging LLC, USA) provide illumination at 16 discrete wavelengths from 365 nm to 940 nm and a filter wheel, equipped with longpass filters (Thorlabs, USA), can exclude the illuminating light to enhance sensitivity to fluorescence. The system is controlled using Spectral XV v2.1.0.0 (Equipoise Imaging LLC, USA) and Capture One v12.0.2.13 (PhaseOne, Denmark) software.\n",
      "Flat field images were collected under the same illumination and camera conditions, and MSI images were flattened and converted to 16-bit TIF images in Matlab R2023b (The Mathworks, USA). The book was held in a custom-built cradle in which the sheet being imaged was laid flat and the opposing sheet was held out of the field of view. A thin sheet of black paper underneath the sheet being imaged ensured that no light penetrated beyond that sheet.2.4 Simulated multispectral imaging Full details of the original method of simulating multispectral images are provided in the work of He and Wang [11,12]. The underlying concept is that if you know how the same broad gamut of colours appears in a standard photograph and in an MSI image stack, you can use that information to convert between the RGB image and the MSI stack. To provide the gamut of colours, we imaged a Digital SG ColorChecker chart (Calibrite LLC, USA) which has a 10 x 14 grid of coloured squares. The outer squares are repeated greyscale values so we focussed on the inner 8 x 12 grid of discrete colours. This ColorChecker has an expanded range of colours compared to previous versions, with slightly better near infrared and ultraviolet coverage. We did, however, notice that two squares fluoresced strongly under ultraviolet illumination, reinforcing our decision to exclude fluorescence from the simulated multispectral image stack. Following the terminology and derivation of He and Wang [11], the response of the RGB camera to a particular colour can be written as 𝑉ோீ஻= ∫𝑚ோீ஻(𝜆) 𝛾(𝜆) 𝑑𝜆 (1) where VRGB is the response of the red, green or blue channel and consists of mRGB(λ), the combined response of the illumination, filters and the camera sensor in a wavelength channel all of which are dependent on an arbitrary number of wavelengths λ, and γ(λ), the spectral reflection of the colour of the object, summed over all wavelengths. The ColorChecker has 96 distinct coloured squares, so with three channels, 𝑉ோீ஻ can be written in matrix form as V which is the 3 × 96 matrix containing the RGB values for each square on the ColorChecker which we measure. We then repeat this for the 16 MSI images in an image stack to obtain V’ which is the measured 16 × 96 matrix of responses of the MSI system to each square on the ColorChecker. Using a process called Wiener estimation which finds a reconstruction matrix W that minimises the mean square error between the RGB and MSI images, He and Wang showed that W = 〈𝐕′𝐕𝐓〉〈𝐕𝐕𝐓〉ି𝟏 (2) where T denotes the transpose operator. This algorithm was implemented in Matlab R2023b (The Mathworks USA), the output of which is a single 16 × 3 W matrix. This W matrix can be applied to an RGB image pixel-by-pixel to simulate each of 16 pseudo-multispectral images.\n",
      "The original digitised images were downloaded and inspected. Multispectral images were acquired of 13 sheets using the UCL Multispectral Imaging System. Simulated MSI images were generated using the process described in section 2.4, and using a W matrix that mapped between the Canon EOS 6D camera used for digitisation and the PhaseOne system used for MSI.2.5 Post processing\n",
      "Various post-processing algorithms have been applied to multispectral images [4]. Sometimes no post-processing is necessary and the required information can be obtained by inspection of the MSI images. Often image subtraction can remove features that are common, highlighting differences between two images. However, the most common basic post processing step is probably Principal Component Analysis [4] (PCA). This generates a sequence of ‘principal component images’ which contain decreasing amounts of information. Typically, the first few principal component images contain the most information and if three are selected, they can be assembled into a false colour image by assigning them to the RGB channels of a colour image. Other, more sophisticated, post processing methods are available [4,5], some of which take advantage of previously measured spectra, and, increasingly, modern methods taken from deep learning such as convolutional neural networks are being applied to heritage imaging [16,17]. Here, we aim to examine the performance of the simulated MSI algorithm and so perform limited post-processing using image subtraction and PCA. All of the image processing was carried out in Matlab R2023b (The Mathworks, USA) using the Image Processing toolbox and Hyperspectral Viewer.3. Results 3.1 Real and simulated multispectral imaging Thirteen sheets were imaged and analysed, but here, to keep the description manageable, one sheet (MS LAT/15 f20v) was chosen for analysis. Figure 2(a) is the digitised image of the full sheet, rotated relative to Figure 1 so that the undertext is horizontal and reads left to right.24 MSI images were acquired. Eight used filters, so the remaining reflectance-only dataset consisted of 16 MSI images at 365, 385, 410, 420, 450, 480, 510, 530, 550, 600, 630, 640, 660, 740, 850 and 940 nm. Again, to keep the visualisation manageable, four images (365, 630, 640 and 940 nm) were selected for display and are shown in Figure 2 along with the corresponding simulated images.\n",
      "It is hard to read details of the script on Figure 2, so an area at the top right of the sheet that shows the undertext was chosen, and the corresponding cropped and enlarged images are shown in Figure 3.\n",
      "Figure 2: Real and simulated multispectral images of MS LAT/15 f20v. (a) is the original digitised image. (b)-(e) are four selected reflectance MSI images and (f)-(l) the corresponding simulated MSI images.\n",
      "Figure 3: Cropped areas of the same real and simulated multispectral images of MS LAT/15 f20v shown in Fig 2. As in Fig 2, (a) is the original digitised image. (b)-(e) are four reflectance MSI images and (f)-(l) the corresponding simulated MSI images. There is a clear difference in the real multispectral images (Figure 2(b-e) and Figure 3 (b-e)), with the iron gall ink used for the overtext and undertext offering increased contrast compared to the parchment at shorter wavelengths. At intermediate wavelengths, the overtext retains contrast, but the contrast with the undertext appears to diminish. There is little contrast for either text at the longer wavelengths. This difference in behaviour between the two wavelengths gives some confidence that MSI should be able to distinguish between them. There is less difference between the simulated images (Figure 2(f-i) and Figure 3 (f-i)). The contrast can be seen to reduce from short to intermediate wavelengths as in the real images, but the longer 940 nm image is almost indistinguishable from the mid-wavelength images. This is presumably due to the low sensitivity of the camera used for digitisation at these wavelengths, as well as the colourchecker chart having little contrast in the near infrared.\n",
      "3.2 Post processing PCA was carried out on the flattened MSI images. The first few PCA images obtained by processing the real MSI images, when ordered by variance, showed various combinations of features, including the text, rubrication, and features from the parchment itself. The strongest signal from the undertext appeared in the eighth principal component image. The sign of a PCA image is arbitrary, meaning that the undertext can appear as black-on-white or as white-on-black. The latter were adjusted by taking the complement of the images so the undertext always appeared dark. The contrast was enhanced by manipulating brightness and contrast. When the simulated MSI images were analysed, the undertext was clearest in the first principal component image and there was little detail other than the rubrication from image 3 onwards. Figure 4(a) shows the digitised image of the same area as shown in Figure 3. Figures b and d show the first and eighth PCA image after processing the full MSI image set, and c and d show the corresponding processed simulated images. Further analysis such as rotating the PCA images between the RGB channels and adjusting contrast and brightness can further enhance the discrimination, but here we were interested in the performance of the simulated MSI images and so chose to minimise the post processing. Figure 4: Cropped areas of the same real and simulated multispectral images of MS LAT/15 f20v shown in Fig 3. As in Fig 3, (a) is the original digitised image. (b) and (d) are the first and eighth PCA images obtained from the full MSI image data set and (c)-(e) are the PCA images acquired from the corresponding simulated MSI images.4. Discussion\n",
      "Full MSI imaging was able to reveal the undertext in greater detail than was previously available, and the text has now been identified as having two different sources. Part of the undertext has been identified as an abridged version of Breviarium Historiae Romanae, a history of the Roman Empire by Eutropius (fl.363–387), thought to have been written in the 13th Century. The undertext on some of the other sheets comes from Julius Caesar’s Commentarii de Bello Gallico (Commentaries on the Gallic War) book 7, section 15. For example, the four lines in the crop shown in Figure 4 (seen most clearly in 4d) are: (1) probata uno die amplius XX urbes Biturigum incen- (2) -duntur. Hoc idem fit in reliquis civitatibus: in omnibus partibus (3) incendia conspiciuntur; quae etsi magno cum dolore omnes (4) ferebant, tamen hoc sibi solati proponebant, { } explorata vic[toria] These were not legible in the digitised image, or when visually examining the sheet even after the text had been identified. The two sources are from the undertext of two different pamphlets written by different scribes, and the undertexts themselves are in different hands. It is interesting that the undertexts are both Roman histories but it is not yet clear whether this is significant. The simulated images behaved as expected, with reasonable reproduction of the central wavelengths and less reliable reproduction at the extremes. There appears to be some modest benefit to examining the simulated MSI images compared to the digitised images and this benefit varied from one sheet to another, but generally the real MSI images gave the best contrast. Figure 5 shows a curious exception to this, where a cropped version of sheet f13r is shown. Figure 5(a-c) show the first three PCA images of the MSI image stack excluding the images that used filters to detect fluorescence, after contrast enhancement. They were then assigned to the red, green and blue channels of a colour image to generate Fig 5(d). Similarly, Fig 5(e-g) show the first three PCA images of the full MSI image stack with 5h being the corresponding false colour image. And Fig 5 (i-k) show the PCA images from the simulated images, that generate the false colour image in Fig 5(l).\n",
      "In this case, the simulated images unexpectedly appeared to show the greatest contrast to the undertext.\n",
      "Figure 5: Images of MS LAT/15 f13r, processed to show the PCA images from the full MSI image stack (a-c), the full stack excluding images that show fluorescence (e-g) and from the simulated images (i- k). Corresponding false colour images are shown in 5d, 5h and 5l.\n",
      "5. Application to Archimedes Palimpsest The photographs of the palimpsest and ColorChecker described in section 3 were taken using the same camera, lens, lighting system and processing pipeline. It would be useful to know how far this can be generalised – can a W matrix calculated for a particular photography set-up be applied to photographs taken using a different set-up?This will not give optimal results, but given that PCA is non-quantitative, an approximate rendering of the multispectral images might still offer some useful information. Note that if more sophisticated, calibrated or quantitative image analysis was to be attempted, we would not consider using an unmatched W matrix. Indeed, we advise against using simulated multispectral images for any extended analysis. We examine the hypothesis that a generic W matrix can be used in some circumstances by reanalysing photographs taken of the Archimedes Palimpsest between 1999 and 2008 ([1–3]), downloaded from https://www.archimedespalimpsest.net/. Previous work has demonstrated that image processing including PCA can enhance visibility of the undertext [3]. Images of many sheets are available. We chose to analyse 168r and 165v which were photographed together and used for more additional processing than most other pairs of sheets. The RGB image “168r-165v_Arch30v-29r_Sinar_TNGSTN_01_pack8.tif” was downloaded and processed as above, using the W matrix obtained in 3.1, which had been calculated for a different camera and MSI system. The stack of simulated MSI images were very similar, and needed a close examination to perceive difference between them which were only really visible in the part of the image that showed the ColorChecker squares (Figure 6). Neither the simulated multispectral images nor PCA were able to reveal any detail of the underwriting that wasn’t visible under standard lighting conditions.\n",
      "Figure 6: Images of the Archimedes Palimpsest. (a-d) are four original images taken from archimedespalimpsest.net, licenced under CC 3.0 (with Unported Access Rights). (e-h) are simulated MSI images, taken at the closest available wavelengths 6. Conclusion We have successfully used standard MSI imaging with minimal image processing to reveal two different works making up the undertext of a palimpsest. This new information will be added to the catalogue records and provide deeper context when this book is used in teaching. This work shows that simulated multispectral images can be generated from standard digitised images taken using an unmodified consumer camera. However, simulated images do not include all\n",
      "the information available in a full MSI dataset and are generally less helpful for analysis. Despite this, in some instances (as in Figure 5), they might provide improved contrast compared to standard photographs and may sometimes even be competitive with a full MSI dataset. This approach potentially allows millions of previously digitised images to be reanalysed, potentially allowing additional analysis to be undertaken at modest cost, without the need for MSI equipment. It appears that the W matrix must be reacquired for a photographic system that is as close as possible to that used for the initial digitisation. The method could even be applied to images taken with a mobile phone, potentially by non-expert users and members of the public. However, data with an unmodified camera will never offer the broad spectral information of a full MSI system or access to images that enhance fluorescence, so the range of applications will remain limited. We would not recommend using simulated MSI image for any quantitative analysis without thorough validation. Simulated or real MSI provides an extremely rich dataset. We obtained 24 images (at 16 wavelengths with and without filters) and viewed numerous combinations of these, by subtracting image pairs and examining PCA images. It is not at all clear how best to present these images to a reader who can interpret them, and we believe anecdotally that the best preferred (or combination of images) may even vary between observers, meaning that there might not be a single best solution. This work relied on previously acquired and openly accessible data, so acts as a demonstration of the importance of making well documented open data available to researchers.7. Acknowledgements We are grateful to Dr Bill Christens-Barry for advice on how best to image a palimpsest.8. References 1.\n",
      "Easton RL, Knox KT, Christens-Barry WA. Multispectral imaging of the Archimedes palimpsest. Applied Imagery Pattern Recognition Workshop, 2003 Proceedings 32nd [Internet]. IEEE; 2003 [cited 2017 Apr 25].p. 111–6. Available from: http://ieeexplore.ieee.org/abstract/document/1284258/ 2. Christens-Barry WA, Boydston K, France FG, Knox KT, Easton Jr RL, Toth MB. Camera system for multispectral imaging of documents. International Society for Optics and Photonics; 2009.p. 724908.3. Easton RL, Christens-Barry WA, Knox KT. Spectral image processing and analysis of the Archimedes Palimpsest.2011 19th European Signal Processing Conference.2011.\n",
      "p. 1440–4.4. Jones C, Duffy C, Gibson A, Terras M. Understanding Multispectral Imaging of Cultural Heritage: Determining Best Practice in MSI Analysis of Historical Artefacts. Journal of Cultural Heritage.2020;45:339–50.5. Striova J, Dal Fovo A, Fontana R. Reflectance imaging spectroscopy in heritage science. Riv Nuovo Cim.2020;43:515–66.\n",
      "6. Delaney JK, Ricciardi P, Glinsman LD, Facini M, Thoury M, Palmer M, et al. Use of imaging spectroscopy, fiber optic reflectance spectroscopy, and X-ray fluorescence to map and identify pigments in illuminated manuscripts. Studies in Conservation.2014;59:91–101.7. Frank J. Bolton, Amir S. Bernat, Kfir Bar-Am, David Levitz, Steven Jacques. Portable, low-cost multispectral imaging system: design, development, validation, and utilization. Journal of Biomedical Optics.2018;23:121612.8. Kim S, Kim J, Hwang M, Kim M, Jin Jo S, Je M, et al. Smartphone-based multispectral imaging and machine-learning based analysis for discrimination between seborrheic dermatitis and psoriasis on the scalp. Biomed Opt Express.2019;10:879–91.\n",
      "9. Stuart MB, McGonigle AJS, Davies M, Hobbs MJ, Boone NA, Stanger LR, et al. Low-Cost Hyperspectral Imaging with A Smartphone. Journal of Imaging.2021;7.10. Daukantas P. Hyperspectral Imaging Meets Biomedicine. Opt Photon News.2020;31:32–9.11. He Q, Wang R. Hyperspectral imaging enabled by an unmodified smartphone for analyzing skin morphological features and monitoring hemodynamics. Biomed Opt Express.2020;11:895–910.12. He Q, Li W, Shi Y, Yu Y, Geng W, Sun Z, et al.\n",
      "SpeCamX: mobile app that turns unmodified smartphones into multispectral imagers. Biomed Opt Express.2023;14:4929–46.13. Valleriani M, Ottone A, editors. Publishing Sacrobosco’s De sphaera in Early Modern Europe: Modes of Material and Scientific Exchange [Internet]. Cham: Springer International Publishing; 2022 [cited 2024 Jan 18]. Available from: https://link.springer.com/10.1007/978-3-030-86600-6 14. Furlong G. Treasures from UCL [Internet]. UCL Press; 2015 [cited 2021 Nov 6]. Available from: https://ucldigitalpress.co.uk/Book/Article/2/9/0/ 15. Sacrobosco J. “Sphera”, “Algorismus” and other tracts [Internet]. UCL Special Collections.c1320. Available from: https://ucl.primo.exlibrisgroup.com/permalink/44UCL_INST/155jbua/alma9931613773504761 16.\n",
      "Kleynhans T, Schmidt Patterson CM, Dooley KA, Messinger DW, Delaney JK. An alternative approach to mapping pigments in paintings with hyperspectral reflectance image cubes using artificial intelligence. Heritage Science.2020;8:84.17. Liu L, Miteva T, Delnevo G, Mirri S, Walter P, de Viguerie L, et al. Neural Networks for Hyperspectral Imaging of Historical Paintings: A Practical Review. Sensors.2023;23:2419.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(final_chunks):\n",
    "    # Tokenize the sentences\n",
    "    print(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4cc673-1303-4431-97da-9b157e397146",
   "metadata": {},
   "source": [
    "#### 2.4. Similarity Search\n",
    "\n",
    "# EXPLAIN\n",
    "Note:\n",
    "We want to: search for a query (e.g. \"macronutrient functions\") and get relevant info from textbook.\n",
    "\n",
    "Steps to do this:\n",
    "1. Define query string\n",
    "2. Turn query string into embedding\n",
    "3. Perform dot product or cosine similarity function between the text embeddings and the query embedding\n",
    "4. Sort the results form 3 in descending order\n",
    "\n",
    "\n",
    "Note: to use dot product for comparison ensure both vector sizes are of the same shape and tensors/vectors are in the same data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de236578-91b4-4a50-8634-26682fef3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define query\n",
    "query = \"good foods for protein\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# 2. Embed the query\n",
    "# Note: it's important to embed your query with the SAME MODEL as embeddings\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True).to(device)\n",
    "\n",
    "# 3. Get similarity scores\n",
    "# with dot product (use cosine similarity if outputs of model aren't normalised)\n",
    "from time import perf_counter as timer\n",
    "\n",
    "start_time = timer\n",
    "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "end_time = timer\n",
    "\n",
    "# 4. Get the top-k results (we keep top 5)\n",
    "top_results_dot_product = torch.topk(dot_scores, 5)\n",
    "top_results_dot_product"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
