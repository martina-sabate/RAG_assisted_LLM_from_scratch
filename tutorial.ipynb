{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa252f14-c3d7-462c-8a63-b733136636d6",
   "metadata": {},
   "source": [
    "# Step 1: Setting up the environment\n",
    "\n",
    "#### To create and activate a virtual environment, run on your terminal:\n",
    "\n",
    "**Windows:**\n",
    "\n",
    "\n",
    "```\n",
    "python -m venv venv\n",
    "```\n",
    "```\n",
    "venv\\Scripts\\Activate\n",
    "```\n",
    "\n",
    "**macOS/Linux:**\n",
    "\n",
    "```\n",
    "python3 -m venv venv\n",
    "```\n",
    "\n",
    "```\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "#### After the environment is activated, install the requirements:\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5368ccd1-8969-49b2-93c0-f3af7b30e185",
   "metadata": {},
   "source": [
    "# Step 2: Pre-processing the document\n",
    "\n",
    "#### What you will need:\n",
    "1. PDF Document (skin cancer detection paper in my case)\n",
    "2. Embedding model (I am using Sentence-Bert)\n",
    "\n",
    "As described in the Readme file, the aim of this tutorial is to build a RAG-assisted LLM that can retrieve information from research papers, helping students and researchers get a quicker understanding of the paper. This example will refer to a pdf titled *Real and simulated multispectral imaging of a palimpsest*. This paper is not yet publicly available since I am currently in the process of publishing, but feel free to use any paper you would like to retrieve information from. The same steps will apply to any paper / book. All you need to do is download it in pdf format and add your file path to the variable 'path'.\n",
    "\n",
    "To pre-process the PDF document, we will use an embedding model. \n",
    "\n",
    "#### ⚠️ Now, what does “embedding” mean in AI?\n",
    "\n",
    "In this tutorial, we are trying to get our AI model to understand a paper (complex text data). The problem is, our model can only understand numbers. That is where embeddings come in.\n",
    "\n",
    "> An embedding is a way of representing complex data (like words or images) as a list of numbers — called a vector — in such a way that the relationships between items are preserved.\n",
    "\n",
    "\n",
    "#### Let’s dive into that:\n",
    "\n",
    "Think of each item (a word, an image, a sentence) as a point in space - a location on a map. The closer two points are, the more related their meanings are.\n",
    "\n",
    "For example:\n",
    "\n",
    "- The word “cat” will be close to “dog”.\n",
    "\n",
    "- The word “car” will be far away from “banana”.\n",
    "\n",
    "That’s because in real life, cats and dogs are similar (both animals, pets), while a car and a banana are not.\n",
    "\n",
    "So embeddings help us map meaning into a mathematical space.\n",
    "\n",
    "#### 🧐 What is an embedding model?\n",
    "\n",
    "An embedding model is an AI model that has learned how to take something complex — like a sentence — and turn it into a vector (a list of numbers) that captures its meaning.\n",
    "\n",
    "Different embedding models specialize in different kinds of data. The table below shows some examples of open-source embedding models for different use cases:\n",
    "\n",
    "\n",
    "| Data Type      | Embedding model examples               | What do they capture?                            |\n",
    "|----------------|----------------------------------------|--------------------------------------------------|\n",
    "| Words          | Word2Vec, GloVe, FastText              | Word meanings, analogies, syntactic similarity   |\n",
    "| Sentences/Text | Sentence-BERT (SBERT), Instructor, E5  | Semantic similarity between sentences/documents  |\n",
    "| Images         | DINO, OpenCLIP                         | Visual concepts, cross-modal (image-text) meaning|\n",
    "| Audio          | Wav2Vec 2.0, Whisper                   | Speech content, audio features                   |\n",
    "| Code           | CodeBERT, GraphCodeBERT                | Code syntax and semantics                        |\n",
    "  \n",
    "\n",
    "In this tutorial we are looking to read PDF documents, therefore, we need a model that embeds data based on semantic similarity. I have chosen Sentence-BERT, but it is interchangable for any sentence / text embedding model. Once you have build your own RAG-assisted LLM, you can experiment with different models and decide what works best for you\n",
    "\n",
    "Note that embedding models do not exactly embed words or sentences, they embed tokens.\n",
    "\n",
    "#### ❓ What is a token?\n",
    "\n",
    "A token is a smallest unit of input that a language model (like GPT or BERT) understands.\n",
    "\n",
    "In most modern NLP systems, tokens are not exactly words — they can be:\n",
    "\n",
    "- A whole word (hello)\n",
    "\n",
    "- A subword (un, believ, able)\n",
    "\n",
    "- A punctuation mark (!, .)\n",
    "\n",
    "- Even just a few characters (Th, is)\n",
    "\n",
    "Think of a token as a \"chunk\" of text — a building block the model processes one at a time.\n",
    "\n",
    "> **Example**\n",
    "> \n",
    "> Sentence: \"This is amazing!\" might be tokenized as:\n",
    "> \n",
    "> ['This', ' is', ' amazing', '!']\n",
    "\n",
    "\n",
    ".\n",
    "> Note that, on average in English text, 1 token is equal to 4 characters.\n",
    "\n",
    "\n",
    "#### Now that we know how the data pre_processing will work, let's get started. \r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2159f5-13b3-4291-b35c-664f452d28ce",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 2.1. Importing the relevant modules, getting the PDF we want to read, and extracting text from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91f0ab73-6e6a-4fa7-9367-347cf32452cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file 'G:\\My Drive\\feines 2025\\MS Imaging paper\\to submit.pdf' exists.\n",
      " \n",
      "'For submission to Heritage Science Real and simulated multispectral imaging...'\n"
     ]
    }
   ],
   "source": [
    "# Import relevant modules\n",
    "import fitz\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Get PDF path (change this variable to your pdf path)\n",
    "#____________________________________________________________________\n",
    "path = r\"G:\\My Drive\\feines 2025\\MS Imaging paper\\to submit.pdf\"\n",
    "#____________________________________________________________________\n",
    "\n",
    "# Check that the path exists\n",
    "if os.path.exists(path):\n",
    "    print(f\"PDF file '{path}' exists.\")\n",
    "else:\n",
    "    print(f\"PDF file '{path}' does not exist\")\n",
    "\n",
    "# Open the PDF file\n",
    "paper = fitz.open(path)\n",
    "    \n",
    "# Define a helper function to extract text from the pdf\n",
    "def extract_text(paper: fitz.Document):\n",
    "  \"\"\"Applies formatting to the PDF textand stores the content in a list of dictionaries\n",
    "  Inputs: \n",
    "      paper (fitz.Document): PDF document\n",
    "  Outputs: \n",
    "      output (list[dict]): List of dictionaries containing the formatted extracted text from each PDF page \n",
    "      and the corresponding page number\n",
    "  \"\"\"\n",
    "\n",
    "  # Define an empty list that will be filled with the extracted text\n",
    "  output = []\n",
    "\n",
    "  for page_number, page in enumerate(paper):\n",
    "    paper_text = page.get_text()\n",
    "    paper_text = re.sub(r'\\s+', ' ', paper_text).strip() # removes any \\n or white spaces\n",
    "    output.append({\"page_number\": page_number,       \n",
    "                   \"text\": paper_text\n",
    "                   })\n",
    "  return output\n",
    "\n",
    "# Check that the helper function works as expected by printing the first sentence\n",
    "output = extract_text(paper=paper)\n",
    "display = output[0][\"text\"][:75]\n",
    "print(\" \")\n",
    "print(f\"'{display}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13288faf-a606-4933-bd7e-10dedd482feb",
   "metadata": {},
   "source": [
    "#### 2.2. Segmenting the extracted text\n",
    "\n",
    "First of all, we will use the Natural Language Processing (NLP) library **SpaCy** to divide our extracted text in sentences.\n",
    "\n",
    "This is due to the fact that embedding models cannot process an infinite number of tokens, therefore we need to limit the number of tokens by chunking the text into groups of sentences.\n",
    "\n",
    "For this tutorial I have split the text in chunks of 8 sentences, although this number is arbitrary. Feel free to experiment and decide what works best with your model. What is the criteria to keep in mind:\n",
    "1. Smaller groups of text will be easier to inspect, making it easier to filter content\n",
    "2. The text chunks need to fit into our embedding model's context window\n",
    "3. Chunks too large will make the context that will be passed to the LLM too vague\n",
    "4. Chunks too short might leave out information that is also relevant / be misleading\n",
    "5. We want to find a chunk size so that the context passed to the LLM will be specific and focused\n",
    "\n",
    "#### Please note that content after this line is still being finalised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe39fd69-bacc-46d4-966d-8891123d7263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This cell tests that the NLP engine (SpaCy) is detecting sentences as expected, and that the split_into_segments function splits the text in groups of 8 sentences. \n",
      "\n",
      "[SpaCy., is an NLP., library., that splits., text., into sentences., Let's., test., it.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['SpaCy.',\n",
       "  'is an NLP.',\n",
       "  'library.',\n",
       "  'that splits.',\n",
       "  'text.',\n",
       "  'into sentences.',\n",
       "  \"Let's.\",\n",
       "  'test.'],\n",
       " ['it.']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the English language model from spaCy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Initialize the language processor\n",
    "nlp_engine = English()\n",
    "\n",
    "# Add sentence segmentation capability\n",
    "# This NLP engine (SpaCy) breaks text into individual sentences\n",
    "nlp_engine.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Verify that sentence segmentation works correctly\n",
    "sample_text = nlp_engine(\"SpaCy. is an NLP. library. that splits. text. into sentences. Let's. test. it.\")\n",
    "assert len(list(sample_text.sents)) == 9\n",
    "print(f\"This cell tests that the NLP engine (SpaCy) is detecting sentences as expected, \\\n",
    "and that the split_into_segments function splits the text in groups of \\\n",
    "{sentences_per_segment} sentences. \\n\")\n",
    "print(list(sample_text.sents))\n",
    "\n",
    "# Define how many sentences should be in each text segment\n",
    "sentences_per_segment = 8\n",
    "\n",
    "# Function to divide sentences into manageable segments\n",
    "def split_into_segments(sentence_collection: list[str], \n",
    "                   segment_length: int = sentences_per_segment):\n",
    "    \"\"\"\n",
    "    Divides a collection of sentences into segments of specified length\n",
    "    \n",
    "    Args:\n",
    "        sentence_collection: List of sentences to divide\n",
    "        segment_length: Maximum number of sentences per segment\n",
    "        \n",
    "    Returns:\n",
    "        List of sentence segments\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    for i in range(0, len(sentence_collection), segment_length):\n",
    "        segments.append(sentence_collection[i:i + segment_length])\n",
    "    return segments\n",
    "\n",
    "# Test segmentation function\n",
    "test_sentences = [sent.text for sent in sample_text.sents]\n",
    "split_into_segments(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5885364d-0169-4f97-8898-0b27d50b5b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 14/14 [00:00<00:00, 189.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process each page in the PDF document\n",
    "for item in tqdm(output):\n",
    "    # Identify sentences in the document text\n",
    "    pdf_processed = nlp_engine(item[\"text\"])\n",
    "    \n",
    "    # Store sentences as strings\n",
    "    item[\"sentences\"] = [str(sent) for sent in pdf_processed.sents]\n",
    "    \n",
    "    # Create sentence segments and count them\n",
    "    item[\"sentence_segments\"] = split_into_segments(\n",
    "        sentence_collection=item[\"sentences\"],\n",
    "        segment_length=sentences_per_segment\n",
    "    )\n",
    "    \n",
    "    # Record metadata\n",
    "    item[\"segment_count\"] = len(item[\"sentence_segments\"])\n",
    "    item[\"sentence_count\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b52ed-48b0-4211-8d83-3a73dc5f0efc",
   "metadata": {},
   "source": [
    "Once the document has been divided in groups of sentences (segments or chunks), we will display a sample of one page in the PDF, to ensure this task was performed correctly. Additionally, we will print statistics on the average number of sentences and sentence segments per page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "705f1166-6672-4924-ab8b-876a1cb8f348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Sample (shortened for display):\n",
      "{ 'page_number': 6,\n",
      "  'segment_count': 2,\n",
      "  'sentence_count': 10,\n",
      "  'sentence_segments': [ [ 'Figure 3: ...',\n",
      "                           'As in Fig ...',\n",
      "                           'b)-(e) are...',\n",
      "                           'There is a...',\n",
      "                           'At interme...',\n",
      "                           'There is l...',\n",
      "                           'This diffe...',\n",
      "                           'There is l...'],\n",
      "                         ['The contra...', 'This is pr...']],\n",
      "  'sentences': [ 'Figure 3: ...',\n",
      "                 'As in Fig ...',\n",
      "                 'b)-(e) are...',\n",
      "                 'There is a...',\n",
      "                 'At interme...',\n",
      "                 'There is l...',\n",
      "                 'This diffe...',\n",
      "                 'There is l...',\n",
      "                 'The contra...',\n",
      "                 'This is pr...'],\n",
      "  'text': 'Figure 3: ...'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Print a random sample from the output\n",
    "print(\"\\nRandom Sample (shortened for display):\")\n",
    "sample_item = random.sample(output, k=1)[0]  # Get one random item\n",
    "\n",
    "# Create a copy of the sample to modify for display\n",
    "display_sample = sample_item.copy()\n",
    "\n",
    "# Truncate the text to first 10 characters\n",
    "if \"text\" in display_sample:\n",
    "    display_sample[\"text\"] = display_sample[\"text\"][:10] + \"...\" if len(display_sample[\"text\"]) > 10 else display_sample[\"text\"]\n",
    "\n",
    "# Truncate each sentence to first 10 characters\n",
    "if \"sentences\" in display_sample:\n",
    "    display_sample[\"sentences\"] = [s[:10] + \"...\" if len(s) > 10 else s for s in display_sample[\"sentences\"]]\n",
    "\n",
    "# Truncate each sentence segment to first 10 characters\n",
    "if \"sentence_segments\" in display_sample:\n",
    "    # For each segment in the list of segments\n",
    "    display_sample[\"sentence_segments\"] = [\n",
    "        # For each sentence in the segment\n",
    "        [s[:10] + \"...\" if len(s) > 10 else s for s in segment]\n",
    "        for segment in display_sample[\"sentence_segments\"]]\n",
    "\n",
    "# Print the modified sample\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(display_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1e302d73-72a5-4abb-ba0e-5b30607c228a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document statistics:\n",
      "Total pages: 14\n",
      "Total sentences: 248\n",
      "Average sentences per page: 17.7\n",
      "Average segments per page: 2.6\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract the data for plotting\n",
    "sentence_counts = [item[\"sentence_count\"] for item in output]\n",
    "segment_counts = [item[\"segment_count\"] for item in output]\n",
    "\n",
    "# Calculate averages\n",
    "avg_sentence_count = np.mean(sentence_counts)\n",
    "avg_segment_count = np.mean(segment_counts)\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# First chart: Sentences per page\n",
    "ax1.bar(range(len(sentence_counts)), sentence_counts, color='skyblue')\n",
    "ax1.axhline(y=avg_sentence_count, color='red', linestyle='--', \n",
    "            label=f'Average: {avg_sentence_count:.1f}')\n",
    "ax1.set_title('Number of Sentences per Page')\n",
    "ax1.set_xlabel('Page Index')\n",
    "ax1.set_ylabel('Sentence Count')\n",
    "ax1.legend()\n",
    "\n",
    "# Second chart: Sentence segments per page\n",
    "ax2.bar(range(len(segment_counts)), segment_counts, color='lightgreen')\n",
    "ax2.axhline(y=avg_segment_count, color='red', linestyle='--', \n",
    "            label=f'Average: {avg_segment_count:.1f}')\n",
    "ax2.set_title('Number of Sentence Segments per Page')\n",
    "ax2.set_xlabel('Page Index')\n",
    "ax2.set_ylabel('Segment Count')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.savefig('document_analysis.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Document statistics:\")\n",
    "print(f\"Total pages: {len(output)}\")\n",
    "print(f\"Total sentences: {sum(sentence_counts)}\")\n",
    "print(f\"Average sentences per page: {avg_sentence_count:.1f}\")\n",
    "print(f\"Average segments per page: {avg_segment_count:.1f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf2e5d1-faab-4b1e-9633-26a7695e0741",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "The final step in section _'2.2. Segmenting the extracted text'_ is to join all sentences within 1 segmented, converting the segment back into a paragraph. Each joined paragraph will be treated as a document entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93d226e6-ed3c-4cba-911f-1a00f3f04b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████| 14/14 [00:00<00:00, 2112.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total segments created: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define text formatting function\n",
    "def format_text(text):\n",
    "    \"\"\"Apply consistent formatting to scientific text for improved readability.\"\"\"\n",
    "    # Fix spacing after common punctuation marks\n",
    "    for punct in ['.', ',', ';', ':', '?', '!']:\n",
    "        text = re.sub(f'\\\\{punct}([^\\\\s])', f'{punct} \\\\1', text)\n",
    "    \n",
    "    # Remove duplicate spaces\n",
    "    text = re.sub(r' {2,}', ' ', text)\n",
    "\n",
    "    # Apply strip() function and remove '\\n'\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "    \n",
    "    # Standardize citation formatting (e.g., \"[1]text\" → \"[1] text\")\n",
    "    text = re.sub(r'(\\[\\d+\\])([A-Za-z])', r'\\1 \\2', text)\n",
    "    \n",
    "    return text\n",
    "    \n",
    "# Convert each segment into its own document entry\n",
    "final_segments = []\n",
    "for item in tqdm(output):\n",
    "  for segment in item[\"sentence_segments\"]:\n",
    "    # Create a new dictionary for this segment\n",
    "    segment_entry = {}\n",
    "    \n",
    "    # Preserve the source page information\n",
    "    segment_entry[\"page_number\"] = item[\"page_number\"]\n",
    "    \n",
    "    # Combine sentences into a single coherent paragraph\n",
    "    combined_text = \"\".join(segment)\n",
    "\n",
    "    # Clean formatting using helper function\n",
    "    cleaned_text = format_text(combined_text)\n",
    "    \n",
    "    # Store the processed text\n",
    "    segment_entry[\"content\"] = cleaned_text\n",
    "    \n",
    "    # Calculate approximate token count (rough estimation)\n",
    "    segment_entry[\"estimated_tokens\"] = len(cleaned_text) / 4\n",
    "    \n",
    "    # Add to our collection\n",
    "    final_segments.append(segment_entry)\n",
    "\n",
    "# Display total number of segments created\n",
    "print(f\"Total segments created: {len(final_segments)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "474804c9-b6ae-445d-824e-91452fd16782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variable 'final_segments' has a length of: 36\n",
      " \n",
      "'Figures b and d show the first...'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(f\"The variable 'final_segments' has a length of: {len(final_segments)}\")\n",
    "\n",
    "display_segment = random. randint(0, len(final_segments))\n",
    "display = final_segments[display_segment][\"content\"][:30]\n",
    "print(\" \")\n",
    "print(f\"'{display}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb193e23-5d52-44af-b978-9ca15d56fbfe",
   "metadata": {},
   "source": [
    "\n",
    "# CONTINUE FROM HERE\n",
    "\n",
    "#### 2.3. Embedding each text chunk\n",
    "The next step is to **embed** each segment of sentences into its own **numerical representation**.\n",
    "\n",
    "For this I will be using the \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8021283f-557f-4ada-9f1b-05136c74092c",
   "metadata": {},
   "source": [
    "# OPTION 1: allenai/specter2_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7221aea-2306-4a23-a98e-857b2274cd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:12<00:00,  6.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 24 chunks\n",
      "Embedding dimension: (768,)\n",
      "Query: EEG reading while blinking\n",
      "Search completed in 0.1275 seconds\n",
      "\n",
      "Top results:\n",
      "1. Score: 0.8653\n",
      "   Page: 13\n",
      "   Text: 9. Stuart MB, McGonigle AJS, Davies M, Hobbs MJ, Boone NA, Stanger LR, et al. Low-Cost Hyperspectral Imaging with A Smartphone. Journal of Imaging.2021;7.10. Daukantas P. Hyperspectral Imaging Meets Biomedicine. Opt Photon News.2020;31:32–9.11. He Q, Wang R. Hyperspectral imaging enabled by an unmodified smartphone for analyzing skin morphological features and monitoring hemodynamics. Biomed Opt Express.2020;11:895–910.12. He Q, Li W, Shi Y, Yu Y, Geng W, Sun Z, et al.\n",
      "\n",
      "2. Score: 0.8605\n",
      "   Page: 13\n",
      "   Text: SpeCamX: mobile app that turns unmodified smartphones into multispectral imagers. Biomed Opt Express.2023;14:4929–46.13. Valleriani M, Ottone A, editors. Publishing Sacrobosco’s De sphaera in Early Modern Europe: Modes of Material and Scientific Exchange [Internet]. Cham: Springer International Publishing; 2022 [cited 2024 Jan 18]. Available from: https://link.springer.com/10.1007/978-3-030-86600-6 14. Furlong G. Treasures from UCL [Internet]. UCL Press; 2015 [cited 2021 Nov 6]. Available from: https://ucldigitalpress.co.uk/Book/Article/2/9/0/ 15. Sacrobosco J. “Sphera”, “Algorismus” and other tracts [Internet]. UCL Special Collections.c1320. Available from: https://ucl.primo.exlibrisgroup.com/permalink/44UCL_INST/155jbua/alma9931613773504761 16.\n",
      "\n",
      "3. Score: 0.8559\n",
      "   Page: 1\n",
      "   Text: Various studies have proposed using smartphone cameras for MSI, mainly motivated by the biomedical optics community, with the aim of monitoring haemodynamics by detecting the different spectral characteristics of oxygenated and deoxygenated haemoglobin in blood. Some of these require modifications or additions to the smartphone [7–10] but a new approach by He and Wang [11,12] was able to derive simulated multispectral images from an unmodified smartphone camera. Here, we adapt the method of He and Wang and apply it to generating simulated multispectral images from digitised photographs of a palimpsest. The photographs were acquired using standard digitisation protocols so the method described here could be applied to any digitised images. The technique requires a colourchecker chart which is imaged using a multispectral imaging system and with standard photography. These images are processed to provide a matrix which can convert a red-green-blue (RGB) colour photograph to a simulated multi-wavelength MSI dataset. Multispectral images have a number of advantages over RGB images, and the method described here can only emulate some of these advantages. Cameras used for RGB and MSI photography are both based on silicon sensors so have the same intrinsic spectral range from about 300 nm in the ultraviolet to 1100 nm in the near infrared. However, a commercial RGB camera has its effective spectral range reduced to about 400-800 nm to match that of the eye, by using standard glass lenses that absorb in the ultraviolet and an infrared blocking filter that excludes longer wavelengths. An MSI system maintains sensitivity further into the ultraviolet and infrared than a commercial RGB camera by using UV-transparent glass lenses and omitting the infrared blocking filter. However, more importantly, the wavelength-selective filters supplied with MSI systems are optimised for enhancing sensitivity to fluorescence. A post-processing technique such as the one proposed here could in principle offer a slightly increased spectral range by enhancing the limited sensitivity at the longest and shortest wavelengths, but it cannot offer significant sensitivity to fluorescence without changes to hardware. Here, therefore we distinguish between four methods: (i) full multispectral imaging of a palimpsest; (ii) reduced MSI imaging (excluding fluorescence); (iii) unprocessed RGB photographs; and (iv) simulated MSI images obtained by processing the RGB images with knowledge gained from the colourchecker chart. This method has the potential to offer simulated MSI for institutions that do not have access to MSI systems, and perhaps more importantly, to the many millions of objects that have already been digitised using standard photographic processes. The software was written in Matlab R2023b (The Mathworks USA) and is available ***.\n",
      "\n",
      "4. Score: 0.8462\n",
      "   Page: 4\n",
      "   Text: It is hard to read details of the script on Figure 2, so an area at the top right of the sheet that shows the undertext was chosen, and the corresponding cropped and enlarged images are shown in Figure 3.\n",
      "\n",
      "5. Score: 0.8398\n",
      "   Page: 13\n",
      "   Text: 6. Delaney JK, Ricciardi P, Glinsman LD, Facini M, Thoury M, Palmer M, et al. Use of imaging spectroscopy, fiber optic reflectance spectroscopy, and X-ray fluorescence to map and identify pigments in illuminated manuscripts. Studies in Conservation.2014;59:91–101.7. Frank J. Bolton, Amir S. Bernat, Kfir Bar-Am, David Levitz, Steven Jacques. Portable, low-cost multispectral imaging system: design, development, validation, and utilization. Journal of Biomedical Optics.2018;23:121612.8. Kim S, Kim J, Hwang M, Kim M, Jin Jo S, Je M, et al. Smartphone-based multispectral imaging and machine-learning based analysis for discrimination between seborrheic dermatitis and psoriasis on the scalp. Biomed Opt Express.2019;10:879–91.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OPTION 1\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from time import perf_counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def embed_chunks(final_chunks):\n",
    "    \"\"\"\n",
    "    Generate embeddings for all sentence chunks using SPECTER2 model.\n",
    "    \n",
    "    Args:\n",
    "        final_chunks: List of dictionaries containing sentence chunks\n",
    "        \n",
    "    Returns:\n",
    "        Updated list with embeddings added to each dictionary\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')\n",
    "    model = AutoModel.from_pretrained('allenai/specter2_base')\n",
    "    \n",
    "    # Set device (use GPU if available)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Process batches to improve efficiency\n",
    "    batch_size = 16\n",
    "    \n",
    "    for i in tqdm(range(0, len(final_chunks), batch_size), desc=\"Generating embeddings\"):\n",
    "        # Get the current batch\n",
    "        batch = final_chunks[i:i+batch_size]\n",
    "        \n",
    "        # Extract texts from the batch\n",
    "        texts = [item[\"sentence_chunk\"] for item in batch]\n",
    "        \n",
    "        # Tokenize the batch\n",
    "        inputs = tokenizer(texts, padding=True, truncation=True, \n",
    "                          max_length=512, return_tensors=\"pt\")\n",
    "        \n",
    "        # Move inputs to the appropriate device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Forward pass to get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Take the CLS token (first token) as the embedding\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        \n",
    "        # Assign embeddings to the items in the batch\n",
    "        for j, item in enumerate(batch):\n",
    "            final_chunks[i+j][\"embedding\"] = embeddings[j]\n",
    "    \n",
    "    return final_chunks\n",
    "\n",
    "\n",
    "def search_similar_chunks(final_chunks, query_text, top_k=5):\n",
    "    \"\"\"\n",
    "    Search for chunks similar to a query using dot product similarity.\n",
    "    \n",
    "    Args:\n",
    "        final_chunks: List of dictionaries containing text chunks and their embeddings\n",
    "        query_text: The query text to search for\n",
    "        top_k: Number of top results to return\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (indices, scores, results) for the top similar chunks\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer (same as used for embedding chunks)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')\n",
    "    model = AutoModel.from_pretrained('allenai/specter2_base')\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Query: {query_text}\")\n",
    "    \n",
    "    # 1. Embed the query using the same model\n",
    "    inputs = tokenizer(query_text, padding=True, truncation=True, \n",
    "                      max_length=512, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Get query embedding\n",
    "    start_time = perf_counter()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        query_embedding = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "    \n",
    "    # 2. Convert all chunk embeddings to a tensor for batch processing\n",
    "    # First, extract all embeddings from final_chunks\n",
    "    all_embeddings = np.array([chunk[\"embedding\"] for chunk in final_chunks])\n",
    "    all_embeddings_tensor = torch.tensor(all_embeddings).to(device)\n",
    "    \n",
    "    # 3. Compute dot product similarity\n",
    "    # Normalize embeddings for better results (optional if model outputs are already normalized)\n",
    "    query_embedding = query_embedding / query_embedding.norm(dim=1, keepdim=True)\n",
    "    all_embeddings_tensor = all_embeddings_tensor / all_embeddings_tensor.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    dot_scores = torch.matmul(query_embedding, all_embeddings_tensor.T)[0]\n",
    "    \n",
    "    # 4. Get top-k results\n",
    "    top_k_scores, top_k_indices = torch.topk(dot_scores, min(top_k, len(final_chunks)))\n",
    "    \n",
    "    end_time = perf_counter()\n",
    "    print(f\"Search completed in {end_time - start_time:.4f} seconds\")\n",
    "    \n",
    "    # Convert to Python lists\n",
    "    top_k_indices = top_k_indices.cpu().numpy()\n",
    "    top_k_scores = top_k_scores.cpu().numpy()\n",
    "    \n",
    "    # Get the actual chunks for the top results\n",
    "    top_results = [final_chunks[idx] for idx in top_k_indices]\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nTop results:\")\n",
    "    for i, (score, result) in enumerate(zip(top_k_scores, top_results)):\n",
    "        print(f\"{i+1}. Score: {score:.4f}\")\n",
    "        print(f\"   Page: {result['page_number']}\")\n",
    "        print(f\"   Text: {result['sentence_chunk']}\")\n",
    "        print()\n",
    "    \n",
    "    return top_k_indices, top_k_scores, top_results\n",
    "\n",
    "    \n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming final_chunks is your processed data\n",
    "    final_embedded_chunks = embed_chunks(final_chunks)\n",
    "    print(f\"Generated embeddings for {len(final_chunks)} chunks\")\n",
    "    \n",
    "    # Optional: Check the embedding dimension\n",
    "    if final_chunks:\n",
    "        print(f\"Embedding dimension: {final_embedded_chunks[0]['embedding'].shape}\")\n",
    "        \n",
    "    query = \"EEG reading while blinking\"\n",
    "    indices, scores, results = search_similar_chunks(final_chunks, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23786a4d-1af6-4886-8b53-a298c090cad9",
   "metadata": {},
   "source": [
    "# OPTION 2: BERT Mpbnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b04606de-9cf5-4293-829c-5a2d21f8f718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 24 chunks\n",
      "Embedding dimension: (768,)\n",
      "Analyzing similarity distribution for query: Adam Gibson\n",
      "Score Statistics:\n",
      "  Mean: -0.0059\n",
      "  Std Dev: 0.0349\n",
      "  Min: -0.0650\n",
      "  Max: 0.0612\n",
      "  Suggested Threshold: 0.0464\n",
      "  Documents above threshold: 2/24\n",
      "Query: Adam Gibson\n",
      "Search completed in 0.1006 seconds\n",
      "\n",
      "Top results:\n",
      "1. Score: 0.0612\n",
      "   Page: 11\n",
      "   Text: Figure 6: Images of the Archimedes Palimpsest. (a-d) are four original images taken from archimedespalimpsest.net, licenced under CC 3.0 (with Unported Access Rights). (e-h) are simulated MSI images, taken at the closest available wavelengths 6. Conclusion We have successfully used standard MSI imaging with minimal image processing to reveal two different works making up the undertext of a palimpsest. This new information will be added to the catalogue records and provide deeper context when this book is used in teaching. This work shows that simulated multispectral images can be generated from standard digitised images taken using an unmodified consumer camera. However, simulated images do not include all\n",
      "\n",
      "2. Score: 0.0605\n",
      "   Page: 12\n",
      "   Text: Easton RL, Knox KT, Christens-Barry WA. Multispectral imaging of the Archimedes palimpsest. Applied Imagery Pattern Recognition Workshop, 2003 Proceedings 32nd [Internet]. IEEE; 2003 [cited 2017 Apr 25].p. 111–6. Available from: http://ieeexplore.ieee.org/abstract/document/1284258/ 2. Christens-Barry WA, Boydston K, France FG, Knox KT, Easton Jr RL, Toth MB. Camera system for multispectral imaging of documents. International Society for Optics and Photonics; 2009.p. 724908.3. Easton RL, Christens-Barry WA, Knox KT. Spectral image processing and analysis of the Archimedes Palimpsest.2011 19th European Signal Processing Conference.2011.\n",
      "\n",
      "3. Score: 0.0447\n",
      "   Page: 1\n",
      "   Text: 2. Methods 2.1 Palimpsest The manuscript is held by UCL Special Collections (MS LAT/15). It is an early 14th Century manuscript volume and was bequeathed to UCL by John Graves (1806-1870), mathematician and Professor of Jurisprudence. It contains a number of different works on mathematics, astronomy and astrology, some by Johannes de Sacro Bosco (c. 1195 – c. 1256) which were some of the first Western European texts to use Arabic numerals [13]. They are written in various hands but bound in a single volume of 33 leaves cut to 217 x 162 mm [14]. Some of the leaves are palimpsests with the undertext visible in the margin of the overtext (Figure 1). Prior to this study, the manuscript was described in the catalogue record [15] as “a\n",
      "\n",
      "4. Score: 0.0430\n",
      "   Page: 13\n",
      "   Text: SpeCamX: mobile app that turns unmodified smartphones into multispectral imagers. Biomed Opt Express.2023;14:4929–46.13. Valleriani M, Ottone A, editors. Publishing Sacrobosco’s De sphaera in Early Modern Europe: Modes of Material and Scientific Exchange [Internet]. Cham: Springer International Publishing; 2022 [cited 2024 Jan 18]. Available from: https://link.springer.com/10.1007/978-3-030-86600-6 14. Furlong G. Treasures from UCL [Internet]. UCL Press; 2015 [cited 2021 Nov 6]. Available from: https://ucldigitalpress.co.uk/Book/Article/2/9/0/ 15. Sacrobosco J. “Sphera”, “Algorismus” and other tracts [Internet]. UCL Special Collections.c1320. Available from: https://ucl.primo.exlibrisgroup.com/permalink/44UCL_INST/155jbua/alma9931613773504761 16.\n",
      "\n",
      "5. Score: 0.0239\n",
      "   Page: 13\n",
      "   Text: 9. Stuart MB, McGonigle AJS, Davies M, Hobbs MJ, Boone NA, Stanger LR, et al. Low-Cost Hyperspectral Imaging with A Smartphone. Journal of Imaging.2021;7.10. Daukantas P. Hyperspectral Imaging Meets Biomedicine. Opt Photon News.2020;31:32–9.11. He Q, Wang R. Hyperspectral imaging enabled by an unmodified smartphone for analyzing skin morphological features and monitoring hemodynamics. Biomed Opt Express.2020;11:895–910.12. He Q, Li W, Shi Y, Yu Y, Geng W, Sun Z, et al.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from time import perf_counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def embed_chunks(final_chunks):\n",
    "    \"\"\"\n",
    "    Generate embeddings for all sentence chunks using MPNet model.\n",
    "    \n",
    "    Args:\n",
    "        final_chunks: List of dictionaries containing sentence chunks\n",
    "        \n",
    "    Returns:\n",
    "        Updated list with embeddings added to each dictionary\n",
    "    \"\"\"\n",
    "    # Load model - MPNet is designed specifically for embeddings\n",
    "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    \n",
    "    # Set device (use GPU if available)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Process batches to improve efficiency\n",
    "    batch_size = 16\n",
    "    \n",
    "    for i in tqdm(range(0, len(final_chunks), batch_size), desc=\"Generating embeddings\"):\n",
    "        # Get the current batch\n",
    "        batch = final_chunks[i:i+batch_size]\n",
    "        \n",
    "        # Extract texts from the batch\n",
    "        texts = [item[\"sentence_chunk\"] for item in batch]\n",
    "        \n",
    "        # Generate embeddings directly (simpler than with Transformers)\n",
    "        # The encode method handles tokenization, model inference, and extraction\n",
    "        embeddings = model.encode(texts, convert_to_tensor=True)\n",
    "        \n",
    "        # Convert to numpy and store\n",
    "        embeddings_np = embeddings.cpu().numpy()\n",
    "        \n",
    "        # Assign embeddings to the items in the batch\n",
    "        for j, item in enumerate(batch):\n",
    "            final_chunks[i+j][\"embedding\"] = embeddings_np[j]\n",
    "    \n",
    "    return final_chunks\n",
    "\n",
    "\n",
    "def search_similar_chunks(final_chunks, query_text, top_k=5):\n",
    "    \"\"\"\n",
    "    Search for chunks similar to a query using cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        final_chunks: List of dictionaries containing text chunks and their embeddings\n",
    "        query_text: The query text to search for\n",
    "        top_k: Number of top results to return\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (indices, scores, results) for the top similar chunks\n",
    "    \"\"\"\n",
    "    # Load model (same as used for embedding chunks)\n",
    "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Query: {query_text}\")\n",
    "    \n",
    "    # 1. Embed the query using the same model\n",
    "    start_time = perf_counter()\n",
    "    query_embedding = model.encode(query_text, convert_to_tensor=True).to(device)\n",
    "    \n",
    "    # 2. Convert all chunk embeddings to a tensor for batch processing\n",
    "    all_embeddings = np.array([chunk[\"embedding\"] for chunk in final_chunks])\n",
    "    all_embeddings_tensor = torch.tensor(all_embeddings).to(device)\n",
    "    \n",
    "    # 3. Compute cosine similarity\n",
    "    # SentenceTransformer models already normalize outputs, but let's ensure\n",
    "    # Normalize embeddings if needed\n",
    "    all_embeddings_tensor = all_embeddings_tensor / all_embeddings_tensor.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute similarity scores (dot product of normalized vectors = cosine similarity)\n",
    "    similarity_scores = torch.matmul(query_embedding.unsqueeze(0), all_embeddings_tensor.T)[0]\n",
    "    \n",
    "    # 4. Get top-k results\n",
    "    top_k_scores, top_k_indices = torch.topk(similarity_scores, min(top_k, len(final_chunks)))\n",
    "    \n",
    "    end_time = perf_counter()\n",
    "    print(f\"Search completed in {end_time - start_time:.4f} seconds\")\n",
    "    \n",
    "    # Convert to Python lists\n",
    "    top_k_indices = top_k_indices.cpu().numpy()\n",
    "    top_k_scores = top_k_scores.cpu().numpy()\n",
    "    \n",
    "    # Get the actual chunks for the top results\n",
    "    top_results = [final_chunks[idx] for idx in top_k_indices]\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nTop results:\")\n",
    "    for i, (score, result) in enumerate(zip(top_k_scores, top_results)):\n",
    "        print(f\"{i+1}. Score: {score:.4f}\")\n",
    "        print(f\"   Page: {result['page_number']}\")\n",
    "        print(f\"   Text: {result['sentence_chunk']}\")  # Show just a preview\n",
    "        print()\n",
    "    \n",
    "    return top_k_indices, top_k_scores, top_results\n",
    "\n",
    "\n",
    "# Enhanced version with statistical analysis\n",
    "def analyze_similarity_distribution(final_chunks, query_text):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of similarity scores for a query.\n",
    "    \n",
    "    Args:\n",
    "        final_chunks: List of dictionaries containing text chunks and their embeddings\n",
    "        query_text: The query text to search for\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (scores, threshold)\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Analyzing similarity distribution for query: {query_text}\")\n",
    "    \n",
    "    # Embed the query\n",
    "    query_embedding = model.encode(query_text, convert_to_tensor=True).to(device)\n",
    "    \n",
    "    # Get all embeddings\n",
    "    all_embeddings = np.array([chunk[\"embedding\"] for chunk in final_chunks])\n",
    "    all_embeddings_tensor = torch.tensor(all_embeddings).to(device)\n",
    "    \n",
    "    # Ensure normalized vectors\n",
    "    all_embeddings_tensor = all_embeddings_tensor / all_embeddings_tensor.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    similarity_scores = torch.matmul(query_embedding.unsqueeze(0), all_embeddings_tensor.T)[0].cpu().numpy()\n",
    "    \n",
    "    # Analyze distribution\n",
    "    mean_score = np.mean(similarity_scores)\n",
    "    std_score = np.std(similarity_scores)\n",
    "    min_score = np.min(similarity_scores)\n",
    "    max_score = np.max(similarity_scores)\n",
    "    \n",
    "    # Calculate a reasonable threshold (mean + 1.5*std is often useful)\n",
    "    threshold = mean_score + 1.5 * std_score\n",
    "    \n",
    "    print(f\"Score Statistics:\")\n",
    "    print(f\"  Mean: {mean_score:.4f}\")\n",
    "    print(f\"  Std Dev: {std_score:.4f}\")\n",
    "    print(f\"  Min: {min_score:.4f}\")\n",
    "    print(f\"  Max: {max_score:.4f}\")\n",
    "    print(f\"  Suggested Threshold: {threshold:.4f}\")\n",
    "    print(f\"  Documents above threshold: {np.sum(similarity_scores > threshold)}/{len(similarity_scores)}\")\n",
    "    \n",
    "    return similarity_scores, threshold\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming final_chunks is your processed data\n",
    "    final_embedded_chunks = embed_chunks(final_chunks)\n",
    "    print(f\"Generated embeddings for {len(final_chunks)} chunks\")\n",
    "    \n",
    "    # Optional: Check the embedding dimension\n",
    "    if final_chunks:\n",
    "        print(f\"Embedding dimension: {final_embedded_chunks[0]['embedding'].shape}\")\n",
    "    \n",
    "    # First analyze distribution\n",
    "    query = \"Adam Gibson\"\n",
    "    scores, threshold = analyze_similarity_distribution(final_embedded_chunks, query)\n",
    "    \n",
    "    # Then search with that threshold\n",
    "    indices, scores, results = search_similar_chunks(final_embedded_chunks, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f7b36dd-9fe6-4c36-921c-ead9bb911473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\My Drive\\feines 2025\\Github\\tutorial_RAG-assisted-LLM\\venv_llm\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\marti\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating embeddings: 100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 24 chunks\n",
      "Embedding dimension: (384,)\n",
      "Analyzing similarity distribution for query: Canon EOS 6D camera with a 50 mm lens\n",
      "Score Statistics:\n",
      "  Mean: 0.0653\n",
      "  Std Dev: 0.0698\n",
      "  Min: -0.0660\n",
      "  Max: 0.2599\n",
      "  Suggested Threshold: 0.1700\n",
      "  Documents above threshold: 1/24\n",
      "Query: Canon EOS 6D camera with a 50 mm lens\n",
      "Search completed in 0.0247 seconds\n",
      "\n",
      "Top results:\n",
      "1. Score: 0.2599\n",
      "   Page: 3\n",
      "   Text: The original digitised images were downloaded and inspected. Multispectral images were acquired of 13 sheets using the UCL Multispectral Imaging System. Simulated MSI images were generated using the process described in section 2.4, and using a W matrix that mapped between the Canon EOS 6D camera used for digitisation and the PhaseOne system used for MSI.2.5 Post processing\n",
      "\n",
      "2. Score: 0.1665\n",
      "   Page: 11\n",
      "   Text: Figure 6: Images of the Archimedes Palimpsest. (a-d) are four original images taken from archimedespalimpsest.net, licenced under CC 3.0 (with Unported Access Rights). (e-h) are simulated MSI images, taken at the closest available wavelengths 6. Conclusion We have successfully used standard MSI imaging with minimal image processing to reveal two different works making up the undertext of a palimpsest. This new information will be added to the catalogue records and provide deeper context when this book is used in teaching. This work shows that simulated multispectral images can be generated from standard digitised images taken using an unmodified consumer camera. However, simulated images do not include all\n",
      "\n",
      "3. Score: 0.1420\n",
      "   Page: 1\n",
      "   Text: Various studies have proposed using smartphone cameras for MSI, mainly motivated by the biomedical optics community, with the aim of monitoring haemodynamics by detecting the different spectral characteristics of oxygenated and deoxygenated haemoglobin in blood. Some of these require modifications or additions to the smartphone [7–10] but a new approach by He and Wang [11,12] was able to derive simulated multispectral images from an unmodified smartphone camera. Here, we adapt the method of He and Wang and apply it to generating simulated multispectral images from digitised photographs of a palimpsest. The photographs were acquired using standard digitisation protocols so the method described here could be applied to any digitised images. The technique requires a colourchecker chart which is imaged using a multispectral imaging system and with standard photography. These images are processed to provide a matrix which can convert a red-green-blue (RGB) colour photograph to a simulated multi-wavelength MSI dataset. Multispectral images have a number of advantages over RGB images, and the method described here can only emulate some of these advantages. Cameras used for RGB and MSI photography are both based on silicon sensors so have the same intrinsic spectral range from about 300 nm in the ultraviolet to 1100 nm in the near infrared. However, a commercial RGB camera has its effective spectral range reduced to about 400-800 nm to match that of the eye, by using standard glass lenses that absorb in the ultraviolet and an infrared blocking filter that excludes longer wavelengths. An MSI system maintains sensitivity further into the ultraviolet and infrared than a commercial RGB camera by using UV-transparent glass lenses and omitting the infrared blocking filter. However, more importantly, the wavelength-selective filters supplied with MSI systems are optimised for enhancing sensitivity to fluorescence. A post-processing technique such as the one proposed here could in principle offer a slightly increased spectral range by enhancing the limited sensitivity at the longest and shortest wavelengths, but it cannot offer significant sensitivity to fluorescence without changes to hardware. Here, therefore we distinguish between four methods: (i) full multispectral imaging of a palimpsest; (ii) reduced MSI imaging (excluding fluorescence); (iii) unprocessed RGB photographs; and (iv) simulated MSI images obtained by processing the RGB images with knowledge gained from the colourchecker chart. This method has the potential to offer simulated MSI for institutions that do not have access to MSI systems, and perhaps more importantly, to the many millions of objects that have already been digitised using standard photographic processes. The software was written in Matlab R2023b (The Mathworks USA) and is available ***.\n",
      "\n",
      "4. Score: 0.1293\n",
      "   Page: 2\n",
      "   Text: palimpsest with the erased text (a 13th century Italian hand?)still visible most clearly on folios 11r- 14r and 19v-20v”.2.2 Digitisation The book was fully digitised in December 2023 and is available from UCL’s digital collections catalogue as part of the Manuscript Collections listed as “Sacrobosco, Sphera, Algorismus and other tracts” [15]. The photographs are 2696 x 3512 pixels and were acquired with a Canon EOS 6D camera with a 50 mm lens at f/8 and ISO100 using a copystand with white tungsten lighting. All images were acquired and colour corrected following UCL Special Collections’ digitisation protocols. Figure 1: MS LAT/15, showing the recto (left) and verso (right) of folio 13. The horizontal overtext is dominant, but the partially erased undertext can be seen running vertically in the margins. Red rubrication is clearly visible, as are some damage and repairs.2.3 Multispectral imaging The UCL Multispectral Imaging system was supplied by R B Toth Associates (USA). It is based around a PhaseOne XF camera (PhaseOne, Denmark) with an IQ3 digital back which generates 11608 × 8708 pixel, monochrome, 16 bit images, and a 120 mm apochromatic lens. Two LED lighting panels (Equipoise Imaging LLC, USA) provide illumination at 16 discrete wavelengths from 365 nm to 940 nm and a filter wheel, equipped with longpass filters (Thorlabs, USA), can exclude the illuminating light to enhance sensitivity to fluorescence. The system is controlled using Spectral XV v2.1.0.0 (Equipoise Imaging LLC, USA) and Capture One v12.0.2.13 (PhaseOne, Denmark) software.\n",
      "\n",
      "5. Score: 0.1117\n",
      "   Page: 12\n",
      "   Text: Easton RL, Knox KT, Christens-Barry WA. Multispectral imaging of the Archimedes palimpsest. Applied Imagery Pattern Recognition Workshop, 2003 Proceedings 32nd [Internet]. IEEE; 2003 [cited 2017 Apr 25].p. 111–6. Available from: http://ieeexplore.ieee.org/abstract/document/1284258/ 2. Christens-Barry WA, Boydston K, France FG, Knox KT, Easton Jr RL, Toth MB. Camera system for multispectral imaging of documents. International Society for Optics and Photonics; 2009.p. 724908.3. Easton RL, Christens-Barry WA, Knox KT. Spectral image processing and analysis of the Archimedes Palimpsest.2011 19th European Signal Processing Conference.2011.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from time import perf_counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def embed_chunks(final_chunks):\n",
    "    \"\"\"\n",
    "    Generate embeddings for all sentence chunks using MiniLM model.\n",
    "    \n",
    "    Args:\n",
    "        final_chunks: List of dictionaries containing sentence chunks\n",
    "        \n",
    "    Returns:\n",
    "        Updated list with embeddings added to each dictionary\n",
    "    \"\"\"\n",
    "    # Load model - MiniLM is faster and lighter than MPNet\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Set device (use GPU if available)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Process batches to improve efficiency\n",
    "    # We can use a larger batch size with the smaller model\n",
    "    batch_size = 32\n",
    "    \n",
    "    for i in tqdm(range(0, len(final_chunks), batch_size), desc=\"Generating embeddings\"):\n",
    "        # Get the current batch\n",
    "        batch = final_chunks[i:i+batch_size]\n",
    "        \n",
    "        # Extract texts from the batch\n",
    "        texts = [item[\"sentence_chunk\"] for item in batch]\n",
    "        \n",
    "        # Generate embeddings directly (simpler than with Transformers)\n",
    "        # The encode method handles tokenization, model inference, and extraction\n",
    "        embeddings = model.encode(texts, convert_to_tensor=True)\n",
    "        \n",
    "        # Convert to numpy and store\n",
    "        embeddings_np = embeddings.cpu().numpy()\n",
    "        \n",
    "        # Assign embeddings to the items in the batch\n",
    "        for j, item in enumerate(batch):\n",
    "            final_chunks[i+j][\"embedding\"] = embeddings_np[j]\n",
    "    \n",
    "    return final_chunks\n",
    "\n",
    "\n",
    "def search_similar_chunks(final_chunks, query_text, top_k=5):\n",
    "    \"\"\"\n",
    "    Search for chunks similar to a query using cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        final_chunks: List of dictionaries containing text chunks and their embeddings\n",
    "        query_text: The query text to search for\n",
    "        top_k: Number of top results to return\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (indices, scores, results) for the top similar chunks\n",
    "    \"\"\"\n",
    "    # Load model (same as used for embedding chunks)\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Query: {query_text}\")\n",
    "    \n",
    "    # 1. Embed the query using the same model\n",
    "    start_time = perf_counter()\n",
    "    query_embedding = model.encode(query_text, convert_to_tensor=True).to(device)\n",
    "    \n",
    "    # 2. Convert all chunk embeddings to a tensor for batch processing\n",
    "    all_embeddings = np.array([chunk[\"embedding\"] for chunk in final_chunks])\n",
    "    all_embeddings_tensor = torch.tensor(all_embeddings).to(device)\n",
    "    \n",
    "    # 3. Compute cosine similarity\n",
    "    # SentenceTransformer models already normalize outputs, but let's ensure\n",
    "    # Normalize embeddings if needed\n",
    "    all_embeddings_tensor = all_embeddings_tensor / all_embeddings_tensor.norm(dim=1, keepdim=True)\n",
    "    query_embedding = query_embedding / query_embedding.norm()\n",
    "    \n",
    "    # Compute similarity scores (dot product of normalized vectors = cosine similarity)\n",
    "    similarity_scores = torch.matmul(query_embedding.unsqueeze(0), all_embeddings_tensor.T)[0]\n",
    "    \n",
    "    # 4. Get top-k results\n",
    "    top_k_scores, top_k_indices = torch.topk(similarity_scores, min(top_k, len(final_chunks)))\n",
    "    \n",
    "    end_time = perf_counter()\n",
    "    print(f\"Search completed in {end_time - start_time:.4f} seconds\")\n",
    "    \n",
    "    # Convert to Python lists\n",
    "    top_k_indices = top_k_indices.cpu().numpy()\n",
    "    top_k_scores = top_k_scores.cpu().numpy()\n",
    "    \n",
    "    # Get the actual chunks for the top results\n",
    "    top_results = [final_chunks[idx] for idx in top_k_indices]\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nTop results:\")\n",
    "    for i, (score, result) in enumerate(zip(top_k_scores, top_results)):\n",
    "        print(f\"{i+1}. Score: {score:.4f}\")\n",
    "        print(f\"   Page: {result['page_number']}\")\n",
    "        print(f\"   Text: {result['sentence_chunk']}\")  # Show just a preview\n",
    "        print()\n",
    "    \n",
    "    return top_k_indices, top_k_scores, top_results\n",
    "\n",
    "\n",
    "# Enhanced version with statistical analysis\n",
    "def analyze_similarity_distribution(final_chunks, query_text):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of similarity scores for a query.\n",
    "    \n",
    "    Args:\n",
    "        final_chunks: List of dictionaries containing text chunks and their embeddings\n",
    "        query_text: The query text to search for\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (scores, threshold)\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Analyzing similarity distribution for query: {query_text}\")\n",
    "    \n",
    "    # Embed the query\n",
    "    query_embedding = model.encode(query_text, convert_to_tensor=True).to(device)\n",
    "    \n",
    "    # Get all embeddings\n",
    "    all_embeddings = np.array([chunk[\"embedding\"] for chunk in final_chunks])\n",
    "    all_embeddings_tensor = torch.tensor(all_embeddings).to(device)\n",
    "    \n",
    "    # Ensure normalized vectors\n",
    "    all_embeddings_tensor = all_embeddings_tensor / all_embeddings_tensor.norm(dim=1, keepdim=True)\n",
    "    query_embedding = query_embedding / query_embedding.norm()\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    similarity_scores = torch.matmul(query_embedding.unsqueeze(0), all_embeddings_tensor.T)[0].cpu().numpy()\n",
    "    \n",
    "    # Analyze distribution\n",
    "    mean_score = np.mean(similarity_scores)\n",
    "    std_score = np.std(similarity_scores)\n",
    "    min_score = np.min(similarity_scores)\n",
    "    max_score = np.max(similarity_scores)\n",
    "    \n",
    "    # Calculate a reasonable threshold (mean + 1.5*std is often useful)\n",
    "    threshold = mean_score + 1.5 * std_score\n",
    "    \n",
    "    print(f\"Score Statistics:\")\n",
    "    print(f\"  Mean: {mean_score:.4f}\")\n",
    "    print(f\"  Std Dev: {std_score:.4f}\")\n",
    "    print(f\"  Min: {min_score:.4f}\")\n",
    "    print(f\"  Max: {max_score:.4f}\")\n",
    "    print(f\"  Suggested Threshold: {threshold:.4f}\")\n",
    "    print(f\"  Documents above threshold: {np.sum(similarity_scores > threshold)}/{len(similarity_scores)}\")\n",
    "    \n",
    "    return similarity_scores, threshold\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming final_chunks is your processed data\n",
    "    final_embedded_chunks = embed_chunks(final_chunks)\n",
    "    print(f\"Generated embeddings for {len(final_chunks)} chunks\")\n",
    "    \n",
    "    # Optional: Check the embedding dimension\n",
    "    if final_chunks:\n",
    "        print(f\"Embedding dimension: {final_embedded_chunks[0]['embedding'].shape}\")\n",
    "    \n",
    "    # First analyze distribution\n",
    "    query = \" UCL Multispectral Imaging system\"\n",
    "    scores, threshold = analyze_similarity_distribution(final_embedded_chunks, query)\n",
    "    \n",
    "    # Then search with that threshold\n",
    "    indices, scores, results = search_similar_chunks(final_embedded_chunks, query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4cc673-1303-4431-97da-9b157e397146",
   "metadata": {},
   "source": [
    "#### 2.4. Similarity Search\n",
    "\n",
    "# EXPLAIN\n",
    "# CHANGE EMBEDDING MODEL E.G. MPBNET\n",
    "Note:\n",
    "We want to: search for a query (e.g. \"macronutrient functions\") and get relevant info from textbook.\n",
    "\n",
    "Steps to do this:\n",
    "1. Define query string\n",
    "2. Turn query string into embedding\n",
    "3. Perform dot product or cosine similarity function between the text embeddings and the query embedding\n",
    "4. Sort the results form 3 in descending order\n",
    "\n",
    "\n",
    "Note: to use dot product for comparison ensure both vector sizes are of the same shape and tensors/vectors are in the same data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d69dda-9931-425e-a791-b85304d9d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from time import perf_counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def search_similar_chunks(final_chunks, query_text, top_k=5):\n",
    "    \"\"\"\n",
    "    Search for chunks similar to a query using dot product similarity.\n",
    "    \n",
    "    Args:\n",
    "        final_chunks: List of dictionaries containing text chunks and their embeddings\n",
    "        query_text: The query text to search for\n",
    "        top_k: Number of top results to return\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (indices, scores, results) for the top similar chunks\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer (same as used for embedding chunks)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')\n",
    "    model = AutoModel.from_pretrained('allenai/specter2_base')\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Query: {query_text}\")\n",
    "    \n",
    "    # 1. Embed the query using the same model\n",
    "    inputs = tokenizer(query_text, padding=True, truncation=True, \n",
    "                      max_length=512, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Get query embedding\n",
    "    start_time = perf_counter()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        query_embedding = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "    \n",
    "    # 2. Convert all chunk embeddings to a tensor for batch processing\n",
    "    # First, extract all embeddings from final_chunks\n",
    "    all_embeddings = np.array([chunk[\"embedding\"] for chunk in final_chunks])\n",
    "    all_embeddings_tensor = torch.tensor(all_embeddings).to(device)\n",
    "    \n",
    "    # 3. Compute dot product similarity\n",
    "    # Normalize embeddings for better results (optional if model outputs are already normalized)\n",
    "    query_embedding = query_embedding / query_embedding.norm(dim=1, keepdim=True)\n",
    "    all_embeddings_tensor = all_embeddings_tensor / all_embeddings_tensor.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    dot_scores = torch.matmul(query_embedding, all_embeddings_tensor.T)[0]\n",
    "    \n",
    "    # 4. Get top-k results\n",
    "    top_k_scores, top_k_indices = torch.topk(dot_scores, min(top_k, len(final_chunks)))\n",
    "    \n",
    "    end_time = perf_counter()\n",
    "    print(f\"Search completed in {end_time - start_time:.4f} seconds\")\n",
    "    \n",
    "    # Convert to Python lists\n",
    "    top_k_indices = top_k_indices.cpu().numpy()\n",
    "    top_k_scores = top_k_scores.cpu().numpy()\n",
    "    \n",
    "    # Get the actual chunks for the top results\n",
    "    top_results = [final_chunks[idx] for idx in top_k_indices]\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nTop results:\")\n",
    "    for i, (score, result) in enumerate(zip(top_k_scores, top_results)):\n",
    "        print(f\"{i+1}. Score: {score:.4f}\")\n",
    "        print(f\"   Page: {result['page_number']}\")\n",
    "        print(f\"   Text: {result['sentence_chunk']}\")\n",
    "        print()\n",
    "    \n",
    "    return top_k_indices, top_k_scores, top_results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"Martina Sabate\"\n",
    "    indices, scores, results = search_similar_chunks(final_chunks, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25198c7-5018-4f5c-9455-ecd984d23902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2 (IMPROVED BY CLAUDE)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from time import perf_counter\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def analyze_similarity_distribution(final_chunks, query_text):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of similarity scores for a query.\n",
    "    \n",
    "    Args:\n",
    "        final_chunks: List of dictionaries containing text chunks and their embeddings\n",
    "        query_text: The query text to search for\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (scores, normalized_scores, threshold)\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')\n",
    "    model = AutoModel.from_pretrained('allenai/specter2_base')\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Analyzing similarity distribution for query: {query_text}\")\n",
    "    \n",
    "    # Embed the query\n",
    "    inputs = tokenizer(query_text, padding=True, truncation=True, \n",
    "                      max_length=512, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        query_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    # Normalize query embedding\n",
    "    query_embedding = query_embedding / query_embedding.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # Get all embeddings\n",
    "    all_embeddings = np.array([chunk[\"embedding\"] for chunk in final_chunks])\n",
    "    all_embeddings_tensor = torch.tensor(all_embeddings).to(device)\n",
    "    all_embeddings_tensor = all_embeddings_tensor / all_embeddings_tensor.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    dot_scores = torch.matmul(query_embedding, all_embeddings_tensor.T)[0].cpu().numpy()\n",
    "    \n",
    "    # Analyze distribution\n",
    "    mean_score = np.mean(dot_scores)\n",
    "    std_score = np.std(dot_scores)\n",
    "    min_score = np.min(dot_scores)\n",
    "    max_score = np.max(dot_scores)\n",
    "    \n",
    "    # Calculate Z-scores for outlier detection\n",
    "    z_scores = (dot_scores - mean_score) / std_score\n",
    "    \n",
    "    # Identify potential threshold\n",
    "    # A common approach is mean + 2*std for outlier detection\n",
    "    threshold = mean_score + 2 * std_score\n",
    "    \n",
    "    # Use MinMaxScaler to rescale scores to [0,1] range\n",
    "    # This helps interpret relative relevance\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_scores = scaler.fit_transform(dot_scores.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(dot_scores, bins=30, alpha=0.7)\n",
    "    plt.axvline(threshold, color='r', linestyle='--', label=f'Threshold: {threshold:.4f}')\n",
    "    plt.axvline(mean_score, color='g', linestyle='-', label=f'Mean: {mean_score:.4f}')\n",
    "    plt.title('Distribution of Similarity Scores')\n",
    "    plt.xlabel('Similarity Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    print(f\"Score Statistics:\")\n",
    "    print(f\"  Mean: {mean_score:.4f}\")\n",
    "    print(f\"  Std Dev: {std_score:.4f}\")\n",
    "    print(f\"  Min: {min_score:.4f}\")\n",
    "    print(f\"  Max: {max_score:.4f}\")\n",
    "    print(f\"  Suggested Threshold: {threshold:.4f}\")\n",
    "    print(f\"  Documents above threshold: {np.sum(dot_scores > threshold)}/{len(dot_scores)}\")\n",
    "    \n",
    "    # Return scores for further analysis\n",
    "    return dot_scores, normalized_scores, threshold\n",
    "\n",
    "\n",
    "def search_similar_chunks(final_chunks, query_text, top_k=5, threshold=None, use_relative_scoring=True):\n",
    "    \"\"\"\n",
    "    Search for chunks similar to a query using improved scoring mechanisms.\n",
    "    \n",
    "    Args:\n",
    "        final_chunks: List of dictionaries containing text chunks and their embeddings\n",
    "        query_text: The query text to search for\n",
    "        top_k: Number of top results to return\n",
    "        threshold: Optional cutoff value for scores\n",
    "        use_relative_scoring: Whether to use relative (normalized) scoring\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (indices, scores, results) for the top similar chunks\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')\n",
    "    model = AutoModel.from_pretrained('allenai/specter2_base')\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Query: {query_text}\")\n",
    "    \n",
    "    # 1. Embed the query\n",
    "    start_time = perf_counter()\n",
    "    inputs = tokenizer(query_text, padding=True, truncation=True, \n",
    "                      max_length=512, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        query_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    # 2. Normalize query embedding\n",
    "    query_embedding = query_embedding / query_embedding.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # 3. Get all chunk embeddings\n",
    "    all_embeddings = np.array([chunk[\"embedding\"] for chunk in final_chunks])\n",
    "    all_embeddings_tensor = torch.tensor(all_embeddings).to(device)\n",
    "    all_embeddings_tensor = all_embeddings_tensor / all_embeddings_tensor.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # 4. Compute similarity scores\n",
    "    dot_scores = torch.matmul(query_embedding, all_embeddings_tensor.T)[0].cpu().numpy()\n",
    "    \n",
    "    # 5. Compute relative scores\n",
    "    if use_relative_scoring:\n",
    "        mean_score = np.mean(dot_scores)\n",
    "        std_score = np.std(dot_scores)\n",
    "        # Convert to Z-scores\n",
    "        relative_scores = (dot_scores - mean_score) / std_score\n",
    "        # Also compute min-max scaled scores for easier interpretation\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_scores = scaler.fit_transform(dot_scores.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # 6. Apply threshold if specified\n",
    "    if threshold is not None:\n",
    "        valid_indices = np.where(dot_scores >= threshold)[0]\n",
    "        if len(valid_indices) == 0:\n",
    "            print(\"No results above threshold\")\n",
    "            return [], [], []\n",
    "        \n",
    "        # Filter based on threshold\n",
    "        filtered_scores = dot_scores[valid_indices]\n",
    "        top_k = min(top_k, len(filtered_scores))\n",
    "        \n",
    "        # Get top-k among valid indices\n",
    "        top_indices = np.argsort(-filtered_scores)[:top_k]\n",
    "        top_indices = valid_indices[top_indices]\n",
    "        top_scores = dot_scores[top_indices]\n",
    "    else:\n",
    "        # Get top-k without threshold\n",
    "        top_indices = np.argsort(-dot_scores)[:top_k]\n",
    "        top_scores = dot_scores[top_indices]\n",
    "    \n",
    "    # Get the actual chunks\n",
    "    top_results = [final_chunks[idx] for idx in top_indices]\n",
    "    \n",
    "    end_time = perf_counter()\n",
    "    print(f\"Search completed in {end_time - start_time:.4f} seconds\")\n",
    "    \n",
    "    # Print results with more context\n",
    "    print(\"\\nTop results:\")\n",
    "    for i, (idx, score) in enumerate(zip(top_indices, top_scores)):\n",
    "        print(f\"{i+1}. Raw Score: {score:.4f}\")\n",
    "        \n",
    "        if use_relative_scoring:\n",
    "            print(f\"   Z-Score: {relative_scores[idx]:.4f}\")\n",
    "            print(f\"   Normalized Score: {scaled_scores[idx]:.4f}\")\n",
    "            \n",
    "        print(f\"   Page: {top_results[i]['page_number']}\")\n",
    "        print(f\"   Text: {top_results[i]['sentence_chunk'][:150]}...\")\n",
    "        print()\n",
    "    \n",
    "    return top_indices, top_scores, top_results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # First analyze the distribution to determine a good threshold\n",
    "    scores, norm_scores, suggested_threshold = analyze_similarity_distribution(\n",
    "        final_chunks, \"good foods for protein\")\n",
    "    \n",
    "    # Then search with the threshold\n",
    "    print(\"\\n=== SEARCH WITH THRESHOLD ===\")\n",
    "    indices, scores, results = search_similar_chunks(\n",
    "        final_chunks, \n",
    "        \"good foods for protein\",\n",
    "        threshold=suggested_threshold,\n",
    "        use_relative_scoring=True\n",
    "    )\n",
    "    \n",
    "    # For comparison, run a more relevant query\n",
    "    print(\"\\n=== RELEVANT QUERY ANALYSIS ===\")\n",
    "    relevant_scores, relevant_norm, relevant_threshold = analyze_similarity_distribution(\n",
    "        final_chunks, \"heritage imaging techniques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de236578-91b4-4a50-8634-26682fef3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define query\n",
    "query = \"good foods for protein\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# 2. Embed the query\n",
    "# Note: it's important to embed your query with the SAME MODEL as embeddings\n",
    "query_embedding = model.encode(query, convert_to_tensor=True).to(device)\n",
    "\n",
    "# 3. Get similarity scores\n",
    "# with dot product (use cosine similarity if outputs of model aren't normalised)\n",
    "from time import perf_counter as timer\n",
    "\n",
    "start_time = timer\n",
    "dot_scores = util.dot_score(a=query_embedding, b=final_embedded_chunks)[0]\n",
    "end_time = timer\n",
    "\n",
    "# 4. Get the top-k results (we keep top 5)\n",
    "top_results_dot_product = torch.topk(dot_scores, 5)\n",
    "top_results_dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4ea67e-e27b-4b7d-b4ff-a2e6640456da",
   "metadata": {},
   "source": [
    "# IMPORT LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6472dd-94d3-4ef7-b469-ddbe92fbff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "import torch\n",
    "\n",
    "# Log in - HuggingFace\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "\n",
    "\n",
    "# 1. Create a quantisation config\n",
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "# Bonus: flash attention 2 - faster attention mechanism\n",
    "# Flash Attention 2 requires GPU with compute score of 8+\n",
    "# For now sdpa as I'm running on CPU\n",
    "if (is_flash_attn_2_available()):\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    attn_implementation = \"sdpa\"\n",
    "\n",
    "print(attn_implementation)\n",
    "\n",
    "# 2. Pick a model to use + log in to HuggingFace\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "\n",
    "# 3. Instantiate tokeniser (turns text into tokens)\n",
    "tokeniser = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "\n",
    "\n",
    "# 4. Instantiate the model\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                                 low_cpu_mem_usage=False, # use as much memory as we can. If true it offloads memory to cpu\n",
    "                                                 attn_implementation=attn_implementation)\n",
    "\n",
    "\n",
    "if not use_quantization_config: # quantization takes care of device setting automatically, so if it's not used, send model to GPU\n",
    "    llm_model.to(\"cuda\")\n",
    "\n",
    "\n",
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    # Get model parameters and buffer sizes\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate model sizes\n",
    "    model_mem_bytes = mem_params + mem_buffers\n",
    "    model_mem_mb = model_mem_bytes / (1024**2)\n",
    "    model_mem_gb = model_mem_bytes / (1024**3)\n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes,\n",
    "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "\n",
    "    input_text = \"What are the macronutrients, and what roles do they play in the human body?\"\n",
    "print_input_text = \"Input text: \" + input_text\n",
    "print_input_text\n",
    "\n",
    "\n",
    "# Create a prompt template\n",
    "dialogue_template = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": input_text}\n",
    "]\n",
    "\n",
    "# Applies the chat template\n",
    "prompt = tokeniser.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False,\n",
    "                                       add_generation_prompt=True)\n",
    "prompt\n",
    "# This is the format we need as input for Gemma model, that is why we need the 'apply_chat_template' function\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize input text (turn into numbers) and send to CPU\n",
    "input_ids = tokeniser(prompt,\n",
    "                      return_tensors=\"pt\").to(\"cuda\")\n",
    "print(f\"Model inputs: {input_ids}\")\n",
    "\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             max_new_tokens=256)\n",
    "print(\"processed\")\n",
    "outputs[0]\n",
    "#** before input_ids means pass in (input_ids + attention mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
